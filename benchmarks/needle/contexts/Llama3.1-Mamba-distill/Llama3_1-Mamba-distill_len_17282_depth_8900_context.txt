<|begin_of_text|><|begin_of_text|>January 2017People who are powerful but uncharismatic will tend to be disliked.
Their power makes them a target for criticism that they don't have
the charisma to disarm. That was Hillary Clinton's problem. It also
tends to be a problem for any CEO who is more of a builder than a
schmoozer. And yet the builder-type CEO is (like Hillary) probably
the best person for the job.I don't think there is any solution to this problem. It's human
nature. The best we can do is to recognize that it's happening, and
to understand that being a magnet for criticism is sometimes a sign
not that someone is the wrong person for a job, but that they're
the right one.November 2021(This essay is derived from a talk at the Cambridge Union.)When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kind. The strongest proof
would be to show that taste exists in the narrowest sense, so I'm
going to talk about taste in art. You have better taste than me if
the art you like is better than the art I like.If there's no such thing as good taste, then there's no such thing
as good art. Because if there is such a
thing as good art, it's
easy to tell which of two people has better taste. Show them a lot
of works by artists they've never seen before and ask them to
choose the best, and whoever chooses the better art has better
taste.So if you want to discard the concept of good taste, you also have
to discard the concept of good art. And that means you have to
discard the possibility of people being good at making it. Which
means there's no way for artists to be good at their jobs. And not
just visual artists, but anyone who is in any sense an artist. You
can't have good actors, or novelists, or composers, or dancers
either. You can have popular novelists, but not good ones.We don't realize how far we'd have to go if we discarded the concept
of good taste, because we don't even debate the most obvious cases.
But it doesn't just mean we can't say which of two famous painters
is better. It means we can't say that any painter is better than a
randomly chosen eight year old.That was how I realized my father was wrong. I started studying
painting. And it was just like other kinds of work I'd done: you
could do it well, or badly, and if you tried hard, you could get
better at it. And it was obvious that Leonardo and Bellini were
much better at it than me. That gap between us was not imaginary.
They were so good. And if they could be good, then art could be
good, and there was such a thing as good taste after all.Now that I've explained how to show there is such a thing as good
taste, I should also explain why people think there isn't. There
are two reasons. One is that there's always so much disagreement
about taste. Most people's response to art is a tangle of unexamined
impulses. Is the artist famous? Is the subject attractive? Is this
the sort of art they're supposed to like? Is it hanging in a famous
museum, or reproduced in a big, expensive book? In practice most
people's response to art is dominated by such extraneous factors.And the people who do claim to have good taste are so often mistaken.
The paintings admired by the so-called experts in one generation
are often so different from those admired a few generations later.
It's easy to conclude there's nothing real there at all. It's only
when you isolate this force, for example by trying to paint and
comparing your work to Bellini's, that you can see that it does in
fact exist.The other reason people doubt that art can be good is that there
doesn't seem to be any room in the art for this goodness. The
argument goes like this. Imagine several people looking at a work
of art and judging how good it is. If being good art really is a
property of objects, it should be in the object somehow. But it
doesn't seem to be; it seems to be something happening in the heads
of each of the observers. And if they disagree, how do you choose
between them?The solution to this puzzle is to realize that the purpose of art
is to work on its human audience, and humans have a lot in common.
And to the extent the things an object acts upon respond in the
same way, that's arguably what it means for the object to have the
corresponding property. If everything a particle interacts with
behaves as if the particle had a mass of m, then it has a mass of
m. So the distinction between "objective" and "subjective" is not
binary, but a matter of degree, depending on how much the subjects
have in common. Particles interacting with one another are at one
pole, but people interacting with art are not all the way at the
other; their reactions aren't random.Because people's responses to art aren't random, art can be designed
to operate on people, and be good or bad depending on how effectively
it does so. Much as a vaccine can be. If someone were talking about
the ability of a vaccine to confer immunity, it would seem very
frivolous to object that conferring immunity wasn't really a property
of vaccines, because acquiring immunity is something that happens
in the immune system of each individual person. Sure, people's
immune systems vary, and a vaccine that worked on one might not
work on another, but that doesn't make it meaningless to talk about
the effectiveness of a vaccine.The situation with art is messier, of course. You can't measure
effectiveness by simply taking a vote, as you do with vaccines.
You have to imagine the responses of subjects with a deep knowledge
of art, and enough clarity of mind to be able to ignore extraneous
influences like the fame of the artist. And even then you'd still
see some disagreement. People do vary, and judging art is hard,
especially recent art. There is definitely not a total order either
of works or of people's ability to judge them. But there is equally
definitely a partial order of both. So while it's not possible to
have perfect taste, it is possible to have good taste.
Thanks to the Cambridge Union for inviting me, and to Trevor
Blackwell, Jessica Livingston, and Robert Morris for reading drafts
of this.
February 2020What should an essay be? Many people would say persuasive. That's
what a lot of us were taught essays should be. But I think we can
aim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough
merely to be correct. It's easy to make a statement correct by
making it vague. That's a common flaw in academic writing, for
example. If you know nothing at all about an issue, you can't go
wrong by saying that the issue is a complex one, that there are
many factors to be considered, that it's a mistake to take too
simplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing.
Useful writing makes claims that are as strong as they can be made
without becoming false.For example, it's more useful to say that Pike's Peak is near the
middle of Colorado than merely somewhere in Colorado. But if I say
it's in the exact middle of Colorado, I've now gone too far, because
it's a bit east of the middle.Precision and correctness are like opposing forces. It's easy to
satisfy one if you ignore the other. The converse of vaporous
academic writing is the bold, but false, rhetoric of demagogues.
Useful writing is bold, but true.It's also two other things: it tells people something important,
and that at least some of them didn't already know.Telling people something they didn't know doesn't always mean
surprising them. Sometimes it means telling them something they
knew unconsciously but had never put into words. In fact those may
be the more valuable insights, because they tend to be more
fundamental.Let's put them all together. Useful writing tells people something
true and important that they didn't already know, and tells them
as unequivocally as possible.Notice these are all a matter of degree. For example, you can't
expect an idea to be novel to everyone. Any insight that you have
will probably have already been had by at least one of the world's
7 billion people. But it's sufficient if an idea is novel to a lot
of readers.Ditto for correctness, importance, and strength. In effect the four
components are like numbers you can multiply together to get a score
for usefulness. Which I realize is almost awkwardly reductive, but
nonetheless true._____
How can you ensure that the things you say are true and novel and
important? Believe it or not, there is a trick for doing this. I
learned it from my friend Robert Morris, who has a horror of saying
anything dumb. His trick is not to say anything unless he's sure
it's worth hearing. This makes it hard to get opinions out of him,
but when you do, they're usually right.Translated into essay writing, what this means is that if you write
a bad sentence, you don't publish it. You delete it and try again.
Often you abandon whole branches of four or five paragraphs. Sometimes
a whole essay.You can't ensure that every idea you have is good, but you can
ensure that every one you publish is, by simply not publishing the
ones that aren't.In the sciences, this is called publication bias, and is considered
bad. When some hypothesis you're exploring gets inconclusive results,
you're supposed to tell people about that too. But with essay
writing, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an
essay fast, trying out all kinds of ideas. Then I spend days rewriting
it very carefully.I've never tried to count how many times I proofread essays, but
I'm sure there are sentences I've read 100 times before publishing
them. When I proofread an essay, there are usually passages that
stick out in an annoying way, sometimes because they're clumsily
written, and sometimes because I'm not sure they're true. The
annoyance starts out unconscious, but after the tenth reading or
so I'm saying "Ugh, that part" each time I hit it. They become like
briars that catch your sleeve as you walk past. Usually I won't
publish an essay till they're all gone  till I can read through
the whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't
think of a way to rephrase it, but I will never knowingly let through
one that doesn't seem correct. You never have to. If a sentence
doesn't seem right, all you have to do is ask why it doesn't, and
you've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You
don't have a deadline. You can work for as long on an essay as you
need to get it right. You don't have to publish the essay at all,
if you can't get it right. Mistakes seem to lose courage in the
face of an enemy with unlimited resources. Or that's what it feels
like. What's really going on is that you have different expectations
for yourself. You're like a parent saying to a child "we can sit
here all night till you eat your vegetables." Except you're the
child too.I'm not saying no mistake gets through. For example, I added condition
(c) in "A Way to Detect Bias" 
after readers pointed out that I'd
omitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I
suggest to young founders for getting startup ideas: to make something
you yourself want. You can use yourself as a proxy for the reader.
The reader is not completely unlike you, so if you write about
topics that seem important to you, they'll probably seem important
to a significant number of readers as well.Importance has two factors. It's the number of people something
matters to, times how much it matters to them. Which means of course
that it's not a rectangle, but a sort of ragged comb, like a Riemann
sum.The way to get novelty is to write about topics you've thought about
a lot. Then you can use yourself as a proxy for the reader in this
department too. Anything you notice that surprises you, who've
thought about the topic a lot, will probably also surprise a
significant number of readers. And here, as with correctness and
importance, you can use the Morris technique to ensure that you
will. If you don't learn anything from writing an essay, don't
publish it.You need humility to measure novelty, because acknowledging the
novelty of an idea means acknowledging your previous ignorance of
it. Confidence and humility are often seen as opposites, but in
this case, as in many others, confidence helps you to be humble.
If you know you're an expert on some topic, you can freely admit
when you learn something you didn't know, because you can be confident
that most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two
things: thinking well, and the skillful use of qualification. These
two counterbalance each other, like the accelerator and clutch in
a car with a manual transmission. As you try to refine the expression
of an idea, you adjust the qualification accordingly. Something
you're sure of, you can state baldly with no qualification at all,
as I did the four components of useful writing. Whereas points that
seem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less
qualification. But you can rarely get it down to zero. Sometimes
you don't even want to, if it's a side point and a fully refined
version would be too long.Some say that qualifications weaken writing. For example, that you
should never begin a sentence in an essay with "I think," because
if you're saying it, then of course you think it. And it's true
that "I think x" is a weaker statement than simply "x." Which is
exactly why you need "I think." You need it to express your degree
of certainty.But qualifications are not scalars. They're not just experimental
error. There must be 50 things they can express: how broadly something
applies, how you know it, how happy you are it's so, even how it
could be falsified. I'm not going to try to explore the structure
of qualification here. It's probably more complex than the whole
topic of writing usefully. Instead I'll just give you a practical
tip: Don't underestimate qualification. It's an important skill in
its own right, not just a sort of tax you have to pay in order to
avoid saying things that are false. So learn and use its full range.
It may not be fully half of having good ideas, but it's part of
having them.There's one other quality I aim for in essays: to say things as
simply as possible. But I don't think this is a component of
usefulness. It's more a matter of consideration for the reader. And
it's a practical aid in getting things right; a mistake is more
obvious when expressed in simple language. But I'll admit that the
main reason I write simply is not for the reader's sake or because
it helps get things right, but because it bothers me to use more
or fancier words than I need to. It seems inelegant, like a program
that's too long.I realize florid writing works for some people. But unless you're
sure you're one of them, the best advice is to write as simply as
you can._____
I believe the formula I've given you, importance + novelty +
correctness + strength, is the recipe for a good essay. But I should
warn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something
they didn't know, they don't always thank you for it. Sometimes the
reason people don't know something is because they don't want to
know it. Usually because it contradicts some cherished belief. And
indeed, if you're looking for novel ideas, popular but mistaken
beliefs are a good place to find them. Every popular mistaken belief
creates a dead zone of ideas around 
it that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything
that annoys people more than having their cherished assumptions
contradicted, it's having them flatly contradicted.Plus if you've used the Morris technique, your writing will seem
quite confident. Perhaps offensively confident, to people who
disagree with you. The reason you'll seem confident is that you are
confident: you've cheated, by only publishing the things you're
sure of.  It will seem to people who try to disagree with you that
you never admit you're wrong. In fact you constantly admit you're
wrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things
worse. Brevity is the diction of command. If you watch someone
delivering unwelcome news from a position of inferiority, you'll
notice they tend to use lots of words, to soften the blow. Whereas
to be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly
than you mean. To put "perhaps" in front of something you're actually
quite sure of. But you'll notice that when writers do this, they
usually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic
tone for a whole essay. I think we just have to face the fact that
elegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that
an essay is correct, it will be invulnerable to attack. That's sort
of true. It will be invulnerable to valid attacks. But in practice
that's little consolation.In fact, the strength component of useful writing will make you
particularly vulnerable to misrepresentation. If you've stated an
idea as strongly as you could without making it false, all anyone
has to do is to exaggerate slightly what you said, and now it is
false.Much of the time they're not even doing it deliberately. One of the
most surprising things you'll discover, if you start writing essays,
is that people who disagree with you rarely disagree with what
you've actually written. Instead they make up something you said
and disagree with that.For what it's worth, the countermove is to ask someone who does
this to quote a specific sentence or passage you wrote that they
believe is false, and explain why. I say "for what it's worth"
because they never do. So although it might seem that this could
get a broken discussion back on track, the truth is that it was
never on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if
they're misinterpretations a reasonably smart and well-intentioned
person might make. In fact it's sometimes better to say something
slightly misleading and then add the correction than to try to get
an idea right in one shot. That can be more efficient, and can also
model the way such an idea would be discovered.But I don't think you should explicitly forestall intentional
misinterpretations in the body of an essay. An essay is a place to
meet honest readers. You don't want to spoil your house by putting
bars on the windows to protect against dishonest ones. The place
to protect against intentional misinterpretations is in end-notes.
But don't think you can predict them all. People are as ingenious
at misrepresenting you when you say something they don't want to
hear as they are at coming up with rationalizations for things they
want to do but know they shouldn't. I suspect it's the same skill._____
As with most other things, the way to get better at writing essays
is to practice. But how do you start? Now that we've examined the
structure of useful writing, we can rephrase that question more
precisely. Which constraint do you relax initially? The answer is,
the first component of importance: the number of people who care
about what you write.If you narrow the topic sufficiently, you can probably find something
you're an expert on. Write about that to start with. If you only
have ten readers who care, that's fine. You're helping them, and
you're writing. Later you can expand the breadth of topics you write
about.The other constraint you can relax is a little surprising: publication.
Writing essays doesn't have to mean publishing them. That may seem
strange now that the trend is to publish every random thought, but
it worked for me. I wrote what amounted to essays in notebooks for
about 15 years. I never published any of them and never expected
to. I wrote them as a way of figuring things out. But when the web
came along I'd had a lot of practice.Incidentally, 
Steve 
Wozniak did the same thing. In high school he
designed computers on paper for fun. He couldn't build them because
he couldn't afford the components. But when Intel launched 4K DRAMs
in 1975, he was ready._____
How many essays are there left to write though? The answer to that
question is probably the most exciting thing I've learned about
essay writing. Nearly all of them are left to write.Although the essay 
is an old form, it hasn't been assiduously
cultivated. In the print era, publication was expensive, and there
wasn't enough demand for essays to publish that many. You could
publish essays if you were already well known for writing something
else, like novels. Or you could write book reviews that you took
over to express your own ideas. But there was not really a direct
path to becoming an essayist. Which meant few essays got written,
and those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish
essays online. You start in obscurity, perhaps, but at least you
can start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for
years, till some change makes it explode. Cryptography did this to
number theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but
that there's a lot left to discover. There's a certain kind of idea
that's best discovered by writing essays. If most essays are still
unwritten, most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for
publication. I wrote several to figure out what Y Combinator should
do, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and
Robert Morris for reading drafts of this.February 2007A few days ago I finally figured out something I've wondered about
for 25 years: the relationship between wisdom and intelligence.
Anyone can see they're not the same by the number of people who are
smart, but not very wise.  And yet intelligence and wisdom do seem
related.  How?What is wisdom?  I'd say it's knowing what to do in a lot of
situations.  I'm not trying to make a deep point here about the
true nature of wisdom, just to figure out how we use the word.  A
wise person is someone who usually knows the right thing to do.And yet isn't being smart also knowing what to do in certain
situations?  For example, knowing what to do when the teacher tells
your elementary school class to add all the numbers from 1 to 100?
[1]Some say wisdom and intelligence apply to different types of
problems—wisdom to human problems and intelligence to abstract
ones.  But that isn't true.  Some wisdom has nothing to do with
people: for example, the wisdom of the engineer who knows certain
structures are less prone to failure than others.  And certainly
smart people can find clever solutions to human problems as well
as abstract ones. 
[2]Another popular explanation is that wisdom comes from experience
while intelligence is innate.  But people are not simply wise in
proportion to how much experience they have.  Other things must
contribute to wisdom besides experience, and some may be innate: a
reflective disposition, for example.Neither of the conventional explanations of the difference between
wisdom and intelligence stands up to scrutiny.  So what is the
difference?  If we look at how people use the words "wise" and
"smart," what they seem to mean is different shapes of performance.Curve"Wise" and "smart" are both ways of saying someone knows what to
do.  The difference is that "wise" means one has a high average
outcome across all situations, and "smart" means one does spectacularly
well in a few.  That is, if you had a graph in which the x axis
represented situations and the y axis the outcome, the graph of the
wise person would be high overall, and the graph of the smart person
would have high peaks.The distinction is similar to the rule that one should judge talent
at its best and character at its worst.  Except you judge intelligence
at its best, and wisdom by its average.  That's how the two are
related: they're the two different senses in which the same curve
can be high.So a wise person knows what to do in most situations, while a smart
person knows what to do in situations where few others could.  We
need to add one more qualification: we should ignore cases where
someone knows what to do because they have inside information. 
[3]
But aside from that, I don't think we can get much more specific
without starting to be mistaken.Nor do we need to.  Simple as it is, this explanation predicts, or
at least accords with, both of the conventional stories about the
distinction between wisdom and intelligence.  Human problems are
the most common type, so being good at solving those is key in
achieving a high average outcome.   And it seems natural that a
high average outcome depends mostly on experience, but that dramatic
peaks can only be achieved by people with certain rare, innate
qualities; nearly anyone can learn to be a good swimmer, but to be
an Olympic swimmer you need a certain body type.This explanation also suggests why wisdom is such an elusive concept:
there's no such thing.  "Wise" means something—that one is
on average good at making the right choice.  But giving the name
"wisdom" to the supposed quality that enables one to do that doesn't
mean such a thing exists.  To the extent "wisdom" means anything,
it refers to a grab-bag of qualities as various as self-discipline,
experience, and empathy.  
[4]Likewise, though "intelligent" means something, we're asking for
trouble if we insist on looking for a single thing called "intelligence."
And whatever its components, they're not all innate.  We use the
word "intelligent" as an indication of ability: a smart person can
grasp things few others could.  It does seem likely there's some
inborn predisposition to intelligence (and wisdom too), but this
predisposition is not itself intelligence.One reason we tend to think of intelligence as inborn is that people
trying to measure it have concentrated on the aspects of it that
are most measurable.  A quality that's inborn will obviously be
more convenient to work with than one that's influenced by experience,
and thus might vary in the course of a study.  The problem comes
when we drag the word "intelligence" over onto what they're measuring.
If they're measuring something inborn, they can't be measuring
intelligence.  Three year olds aren't smart.   When we describe one
as smart, it's shorthand for "smarter than other three year olds."SplitPerhaps it's a technicality to point out that a predisposition to
intelligence is not the same as intelligence.  But it's an important
technicality, because it reminds us that we can become smarter,
just as we can become wiser.The alarming thing is that we may have to choose between the two.If wisdom and intelligence are the average and peaks of the same
curve, then they converge as the number of points on the curve
decreases.  If there's just one point, they're identical: the average
and maximum are the same.  But as the number of points increases,
wisdom and intelligence diverge.  And historically the number of
points on the curve seems to have been increasing: our ability is
tested in an ever wider range of situations.In the time of Confucius and Socrates, people seem to have regarded
wisdom, learning, and intelligence as more closely related than we
do.  Distinguishing between "wise" and "smart" is a modern habit.
[5]
And the reason we do is that they've been diverging.  As knowledge
gets more specialized, there are more points on the curve, and the
distinction between the spikes and the average becomes sharper,
like a digital image rendered with more pixels.One consequence is that some old recipes may have become obsolete.
At the very least we have to go back and figure out if they were
really recipes for wisdom or intelligence.  But the really striking
change, as intelligence and wisdom drift apart, is that we may have
to decide which we prefer.  We may not be able to optimize for both
simultaneously.Society seems to have voted for intelligence.  We no longer admire
the sage—not the way people did two thousand years ago.  Now
we admire the genius.  Because in fact the distinction we began
with has a rather brutal converse: just as you can be smart without
being very wise, you can be wise without being very smart.  That
doesn't sound especially admirable.  That gets you James Bond, who
knows what to do in a lot of situations, but has to rely on Q for
the ones involving math.Intelligence and wisdom are obviously not mutually exclusive.  In
fact, a high average may help support high peaks.  But there are
reasons to believe that at some point you have to choose between
them.  One is the example of very smart people, who are so often
unwise that in popular culture this now seems to be regarded as the
rule rather than the exception.  Perhaps the absent-minded professor
is wise in his way, or wiser than he seems, but he's not wise in
the way Confucius or Socrates wanted people to be. 
[6]NewFor both Confucius and Socrates, wisdom, virtue, and happiness were
necessarily related.  The wise man was someone who knew what the
right choice was and always made it; to be the right choice, it had
to be morally right; he was therefore always happy, knowing he'd
done the best he could.  I can't think of many ancient philosophers
who would have disagreed with that, so far as it goes."The superior man is always happy; the small man sad," said Confucius.
[7]Whereas a few years ago I read an interview with a mathematician
who said that most nights he went to bed discontented, feeling he
hadn't made enough progress.  
[8]
The Chinese and Greek words we
translate as "happy" didn't mean exactly what we do by it, but
there's enough overlap that this remark contradicts them.Is the mathematician a small man because he's discontented?  No;
he's just doing a kind of work that wasn't very common in Confucius's
day.Human knowledge seems to grow fractally.  Time after time, something
that seemed a small and uninteresting area—experimental error,
even—turns out, when examined up close, to have as much in
it as all knowledge up to that point.  Several of the fractal buds
that have exploded since ancient times involve inventing and
discovering new things.  Math, for example, used to be something a
handful of people did part-time.  Now it's the career of thousands.
And in work that involves making new things, some old rules don't
apply.Recently I've spent some time advising people, and there I find the
ancient rule still works: try to understand the situation as well
as you can, give the best advice you can based on your experience,
and then don't worry about it, knowing you did all you could.  But
I don't have anything like this serenity when I'm writing an essay.
Then I'm worried.  What if I run out of ideas?  And when I'm writing,
four nights out of five I go to bed discontented, feeling I didn't
get enough done.Advising people and writing are fundamentally different types of
work.  When people come to you with a problem and you have to figure
out the right thing to do, you don't (usually) have to invent
anything.  You just weigh the alternatives and try to judge which
is the prudent choice.  But prudence can't tell me what sentence
to write next.  The search space is too big.Someone like a judge or a military officer can in much of his work
be guided by duty, but duty is no guide in making things.  Makers
depend on something more precarious: inspiration.  And like most
people who lead a precarious existence, they tend to be worried,
not contented.  In that respect they're more like the small man of
Confucius's day, always one bad harvest (or ruler) away from
starvation. Except instead of being at the mercy of weather and
officials, they're at the mercy of their own imagination.LimitsTo me it was a relief just to realize it might be ok to be discontented.
The idea that a successful person should be happy has thousands of
years of momentum behind it.  If I was any good, why didn't I have
the easy confidence winners are supposed to have?  But that, I now
believe, is like a runner asking "If I'm such a good athlete, why
do I feel so tired?" Good runners still get tired; they just get
tired at higher speeds.People whose work is to invent or discover things are in the same
position as the runner.  There's no way for them to do the best
they can, because there's no limit to what they could do.  The
closest you can come is to compare yourself to other people.  But
the better you do, the less this matters.  An undergrad who gets
something published feels like a star.  But for someone at the top
of the field, what's the test of doing well?  Runners can at least
compare themselves to others doing exactly the same thing; if you
win an Olympic gold medal, you can be fairly content, even if you
think you could have run a bit faster.  But what is a novelist to
do?Whereas if you're doing the kind of work in which problems are
presented to you and you have to choose between several alternatives,
there's an upper bound on your performance: choosing the best every
time.  In ancient societies, nearly all work seems to have been of
this type.  The peasant had to decide whether a garment was worth
mending, and the king whether or not to invade his neighbor, but
neither was expected to invent anything.  In principle they could
have; the king could have invented firearms, then invaded his
neighbor.  But in practice innovations were so rare that they weren't
expected of you, any more than goalkeepers are expected to score
goals. 
[9]
In practice, it seemed as if there was a correct decision
in every situation, and if you made it you'd done your job perfectly,
just as a goalkeeper who prevents the other team from scoring is
considered to have played a perfect game.In this world, wisdom seemed paramount.  
[10]
Even now, most people
do work in which problems are put before them and they have to
choose the best alternative.  But as knowledge has grown more
specialized, there are more and more types of work in which people
have to make up new things, and in which performance is therefore
unbounded.  Intelligence has become increasingly important relative
to wisdom because there is more room for spikes.RecipesAnother sign we may have to choose between intelligence and wisdom
is how different their recipes are.  Wisdom seems to come largely
from curing childish qualities, and intelligence largely from
cultivating them.Recipes for wisdom, particularly ancient ones, tend to have a
remedial character.  To achieve wisdom one must cut away all the
debris that fills one's head on emergence from childhood, leaving
only the important stuff.  Both self-control and experience have
this effect: to eliminate the random biases that come from your own
nature and from the circumstances of your upbringing respectively.
That's not all wisdom is, but it's a large part of it.  Much of
what's in the sage's head is also in the head of every twelve year
old.  The difference is that in the head of the twelve year old
it's mixed together with a lot of random junk.The path to intelligence seems to be through working on hard problems.
You develop intelligence as you might develop muscles, through
exercise.  But there can't be too much compulsion here.  No amount
of discipline can replace genuine curiosity.  So cultivating
intelligence seems to be a matter of identifying some bias in one's
character—some tendency to be interested in certain types of
things—and nurturing it.  Instead of obliterating your
idiosyncrasies in an effort to make yourself a neutral vessel for
the truth, you select one and try to grow it from a seedling into
a tree.The wise are all much alike in their wisdom, but very smart people
tend to be smart in distinctive ways.Most of our educational traditions aim at wisdom. So perhaps one
reason schools work badly is that they're trying to make intelligence
using recipes for wisdom.  Most recipes for wisdom have an element
of subjection.  At the very least, you're supposed to do what the
teacher says.  The more extreme recipes aim to break down your
individuality the way basic training does.  But that's not the route
to intelligence.  Whereas wisdom comes through humility, it may
actually help, in cultivating intelligence, to have a mistakenly
high opinion of your abilities, because that encourages you to keep
working.  Ideally till you realize how mistaken you were.(The reason it's hard to learn new skills late in life is not just
that one's brain is less malleable.  Another probably even worse
obstacle is that one has higher standards.)I realize we're on dangerous ground here.  I'm not proposing the
primary goal of education should be to increase students' "self-esteem."
That just breeds laziness.  And in any case, it doesn't really fool
the kids, not the smart ones.  They can tell at a young age that a
contest where everyone wins is a fraud.A teacher has to walk a narrow path: you want to encourage kids to
come up with things on their own, but you can't simply applaud
everything they produce.  You have to be a good audience: appreciative,
but not too easily impressed.  And that's a lot of work.  You have
to have a good enough grasp of kids' capacities at different ages
to know when to be surprised.That's the opposite of traditional recipes for education.  Traditionally
the student is the audience, not the teacher; the student's job is
not to invent, but to absorb some prescribed body of material.  (The
use of the term "recitation" for sections in some colleges is a
fossil of this.) The problem with these old traditions is that
they're too much influenced by recipes for wisdom.DifferentI deliberately gave this essay a provocative title; of course it's
worth being wise.  But I think it's important to understand the
relationship between intelligence and wisdom, and particularly what
seems to be the growing gap between them.  That way we can avoid
applying rules and standards to intelligence that are really meant
for wisdom.  These two senses of "knowing what to do" are more
different than most people realize.  The path to wisdom is through
discipline, and the path to intelligence through carefully selected
self-indulgence.  Wisdom is universal, and intelligence idiosyncratic.
And while wisdom yields calmness, intelligence much of the time
leads to discontentment.That's particularly worth remembering.  A physicist friend recently
told me half his department was on Prozac.  Perhaps if we acknowledge
that some amount of frustration is inevitable in certain kinds
of work, we can mitigate its effects.  Perhaps we can box it up and
put it away some of the time, instead of letting it flow together
with everyday sadness to produce what seems an alarmingly large
pool.  At the very least, we can avoid being discontented about
being discontented.If you feel exhausted, it's not necessarily because there's something
wrong with you.  Maybe you're just running fast.Notes[1]
Gauss was supposedly asked this when he was 10.  Instead of
laboriously adding together the numbers like the other students,
he saw that they consisted of 50 pairs that each summed to 101 (100
+ 1, 99 + 2, etc), and that he could just multiply 101 by 50 to get
the answer, 5050.[2]
A variant is that intelligence is the ability to solve problems,
and wisdom the judgement to know how to use those solutions.   But
while this is certainly an important relationship between wisdom
and intelligence, it's not the distinction between them.  Wisdom
is useful in solving problems too, and intelligence can help in
deciding what to do with the solutions.[3]
In judging both intelligence and wisdom we have to factor out
some knowledge. People who know the combination of a safe will be
better at opening it than people who don't, but no one would say
that was a test of intelligence or wisdom.But knowledge overlaps with wisdom and probably also intelligence.
A knowledge of human nature is certainly part of wisdom.  So where
do we draw the line?Perhaps the solution is to discount knowledge that at some point
has a sharp drop in utility.  For example, understanding French
will help you in a large number of situations, but its value drops
sharply as soon as no one else involved knows French.  Whereas the
value of understanding vanity would decline more gradually.The knowledge whose utility drops sharply is the kind that has
little relation to other knowledge.  This includes mere conventions,
like languages and safe combinations, and also what we'd call
"random" facts, like movie stars' birthdays, or how to distinguish
1956 from 1957 Studebakers.[4]
People seeking some single thing called "wisdom" have been
fooled by grammar.  Wisdom is just knowing the right thing to do,
and there are a hundred and one different qualities that help in
that.  Some, like selflessness, might come from meditating in an
empty room, and others, like a knowledge of human nature, might
come from going to drunken parties.Perhaps realizing this will help dispel the cloud of semi-sacred
mystery that surrounds wisdom in so many people's eyes.  The mystery
comes mostly from looking for something that doesn't exist.  And
the reason there have historically been so many different schools
of thought about how to achieve wisdom is that they've focused on
different components of it.When I use the word "wisdom" in this essay, I mean no more than
whatever collection of qualities helps people make the right choice
in a wide variety of situations.[5]
Even in English, our sense of the word "intelligence" is
surprisingly recent.  Predecessors like "understanding" seem to
have had a broader meaning.[6]
There is of course some uncertainty about how closely the remarks
attributed to Confucius and Socrates resemble their actual opinions.
I'm using these names as we use the name "Homer," to mean the
hypothetical people who said the things attributed to them.[7]
Analects VII:36, Fung trans.Some translators use "calm" instead of "happy."  One source of
difficulty here is that present-day English speakers have a different
idea of happiness from many older societies.  Every language probably
has a word meaning "how one feels when things are going well," but
different cultures react differently when things go well.  We react
like children, with smiles and laughter.  But in a more reserved
society, or in one where life was tougher, the reaction might be a
quiet contentment.[8]
It may have been Andrew Wiles, but I'm not sure.  If anyone
remembers such an interview, I'd appreciate hearing from you.[9]
Confucius claimed proudly that he had never invented
anything—that he had simply passed on an accurate account of
ancient traditions.  [Analects VII:1] It's hard for us now to
appreciate how important a duty it must have been in preliterate
societies to remember and pass on the group's accumulated knowledge.
Even in Confucius's time it still seems to have been the first duty
of the scholar.[10]
The bias toward wisdom in ancient philosophy may be exaggerated
by the fact that, in both Greece and China, many of the first
philosophers (including Confucius and Plato) saw themselves as
teachers of administrators, and so thought disproportionately about
such matters.  The few people who did invent things, like storytellers,
must have seemed an outlying data point that could be ignored.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston,
and Robert Morris for reading drafts of this.January 2003(This article is derived from a keynote talk at the fall 2002 meeting
of NEPLS.)Visitors to this country are often surprised to find that
Americans like to begin a conversation by asking "what do you do?"
I've never liked this question.  I've rarely had a
neat answer to it.  But I think I have finally solved the problem.
Now, when someone asks me what I do, I look them straight
in the eye and say "I'm designing a 
new dialect of Lisp."   
I recommend this answer to anyone who doesn't like being asked what
they do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.
I'm just designing one, in the same way that someone might design
a building or a chair or a new typeface.
I'm not trying to discover anything new.  I just want
to make a language that will be good to program in.  In some ways,
this assumption makes life a lot easier.The difference between design and research seems to be a question
of new versus good.  Design doesn't have to be new, but it has to  
be good.  Research doesn't have to be good, but it has to be new.
I think these two paths converge at the top: the best design
surpasses its predecessors by using new ideas, and the best research
solves problems that are not only new, but actually worth solving.
So ultimately we're aiming for the same destination, just approaching
it from different directions.What I'm going to talk about today is what your target looks like
from the back.  What do you do differently when you treat
programming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.
Design begins by asking, who is this
for and what do they need from it?  A good architect,
for example, does not begin by creating a design that he then
imposes on the users, but by studying the intended users and figuring
out what they need.Notice I said "what they need," not "what they want."  I don't mean
to give the impression that working as a designer means working as 
a sort of short-order cook, making whatever the client tells you
to.  This varies from field to field in the arts, but
I don't think there is any field in which the best work is done by
the people who just make exactly what the customers tell them to.The customer is always right in
the sense that the measure of good design is how well it works
for the user.  If you make a novel that bores everyone, or a chair
that's horribly uncomfortable to sit in, then you've done a bad
job, period.  It's no defense to say that the novel or the chair  
is designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making
what the user tells you to.  Users don't know what all the choices
are, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design
for the user, but you have to design what the user needs, not simply  
what he says he wants.
It's much like being a doctor.  You can't just treat a patient's
symptoms.  When a patient tells you his symptoms, you have to figure
out what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the
practice of good design can be derived, and around which most design
issues center.If good design must do what the user needs, who is the user?  When
I say that design must be for users, I don't mean to imply that good 
design aims at some kind of  
lowest common denominator.  You can pick any group of users you
want.  If you're designing a tool, for example, you can design it
for anyone from beginners to experts, and what's good design
for one group might be bad for another.  The point
is, you have to pick some group of users.  I don't think you can
even talk about good or bad design except with
reference to some intended user.You're most likely to get good design if the intended users include
the designer himself.  When you design something
for a group that doesn't include you, it tends to be for people
you consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,
seems inevitably to corrupt the designer.
I suspect that very few housing
projects in the US were designed by architects who expected to live
in them.   You can see the same thing
in programming languages.  C, Lisp, and Smalltalk were created for
their own designers to use.  Cobol, Ada, and Java, were created   
for other people to use.If you think you're designing something for idiots, the odds are
that you're not designing something good, even for idiots.
Even if you're designing something for the most sophisticated
users, though, you're still designing for humans.  It's different 
in research.  In math you
don't choose abstractions because they're
easy for humans to understand; you choose whichever make the
proof shorter.  I think this is true for the sciences generally.
Scientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is
all about people.  The human body is a strange
thing, but when you're designing a chair,
that's what you're designing for, and there's no way around it.
All the arts have to pander to the interests and limitations
of humans.   In painting, for example, all other things being
equal a painting with people in it will be more interesting than
one without.  It is not merely an accident of history that
the great paintings of the Renaissance are all full of people.
If they hadn't been, painting as a medium wouldn't have the prestige
that it does.Like it or not, programming languages are also for people,
and I suspect the human brain is just as lumpy and idiosyncratic
as the human body.  Some ideas are easy for people to grasp
and some aren't.  For example, we seem to have a very limited
capacity for dealing with detail.  It's this fact that makes
programing languages a good idea in the first place; if we
could handle the detail, we could just program in machine
language.Remember, too, that languages are not
primarily a form for finished programs, but something that
programs have to be developed in.  Anyone in the arts could
tell you that you might want different mediums for the
two situations.  Marble, for example, is a nice, durable
medium for finished ideas, but a hopelessly inflexible one
for developing new ideas.A program, like a proof,
is a pruned version of a tree that in the past has had
false starts branching off all over it.  So the test of
a language is not simply how clean the finished program looks
in it, but how clean the path to the finished program was.
A design choice that gives you elegant finished programs
may not give you an elegant design process.  For example, 
I've written a few macro-defining macros full of nested
backquotes that look now like little gems, but writing them
took hours of the ugliest trial and error, and frankly, I'm still
not entirely sure they're correct.We often act as if the test of a language were how good
finished programs look in it.
It seems so convincing when you see the same program
written in two languages, and one version is much shorter.
When you approach the problem from the direction of the
arts, you're less likely to depend on this sort of
test.  You don't want to end up with a programming
language like marble.For example, it is a huge win in developing software to
have an interactive toplevel, what in Lisp is called a
read-eval-print loop.  And when you have one this has
real effects on the design of the language.  It would not
work well for a language where you have to declare
variables before using them, for example.  When you're
just typing expressions into the toplevel, you want to be 
able to set x to some value and then start doing things
to x.  You don't want to have to declare the type of x
first.  You may dispute either of the premises, but if
a language has to have a toplevel to be convenient, and
mandatory type declarations are incompatible with a
toplevel, then no language that makes type declarations  
mandatory could be convenient to program in.In practice, to get good design you have to get close, and stay
close, to your users.  You have to calibrate your ideas on actual
users constantly, especially in the beginning.  One of the reasons
Jane Austen's novels are so good is that she read them out loud to
her family.  That's why she never sinks into self-indulgently arty
descriptions of landscapes,
or pretentious philosophizing.  (The philosophy's there, but it's
woven into the story instead of being pasted onto it like a label.)
If you open an average "literary" novel and imagine reading it out loud
to your friends as something you'd written, you'll feel all too
keenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.
Actually, there are several ideas mixed together in the concept of
Worse is Better, which is why people are still arguing about
whether worse
is actually better or not.  But one of the main ideas in that
mix is that if you're building something new, you should get a
prototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.
Instead of getting a prototype out quickly and gradually refining
it, you try to create the complete, finished, product in one long
touchdown pass.  As far as I know, this is a
recipe for disaster.  Countless startups destroyed themselves this
way during the Internet bubble.  I've never heard of a case
where it worked.What people outside the software world may not realize is that
Worse is Better is found throughout the arts.
In drawing, for example, the idea was discovered during the
Renaissance.  Now almost every drawing teacher will tell you that
the right way to get an accurate drawing is not to
work your way slowly around the contour of an object, because errors will
accumulate and you'll find at the end that the lines don't meet.
Instead you should draw a few quick lines in roughly the right place,
and then gradually refine this initial sketch.In most fields, prototypes
have traditionally been made out of different materials.
Typefaces to be cut in metal were initially designed  
with a brush on paper.  Statues to be cast in bronze   
were modelled in wax.  Patterns to be embroidered on tapestries
were drawn on paper with ink wash.  Buildings to be
constructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it
first became popular in the fifteenth century, was that you
could actually make the finished work from the prototype.
You could make a preliminary drawing if you wanted to, but you
weren't held to it; you could work out all the details, and
even make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to
be just a model; you can refine it into the finished product.
I think you should always do this when you can.  It lets you
take advantage of new insights you have along the way.  But
perhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people
don't talk more about it.  One of my first
drawing teachers told me: if you're bored when you're
drawing something, the drawing will look boring.
For example, suppose you have to draw a building, and you
decide to draw each brick individually.  You can do this
if you want, but if you get bored halfway through and start
making the bricks mechanically instead of observing each one,   
the drawing will look worse than if you had merely suggested
the bricks.Building something by gradually refining a prototype is good
for morale because it keeps you engaged.  In software, my  
rule is: always have working code.  If you're writing
something that you'll be able to test in an hour, then you
have the prospect of an immediate reward to motivate you.
The same is true in the arts, and particularly in oil painting.
Most painters start with a blurry sketch and gradually
refine it.
If you work this way, then in principle
you never have to end the day with something that actually
looks unfinished.  Indeed, there is even a saying among
painters: "A painting is never finished, you just stop
working on it."  This idea will be familiar to anyone who
has worked on software.Morale is another reason that it's hard to design something
for an unsophisticated user.   It's hard to stay interested in
something you don't like yourself.  To make something  
good, you have to be thinking, "wow, this is really great,"
not "what a piece of shit; those fools will love it."Design means making things for humans.  But it's not just the
user who's human.  The designer is human too.Notice all this time I've been talking about "the designer."
Design usually has to be under the control of a single person to
be any good.   And yet it seems to be possible for several people
to collaborate on a research project.  This seems to
me one of the most interesting differences between research and
design.There have been famous instances of collaboration in the arts,
but most of them seem to have been cases of molecular bonding rather
than nuclear fusion.  In an opera it's common for one person to
write the libretto and another to write the music.   And during the Renaissance, 
journeymen from northern
Europe were often employed to do the landscapes in the
backgrounds of Italian paintings.  But these aren't true collaborations.
They're more like examples of Robert Frost's
"good fences make good neighbors."  You can stick instances
of good design together, but within each individual project,
one person has to be in control.I'm not saying that good design requires that one person think
of everything.  There's nothing more valuable than the advice
of someone whose judgement you trust.  But after the talking is
done, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  
design can't?  This is an interesting question.  I don't 
know the answer.  Perhaps,
if design and research converge, the best research is also
good design, and in fact can't be done by collaborators.
A lot of the most famous scientists seem to have worked alone.
But I don't know enough to say whether there
is a pattern here.  It could be simply that many famous scientists
worked when collaboration was less common.Whatever the story is in the sciences, true collaboration
seems to be vanishingly rare in the arts.  Design by committee is a
synonym for bad design.  Why is that so?  Is there some way to
beat this limitation?I'm inclined to think there isn't-- that good design requires
a dictator.  One reason is that good design has to   
be all of a piece.  Design is not just for humans, but
for individual humans.  If a design represents an idea that  
fits in one person's head, then the idea will fit in the user's
head too.Related:May 2004When people care enough about something to do it well, those who
do it best tend to be far better than everyone else.  There's a
huge gap between Leonardo and second-rate contemporaries like
Borgognone.  You see the same gap between Raymond Chandler and the
average writer of detective novels.  A top-ranked professional chess
player could play ten thousand games against an ordinary club player
without losing once.Like chess or painting or writing novels, making money is a very
specialized skill.   But for some reason we treat this skill
differently.  No one complains when a few people surpass all the
rest at playing chess or writing novels, but when a few people make
more money than the rest, we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other
skill.  What causes people to react so strongly when the skill is
making money?I think there are three reasons we treat making money as different:
the misleading model of wealth we learn as children; the disreputable
way in which, till recently, most fortunes were accumulated; and
the worry that great variations in income are somehow bad for
society.  As far as I can tell, the first is mistaken, the second
outdated, and the third empirically false.  Could it be that, in a
modern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric
sockets.  I didn't realize there were power plants out there
generating it.  Likewise, it doesn't occur to most kids that wealth
is something that has to be generated.  It seems to be something
that flows from parents.Because of the circumstances in which they encounter it, children
tend to misunderstand wealth.  They confuse it with money.  They
think that there is a fixed amount of it.  And they think of it as
something that's distributed by authorities (and so should be
distributed equally), rather than something that has to be created
(and might be created unequally).In fact, wealth is not money.  Money is just a convenient way of
trading one form of wealth for another.  Wealth is the underlying
stuff—the goods and services we buy.  When you travel to a
rich or poor country, you don't have to look at people's bank
accounts to tell which kind you're in.  You can see
wealth—in buildings and streets, in the clothes and the health
of the people.Where does wealth come from?  People make it.  This was easier to
grasp when most people lived on farms, and made many of the things
they wanted with their own hands.  Then you could see in the house,
the herds, and the granary the wealth that each family created.  It
was obvious then too that the wealth of the world was not a fixed
quantity that had to be shared out, like slices of a pie.  If you
wanted more wealth, you could make it.This is just as true today, though few of us create wealth directly
for ourselves (except for a few vestigial domestic tasks).  Mostly
we create wealth for other people in exchange for money, which we
then trade for the forms of wealth we want. 
[1]Because kids are unable to create wealth, whatever they have has
to be given to them.  And when wealth is something you're given,
then of course it seems that it should be distributed equally.
[2]
As in most families it is.  The kids see to that.  "Unfair," they
cry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you
want something, you either have to make it, or do something of
equivalent value for someone else, in order to get them to give you
enough money to buy it.  In the real world, wealth is (except for
a few specialists like thieves and speculators) something you have
to create, not something that's distributed by Daddy.  And since
the ability and desire to create it vary from person to person,
it's not made equally.You get paid by doing or making something people want, and those
who make more money are often simply better at doing what people
want.  Top actors make a lot more money than B-list actors.  The
B-list actors might be almost as charismatic, but when people go
to the theater and look at the list of movies playing, they want
that extra oomph that the big stars have.Doing what people want is not the only way to get money, of course.
You could also rob banks, or solicit bribes, or establish a monopoly.
Such tricks account for some variation in wealth, and indeed for
some of the biggest individual fortunes, but they are not the root
cause of variation in income.  The root cause of variation in income,
as Occam's Razor implies, is the same as the root cause of variation
in every other human skill.In the United States, the CEO of a large public company makes about
100 times as much as the average person. 
[3]
Basketball players
make about 128 times as much, and baseball players 72 times as much.
Editorials quote this kind of statistic with horror.  But I have
no trouble imagining that one person could be 100 times as productive
as another.  In ancient Rome the price of slaves varied by
a factor of 50 depending on their skills. 
[4]
And that's without
considering motivation, or the extra leverage in productivity that
you can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early
Christian writers, arguing from first principles about whether the
Earth was round, when they could just walk outside and check.
[5]
How much someone's work is worth is not a policy question.  It's
something the market already determines."Are they really worth 100 of us?" editorialists ask.  Depends on
what you mean by worth.  If you mean worth in the sense of what
people will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there
not others whose incomes really do reflect the wealth they generate?
Steve Jobs saved a company that was in a terminal decline.  And not
merely in the way a turnaround specialist does, by cutting costs;
he had to decide what Apple's next products should be.  Few others
could have done it.  And regardless of the case with CEOs, it's
hard to see how anyone could argue that the salaries of professional
basketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really
generate so much more wealth than another.  The key to this mystery
is to revisit that question, are they really worth 100 of us?
Would a basketball team trade one of their players for 100
random people?  What would Apple's next product look like if you
replaced Steve Jobs with a committee of 100 random people? 
[6]
These
things don't scale linearly.<|begin_of_text|>
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
  Perhaps the CEO or the professional
athlete has only ten times (whatever that means) the skill and
determination of an ordinary person.  But it makes all the difference
that it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid,
what are we really saying?  In a free market, prices are determined
by what buyers want.  People like baseball more than  poetry, so
baseball players make more than poets.  To say that a certain kind
of work is underpaid is thus identical with saying that people want
the wrong things.Well, of course people want the wrong things.  It seems odd to be
surprised by that.  And it seems even odder to say that it's
unjust that certain kinds of work are underpaid. 
[7]
Then
you're saying that it's unjust that people want the wrong things.
It's  lamentable that people prefer reality TV and corndogs to
Shakespeare and steamed vegetables, but unjust?  That seems like
saying that blue is heavy, or that up is circular.The appearance of the word "unjust" here is the unmistakable spectral
signature of the Daddy Model.  Why else would this idea occur in
this odd context?  Whereas if the speaker were still operating on
the Daddy Model, and saw wealth as something that flowed from a
common source and had to be shared out, rather than something
generated by doing what other people wanted, this is exactly what
you'd get on noticing that some people made much more than others.When we talk about "unequal distribution of income," we should
also ask, where does that income come from?
[8]
Who made the wealth
it represents?  Because to the extent that income varies simply
according to how much wealth people create, the distribution may
be unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth
alarming is that for most of human history the usual way to accumulate
a fortune was to steal it: in pastoral societies by cattle raiding;
in agricultural societies by appropriating others' estates in times
of war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates
confiscated from the losers.  In England in the 1060s, when William
the Conqueror distributed the estates of the defeated Anglo-Saxon
nobles to his followers, the conflict was military.  By the 1530s,
when Henry VIII distributed the estates of the monasteries to his
followers, it was mostly political. 
[9]
But the principle was the
same.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials
used taxation instead of confiscation.  But here too we see the
same principle: the way to get rich was not to create wealth, but
to serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class.
Now we think of the middle class as people who are neither rich nor
poor, but originally they were a distinct group.  In a feudal
society, there are just two classes: a warrior aristocracy, and the
serfs who work their estates.  The middle class were a new, third
group who lived in towns and supported themselves by manufacturing
and trade.Starting in the tenth and eleventh centuries, petty nobles and
former serfs banded together in towns that gradually became powerful
enough to ignore the local feudal lords. 
[10]
Like serfs, the middle
class made a living largely by creating wealth.  (In port cities
like Genoa and Pisa, they also engaged in piracy.) But unlike serfs
they had an incentive to create a lot of it.  Any wealth a serf
created belonged to his master.  There was not much point in making
more than you could hide.  Whereas the independence of the townsmen
allowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as
a whole started to get richer very rapidly.  Nearly everything we
have was created by the middle class.  Indeed, the other two classes
have effectively disappeared in industrial societies, and their
names been given to either end of the middle class.  (In the original
sense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation
definitively replaced corruption as the best way to get rich.  In
England, at least, corruption only became unfashionable (and in
fact only started to be called "corruption") when there started to
be other, faster ways to get rich.Seventeenth-century England was much like the third world today,
in that government office was a recognized route to wealth.  The
great fortunes of that time still derived more from what we would
now call corruption than from commerce. 
[11]
By the nineteenth
century that had changed.  There continued to be bribes, as there
still are everywhere, but politics had by then been left to men who
were driven more by vanity than greed.  Technology had made it
possible to create wealth faster than you could steal it.  The
prototypical rich man of the nineteenth century was not a courtier
but an industrialist.With the rise of the middle class, wealth stopped being a zero-sum
game.  Jobs and Wozniak didn't have to make us poor to make themselves
rich.  Quite the opposite: they created things that made our lives
materially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth
was to steal it, we tend to be suspicious of rich people.  Idealistic
undergraduates find their unconsciously preserved child's model of
wealth confirmed by eminent writers of the past.  It is a case of
the mistaken meeting the outdated."Behind every great fortune, there is a crime," Balzac wrote.  Except
he didn't.  What he actually said was that a great fortune with no
apparent cause was probably due to a crime well enough executed
that it had been forgotten.  If we were talking about Europe in
1000, or most of the third world today, the standard misquotation
would be spot on.  But Balzac lived in nineteenth-century France,
where the Industrial Revolution was well advanced.  He knew you
could make a fortune without stealing it.  After all, he did himself,
as a popular novelist.
[12]Only a few countries (by no coincidence, the richest ones) have
reached this stage.  In most, corruption still has the upper hand.
In most, the fastest way to get wealth is by stealing it.  And so
when we see increasing differences in income in a rich country,
there is a tendency to worry that it's sliding back toward becoming
another Venezuela.  I think the opposite is happening. I think
you're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?  It will
certainly increase the gap between the productive and the unproductive.
That's the whole point of technology.   With a tractor an energetic
farmer could plow six times as much land in a day as he could with
a team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In
high school I made money by mowing lawns and scooping ice cream at
Baskin-Robbins.  This was the only kind of work available at the
time.  Now high school kids could write software or design web
sites.  But only some of them will; the rest will still be scooping
ice cream.I remember very vividly when in 1985 improved technology made it
possible for me to buy a computer of my own.  Within months I was
using it to make money as a freelance programmer.  A few years
before, I couldn't have done this.  A few years before, there was
no such thing as a freelance programmer.  But Apple created
wealth, in the form of powerful, inexpensive computers, and programmers
immediately set to work using it to create more.As this example suggests, the rate at which technology increases
our productive capacity is probably exponential, rather than linear.
So we should expect to see ever-increasing variation in individual
productivity as time goes on.   Will that increase the gap between
rich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to
decrease other gaps.  A hundred years ago, the rich led a different
kind of life from ordinary people.  They lived in houses
full of