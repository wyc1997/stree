<|begin_of_text|><|begin_of_text|>January 2017People who are powerful but uncharismatic will tend to be disliked.
Their power makes them a target for criticism that they don't have
the charisma to disarm. That was Hillary Clinton's problem. It also
tends to be a problem for any CEO who is more of a builder than a
schmoozer. And yet the builder-type CEO is (like Hillary) probably
the best person for the job.I don't think there is any solution to this problem. It's human
nature. The best we can do is to recognize that it's happening, and
to understand that being a magnet for criticism is sometimes a sign
not that someone is the wrong person for a job, but that they're
the right one.November 2021(This essay is derived from a talk at the Cambridge Union.)When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kind. The strongest proof
would be to show that taste exists in the narrowest sense, so I'm
going to talk about taste in art. You have better taste than me if
the art you like is better than the art I like.If there's no such thing as good taste, then there's no such thing
as good art. Because if there is such a
thing as good art, it's
easy to tell which of two people has better taste. Show them a lot
of works by artists they've never seen before and ask them to
choose the best, and whoever chooses the better art has better
taste.So if you want to discard the concept of good taste, you also have
to discard the concept of good art. And that means you have to
discard the possibility of people being good at making it. Which
means there's no way for artists to be good at their jobs. And not
just visual artists, but anyone who is in any sense an artist. You
can't have good actors, or novelists, or composers, or dancers
either. You can have popular novelists, but not good ones.We don't realize how far we'd have to go if we discarded the concept
of good taste, because we don't even debate the most obvious cases.
But it doesn't just mean we can't say which of two famous painters
is better. It means we can't say that any painter is better than a
randomly chosen eight year old.That was how I realized my father was wrong. I started studying
painting. And it was just like other kinds of work I'd done: you
could do it well, or badly, and if you tried hard, you could get
better at it. And it was obvious that Leonardo and Bellini were
much better at it than me. That gap between us was not imaginary.
They were so good. And if they could be good, then art could be
good, and there was such a thing as good taste after all.Now that I've explained how to show there is such a thing as good
taste, I should also explain why people think there isn't. There
are two reasons. One is that there's always so much disagreement
about taste. Most people's response to art is a tangle of unexamined
impulses. Is the artist famous? Is the subject attractive? Is this
the sort of art they're supposed to like? Is it hanging in a famous
museum, or reproduced in a big, expensive book? In practice most
people's response to art is dominated by such extraneous factors.And the people who do claim to have good taste are so often mistaken.
The paintings admired by the so-called experts in one generation
are often so different from those admired a few generations later.
It's easy to conclude there's nothing real there at all. It's only
when you isolate this force, for example by trying to paint and
comparing your work to Bellini's, that you can see that it does in
fact exist.The other reason people doubt that art can be good is that there
doesn't seem to be any room in the art for this goodness. The
argument goes like this. Imagine several people looking at a work
of art and judging how good it is. If being good art really is a
property of objects, it should be in the object somehow. But it
doesn't seem to be; it seems to be something happening in the heads
of each of the observers. And if they disagree, how do you choose
between them?The solution to this puzzle is to realize that the purpose of art
is to work on its human audience, and humans have a lot in common.
And to the extent the things an object acts upon respond in the
same way, that's arguably what it means for the object to have the
corresponding property. If everything a particle interacts with
behaves as if the particle had a mass of m, then it has a mass of
m. So the distinction between "objective" and "subjective" is not
binary, but a matter of degree, depending on how much the subjects
have in common. Particles interacting with one another are at one
pole, but people interacting with art are not all the way at the
other; their reactions aren't random.Because people's responses to art aren't random, art can be designed
to operate on people, and be good or bad depending on how effectively
it does so. Much as a vaccine can be. If someone were talking about
the ability of a vaccine to confer immunity, it would seem very
frivolous to object that conferring immunity wasn't really a property
of vaccines, because acquiring immunity is something that happens
in the immune system of each individual person. Sure, people's
immune systems vary, and a vaccine that worked on one might not
work on another, but that doesn't make it meaningless to talk about
the effectiveness of a vaccine.The situation with art is messier, of course. You can't measure
effectiveness by simply taking a vote, as you do with vaccines.
You have to imagine the responses of subjects with a deep knowledge
of art, and enough clarity of mind to be able to ignore extraneous
influences like the fame of the artist. And even then you'd still
see some disagreement. People do vary, and judging art is hard,
especially recent art. There is definitely not a total order either
of works or of people's ability to judge them. But there is equally
definitely a partial order of both. So while it's not possible to
have perfect taste, it is possible to have good taste.
Thanks to the Cambridge Union for inviting me, and to Trevor
Blackwell, Jessica Livingston, and Robert Morris for reading drafts
of this.
February 2020What should an essay be? Many people would say persuasive. That's
what a lot of us were taught essays should be. But I think we can
aim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough
merely to be correct. It's easy to make a statement correct by
making it vague. That's a common flaw in academic writing, for
example. If you know nothing at all about an issue, you can't go
wrong by saying that the issue is a complex one, that there are
many factors to be considered, that it's a mistake to take too
simplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing.
Useful writing makes claims that are as strong as they can be made
without becoming false.For example, it's more useful to say that Pike's Peak is near the
middle of Colorado than merely somewhere in Colorado. But if I say
it's in the exact middle of Colorado, I've now gone too far, because
it's a bit east of the middle.Precision and correctness are like opposing forces. It's easy to
satisfy one if you ignore the other. The converse of vaporous
academic writing is the bold, but false, rhetoric of demagogues.
Useful writing is bold, but true.It's also two other things: it tells people something important,
and that at least some of them didn't already know.Telling people something they didn't know doesn't always mean
surprising them. Sometimes it means telling them something they
knew unconsciously but had never put into words. In fact those may
be the more valuable insights, because they tend to be more
fundamental.Let's put them all together. Useful writing tells people something
true and important that they didn't already know, and tells them
as unequivocally as possible.Notice these are all a matter of degree. For example, you can't
expect an idea to be novel to everyone. Any insight that you have
will probably have already been had by at least one of the world's
7 billion people. But it's sufficient if an idea is novel to a lot
of readers.Ditto for correctness, importance, and strength. In effect the four
components are like numbers you can multiply together to get a score
for usefulness. Which I realize is almost awkwardly reductive, but
nonetheless true._____
How can you ensure that the things you say are true and novel and
important? Believe it or not, there is a trick for doing this. I
learned it from my friend Robert Morris, who has a horror of saying
anything dumb. His trick is not to say anything unless he's sure
it's worth hearing. This makes it hard to get opinions out of him,
but when you do, they're usually right.Translated into essay writing, what this means is that if you write
a bad sentence, you don't publish it. You delete it and try again.
Often you abandon whole branches of four or five paragraphs. Sometimes
a whole essay.You can't ensure that every idea you have is good, but you can
ensure that every one you publish is, by simply not publishing the
ones that aren't.In the sciences, this is called publication bias, and is considered
bad. When some hypothesis you're exploring gets inconclusive results,
you're supposed to tell people about that too. But with essay
writing, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an
essay fast, trying out all kinds of ideas. Then I spend days rewriting
it very carefully.I've never tried to count how many times I proofread essays, but
I'm sure there are sentences I've read 100 times before publishing
them. When I proofread an essay, there are usually passages that
stick out in an annoying way, sometimes because they're clumsily
written, and sometimes because I'm not sure they're true. The
annoyance starts out unconscious, but after the tenth reading or
so I'm saying "Ugh, that part" each time I hit it. They become like
briars that catch your sleeve as you walk past. Usually I won't
publish an essay till they're all gone  till I can read through
the whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't
think of a way to rephrase it, but I will never knowingly let through
one that doesn't seem correct. You never have to. If a sentence
doesn't seem right, all you have to do is ask why it doesn't, and
you've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You
don't have a deadline. You can work for as long on an essay as you
need to get it right. You don't have to publish the essay at all,
if you can't get it right. Mistakes seem to lose courage in the
face of an enemy with unlimited resources. Or that's what it feels
like. What's really going on is that you have different expectations
for yourself. You're like a parent saying to a child "we can sit
here all night till you eat your vegetables." Except you're the
child too.I'm not saying no mistake gets through. For example, I added condition
(c) in "A Way to Detect Bias" 
after readers pointed out that I'd
omitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I
suggest to young founders for getting startup ideas: to make something
you yourself want. You can use yourself as a proxy for the reader.
The reader is not completely unlike you, so if you write about
topics that seem important to you, they'll probably seem important
to a significant number of readers as well.Importance has two factors. It's the number of people something
matters to, times how much it matters to them. Which means of course
that it's not a rectangle, but a sort of ragged comb, like a Riemann
sum.The way to get novelty is to write about topics you've thought about
a lot. Then you can use yourself as a proxy for the reader in this
department too. Anything you notice that surprises you, who've
thought about the topic a lot, will probably also surprise a
significant number of readers. And here, as with correctness and
importance, you can use the Morris technique to ensure that you
will. If you don't learn anything from writing an essay, don't
publish it.You need humility to measure novelty, because acknowledging the
novelty of an idea means acknowledging your previous ignorance of
it. Confidence and humility are often seen as opposites, but in
this case, as in many others, confidence helps you to be humble.
If you know you're an expert on some topic, you can freely admit
when you learn something you didn't know, because you can be confident
that most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two
things: thinking well, and the skillful use of qualification. These
two counterbalance each other, like the accelerator and clutch in
a car with a manual transmission. As you try to refine the expression
of an idea, you adjust the qualification accordingly. Something
you're sure of, you can state baldly with no qualification at all,
as I did the four components of useful writing. Whereas points that
seem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less
qualification. But you can rarely get it down to zero. Sometimes
you don't even want to, if it's a side point and a fully refined
version would be too long.Some say that qualifications weaken writing. For example, that you
should never begin a sentence in an essay with "I think," because
if you're saying it, then of course you think it. And it's true
that "I think x" is a weaker statement than simply "x." Which is
exactly why you need "I think." You need it to express your degree
of certainty.But qualifications are not scalars. They're not just experimental
error. There must be 50 things they can express: how broadly something
applies, how you know it, how happy you are it's so, even how it
could be falsified. I'm not going to try to explore the structure
of qualification here. It's probably more complex than the whole
topic of writing usefully. Instead I'll just give you a practical
tip: Don't underestimate qualification. It's an important skill in
its own right, not just a sort of tax you have to pay in order to
avoid saying things that are false. So learn and use its full range.
It may not be fully half of having good ideas, but it's part of
having them.There's one other quality I aim for in essays: to say things as
simply as possible. But I don't think this is a component of
usefulness. It's more a matter of consideration for the reader. And
it's a practical aid in getting things right; a mistake is more
obvious when expressed in simple language. But I'll admit that the
main reason I write simply is not for the reader's sake or because
it helps get things right, but because it bothers me to use more
or fancier words than I need to. It seems inelegant, like a program
that's too long.I realize florid writing works for some people. But unless you're
sure you're one of them, the best advice is to write as simply as
you can._____
I believe the formula I've given you, importance + novelty +
correctness + strength, is the recipe for a good essay. But I should
warn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something
they didn't know, they don't always thank you for it. Sometimes the
reason people don't know something is because they don't want to
know it. Usually because it contradicts some cherished belief. And
indeed, if you're looking for novel ideas, popular but mistaken
beliefs are a good place to find them. Every popular mistaken belief
creates a dead zone of ideas around 
it that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything
that annoys people more than having their cherished assumptions
contradicted, it's having them flatly contradicted.Plus if you've used the Morris technique, your writing will seem
quite confident. Perhaps offensively confident, to people who
disagree with you. The reason you'll seem confident is that you are
confident: you've cheated, by only publishing the things you're
sure of.  It will seem to people who try to disagree with you that
you never admit you're wrong. In fact you constantly admit you're
wrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things
worse. Brevity is the diction of command. If you watch someone
delivering unwelcome news from a position of inferiority, you'll
notice they tend to use lots of words, to soften the blow. Whereas
to be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly
than you mean. To put "perhaps" in front of something you're actually
quite sure of. But you'll notice that when writers do this, they
usually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic
tone for a whole essay. I think we just have to face the fact that
elegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that
an essay is correct, it will be invulnerable to attack. That's sort
of true. It will be invulnerable to valid attacks. But in practice
that's little consolation.In fact, the strength component of useful writing will make you
particularly vulnerable to misrepresentation. If you've stated an
idea as strongly as you could without making it false, all anyone
has to do is to exaggerate slightly what you said, and now it is
false.Much of the time they're not even doing it deliberately. One of the
most surprising things you'll discover, if you start writing essays,
is that people who disagree with you rarely disagree with what
you've actually written. Instead they make up something you said
and disagree with that.For what it's worth, the countermove is to ask someone who does
this to quote a specific sentence or passage you wrote that they
believe is false, and explain why. I say "for what it's worth"
because they never do. So although it might seem that this could
get a broken discussion back on track, the truth is that it was
never on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if
they're misinterpretations a reasonably smart and well-intentioned
person might make. In fact it's sometimes better to say something
slightly misleading and then add the correction than to try to get
an idea right in one shot. That can be more efficient, and can also
model the way such an idea would be discovered.But I don't think you should explicitly forestall intentional
misinterpretations in the body of an essay. An essay is a place to
meet honest readers. You don't want to spoil your house by putting
bars on the windows to protect against dishonest ones. The place
to protect against intentional misinterpretations is in end-notes.
But don't think you can predict them all. People are as ingenious
at misrepresenting you when you say something they don't want to
hear as they are at coming up with rationalizations for things they
want to do but know they shouldn't. I suspect it's the same skill._____
As with most other things, the way to get better at writing essays
is to practice. But how do you start? Now that we've examined the
structure of useful writing, we can rephrase that question more
precisely. Which constraint do you relax initially? The answer is,
the first component of importance: the number of people who care
about what you write.If you narrow the topic sufficiently, you can probably find something
you're an expert on. Write about that to start with. If you only
have ten readers who care, that's fine. You're helping them, and
you're writing. Later you can expand the breadth of topics you write
about.The other constraint you can relax is a little surprising: publication.
Writing essays doesn't have to mean publishing them. That may seem
strange now that the trend is to publish every random thought, but
it worked for me. I wrote what amounted to essays in notebooks for
about 15 years. I never published any of them and never expected
to. I wrote them as a way of figuring things out. But when the web
came along I'd had a lot of practice.Incidentally, 
Steve 
Wozniak did the same thing. In high school he
designed computers on paper for fun. He couldn't build them because
he couldn't afford the components. But when Intel launched 4K DRAMs
in 1975, he was ready._____
How many essays are there left to write though? The answer to that
question is probably the most exciting thing I've learned about
essay writing. Nearly all of them are left to write.Although the essay 
is an old form, it hasn't been assiduously
cultivated. In the print era, publication was expensive, and there
wasn't enough demand for essays to publish that many. You could
publish essays if you were already well known for writing something
else, like novels. Or you could write book reviews that you took
over to express your own ideas. But there was not really a direct
path to becoming an essayist. Which meant few essays got written,
and those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish
essays online. You start in obscurity, perhaps, but at least you
can start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for
years, till some change makes it explode. Cryptography did this to
number theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but
that there's a lot left to discover. There's a certain kind of idea
that's best discovered by writing essays. If most essays are still
unwritten, most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for
publication. I wrote several to figure out what Y Combinator should
do, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and
Robert Morris for reading drafts of this.February 2007A few days ago I finally figured out something I've wondered about
for 25 years: the relationship between wisdom and intelligence.
Anyone can see they're not the same by the number of people who are
smart, but not very wise.  And yet intelligence and wisdom do seem
related.  How?What is wisdom?  I'd say it's knowing what to do in a lot of
situations.  I'm not trying to make a deep point here about the
true nature of wisdom, just to figure out how we use the word.  A
wise person is someone who usually knows the right thing to do.And yet isn't being smart also knowing what to do in certain
situations?  For example, knowing what to do when the teacher tells
your elementary school class to add all the numbers from 1 to 100?
[1]Some say wisdom and intelligence apply to different types of
problems—wisdom to human problems and intelligence to abstract
ones.  But that isn't true.  Some wisdom has nothing to do with
people: for example, the wisdom of the engineer who knows certain
structures are less prone to failure than others.  And certainly
smart people can find clever solutions to human problems as well
as abstract ones. 
[2]Another popular explanation is that wisdom comes from experience
while intelligence is innate.  But people are not simply wise in
proportion to how much experience they have.  Other things must
contribute to wisdom besides experience, and some may be innate: a
reflective disposition, for example.Neither of the conventional explanations of the difference between
wisdom and intelligence stands up to scrutiny.  So what is the
difference?  If we look at how people use the words "wise" and
"smart," what they seem to mean is different shapes of performance.Curve"Wise" and "smart" are both ways of saying someone knows what to
do.  The difference is that "wise" means one has a high average
outcome across all situations, and "smart" means one does spectacularly
well in a few.  That is, if you had a graph in which the x axis
represented situations and the y axis the outcome, the graph of the
wise person would be high overall, and the graph of the smart person
would have high peaks.The distinction is similar to the rule that one should judge talent
at its best and character at its worst.  Except you judge intelligence
at its best, and wisdom by its average.  That's how the two are
related: they're the two different senses in which the same curve
can be high.So a wise person knows what to do in most situations, while a smart
person knows what to do in situations where few others could.  We
need to add one more qualification: we should ignore cases where
someone knows what to do because they have inside information. 
[3]
But aside from that, I don't think we can get much more specific
without starting to be mistaken.Nor do we need to.  Simple as it is, this explanation predicts, or
at least accords with, both of the conventional stories about the
distinction between wisdom and intelligence.  Human problems are
the most common type, so being good at solving those is key in
achieving a high average outcome.   And it seems natural that a
high average outcome depends mostly on experience, but that dramatic
peaks can only be achieved by people with certain rare, innate
qualities; nearly anyone can learn to be a good swimmer, but to be
an Olympic swimmer you need a certain body type.This explanation also suggests why wisdom is such an elusive concept:
there's no such thing.  "Wise" means something—that one is
on average good at making the right choice.  But giving the name
"wisdom" to the supposed quality that enables one to do that doesn't
mean such a thing exists.  To the extent "wisdom" means anything,
it refers to a grab-bag of qualities as various as self-discipline,
experience, and empathy.  
[4]Likewise, though "intelligent" means something, we're asking for
trouble if we insist on looking for a single thing called "intelligence."
And whatever its components, they're not all innate.  We use the
word "intelligent" as an indication of ability: a smart person can
grasp things few others could.  It does seem likely there's some
inborn predisposition to intelligence (and wisdom too), but this
predisposition is not itself intelligence.One reason we tend to think of intelligence as inborn is that people
trying to measure it have concentrated on the aspects of it that
are most measurable.  A quality that's inborn will obviously be
more convenient to work with than one that's influenced by experience,
and thus might vary in the course of a study.  The problem comes
when we drag the word "intelligence" over onto what they're measuring.
If they're measuring something inborn, they can't be measuring
intelligence.  Three year olds aren't smart.   When we describe one
as smart, it's shorthand for "smarter than other three year olds."SplitPerhaps it's a technicality to point out that a predisposition to
intelligence is not the same as intelligence.  But it's an important
technicality, because it reminds us that we can become smarter,
just as we can become wiser.The alarming thing is that we may have to choose between the two.If wisdom and intelligence are the average and peaks of the same
curve, then they converge as the number of points on the curve
decreases.  If there's just one point, they're identical: the average
and maximum are the same.  But as the number of points increases,
wisdom and intelligence diverge.  And historically the number of
points on the curve seems to have been increasing: our ability is
tested in an ever wider range of situations.In the time of Confucius and Socrates, people seem to have regarded
wisdom, learning, and intelligence as more closely related than we
do.  Distinguishing between "wise" and "smart" is a modern habit.
[5]
And the reason we do is that they've been diverging.  As knowledge
gets more specialized, there are more points on the curve, and the
distinction between the spikes and the average becomes sharper,
like a digital image rendered with more pixels.One consequence is that some old recipes may have become obsolete.
At the very least we have to go back and figure out if they were
really recipes for wisdom or intelligence.  But the really striking
change, as intelligence and wisdom drift apart, is that we may have
to decide which we prefer.  We may not be able to optimize for both
simultaneously.Society seems to have voted for intelligence.  We no longer admire
the sage—not the way people did two thousand years ago.  Now
we admire the genius.  Because in fact the distinction we began
with has a rather brutal converse: just as you can be smart without
being very wise, you can be wise without being very smart.  That
doesn't sound especially admirable.  That gets you James Bond, who
knows what to do in a lot of situations, but has to rely on Q for
the ones involving math.Intelligence and wisdom are obviously not mutually exclusive.  In
fact, a high average may help support high peaks.  But there are
reasons to believe that at some point you have to choose between
them.  One is the example of very smart people, who are so often
unwise that in popular culture this now seems to be regarded as the
rule rather than the exception.  Perhaps the absent-minded professor
is wise in his way, or wiser than he seems, but he's not wise in
the way Confucius or Socrates wanted people to be. 
[6]NewFor both Confucius and Socrates, wisdom, virtue, and happiness were
necessarily related.  The wise man was someone who knew what the
right choice was and always made it; to be the right choice, it had
to be morally right; he was therefore always happy, knowing he'd
done the best he could.  I can't think of many ancient philosophers
who would have disagreed with that, so far as it goes."The superior man is always happy; the small man sad," said Confucius.
[7]Whereas a few years ago I read an interview with a mathematician
who said that most nights he went to bed discontented, feeling he
hadn't made enough progress.  
[8]
The Chinese and Greek words we
translate as "happy" didn't mean exactly what we do by it, but
there's enough overlap that this remark contradicts them.Is the mathematician a small man because he's discontented?  No;
he's just doing a kind of work that wasn't very common in Confucius's
day.Human knowledge seems to grow fractally.  Time after time, something
that seemed a small and uninteresting area—experimental error,
even—turns out, when examined up close, to have as much in
it as all knowledge up to that point.  Several of the fractal buds
that have exploded since ancient times involve inventing and
discovering new things.  Math, for example, used to be something a
handful of people did part-time.  Now it's the career of thousands.
And in work that involves making new things, some old rules don't
apply.Recently I've spent some time advising people, and there I find the
ancient rule still works: try to understand the situation as well
as you can, give the best advice you can based on your experience,
and then don't worry about it, knowing you did all you could.  But
I don't have anything like this serenity when I'm writing an essay.
Then I'm worried.  What if I run out of ideas?  And when I'm writing,
four nights out of five I go to bed discontented, feeling I didn't
get enough done.Advising people and writing are fundamentally different types of
work.  When people come to you with a problem and you have to figure
out the right thing to do, you don't (usually) have to invent
anything.  You just weigh the alternatives and try to judge which
is the prudent choice.  But prudence can't tell me what sentence
to write next.  The search space is too big.Someone like a judge or a military officer can in much of his work
be guided by duty, but duty is no guide in making things.  Makers
depend on something more precarious: inspiration.  And like most
people who lead a precarious existence, they tend to be worried,
not contented.  In that respect they're more like the small man of
Confucius's day, always one bad harvest (or ruler) away from
starvation. Except instead of being at the mercy of weather and
officials, they're at the mercy of their own imagination.LimitsTo me it was a relief just to realize it might be ok to be discontented.
The idea that a successful person should be happy has thousands of
years of momentum behind it.  If I was any good, why didn't I have
the easy confidence winners are supposed to have?  But that, I now
believe, is like a runner asking "If I'm such a good athlete, why
do I feel so tired?" Good runners still get tired; they just get
tired at higher speeds.People whose work is to invent or discover things are in the same
position as the runner.  There's no way for them to do the best
they can, because there's no limit to what they could do.  The
closest you can come is to compare yourself to other people.  But
the better you do, the less this matters.  An undergrad who gets
something published feels like a star.  But for someone at the top
of the field, what's the test of doing well?  Runners can at least
compare themselves to others doing exactly the same thing; if you
win an Olympic gold medal, you can be fairly content, even if you
think you could have run a bit faster.  But what is a novelist to
do?Whereas if you're doing the kind of work in which problems are
presented to you and you have to choose between several alternatives,
there's an upper bound on your performance: choosing the best every
time.  In ancient societies, nearly all work seems to have been of
this type.  The peasant had to decide whether a garment was worth
mending, and the king whether or not to invade his neighbor, but
neither was expected to invent anything.  In principle they could
have; the king could have invented firearms, then invaded his
neighbor.  But in practice innovations were so rare that they weren't
expected of you, any more than goalkeepers are expected to score
goals. 
[9]
In practice, it seemed as if there was a correct decision
in every situation, and if you made it you'd done your job perfectly,
just as a goalkeeper who prevents the other team from scoring is
considered to have played a perfect game.In this world, wisdom seemed paramount.  
[10]
Even now, most people
do work in which problems are put before them and they have to
choose the best alternative.  But as knowledge has grown more
specialized, there are more and more types of work in which people
have to make up new things, and in which performance is therefore
unbounded.  Intelligence has become increasingly important relative
to wisdom because there is more room for spikes.RecipesAnother sign we may have to choose between intelligence and wisdom
is how different their recipes are.  Wisdom seems to come largely
from curing childish qualities, and intelligence largely from
cultivating them.Recipes for wisdom, particularly ancient ones, tend to have a
remedial character.  To achieve wisdom one must cut away all the
debris that fills one's head on emergence from childhood, leaving
only the important stuff.  Both self-control and experience have
this effect: to eliminate the random biases that come from your own
nature and from the circumstances of your upbringing respectively.
That's not all wisdom is, but it's a large part of it.  Much of
what's in the sage's head is also in the head of every twelve year
old.  The difference is that in the head of the twelve year old
it's mixed together with a lot of random junk.The path to intelligence seems to be through working on hard problems.
You develop intelligence as you might develop muscles, through
exercise.  But there can't be too much compulsion here.  No amount
of discipline can replace genuine curiosity.  So cultivating
intelligence seems to be a matter of identifying some bias in one's
character—some tendency to be interested in certain types of
things—and nurturing it.  Instead of obliterating your
idiosyncrasies in an effort to make yourself a neutral vessel for
the truth, you select one and try to grow it from a seedling into
a tree.The wise are all much alike in their wisdom, but very smart people
tend to be smart in distinctive ways.Most of our educational traditions aim at wisdom. So perhaps one
reason schools work badly is that they're trying to make intelligence
using recipes for wisdom.  Most recipes for wisdom have an element
of subjection.  At the very least, you're supposed to do what the
teacher says.  The more extreme recipes aim to break down your
individuality the way basic training does.  But that's not the route
to intelligence.  Whereas wisdom comes through humility, it may
actually help, in cultivating intelligence, to have a mistakenly
high opinion of your abilities, because that encourages you to keep
working.  Ideally till you realize how mistaken you were.(The reason it's hard to learn new skills late in life is not just
that one's brain is less malleable.  Another probably even worse
obstacle is that one has higher standards.)I realize we're on dangerous ground here.  I'm not proposing the
primary goal of education should be to increase students' "self-esteem."
That just breeds laziness.  And in any case, it doesn't really fool
the kids, not the smart ones.  They can tell at a young age that a
contest where everyone wins is a fraud.A teacher has to walk a narrow path: you want to encourage kids to
come up with things on their own, but you can't simply applaud
everything they produce.  You have to be a good audience: appreciative,
but not too easily impressed.  And that's a lot of work.  You have
to have a good enough grasp of kids' capacities at different ages
to know when to be surprised.That's the opposite of traditional recipes for education.  Traditionally
the student is the audience, not the teacher; the student's job is
not to invent, but to absorb some prescribed body of material.  (The
use of the term "recitation" for sections in some colleges is a
fossil of this.) The problem with these old traditions is that
they're too much influenced by recipes for wisdom.DifferentI deliberately gave this essay a provocative title; of course it's
worth being wise.  But I think it's important to understand the
relationship between intelligence and wisdom, and particularly what
seems to be the growing gap between them.  That way we can avoid
applying rules and standards to intelligence that are really meant
for wisdom.  These two senses of "knowing what to do" are more
different than most people realize.  The path to wisdom is through
discipline, and the path to intelligence through carefully selected
self-indulgence.  Wisdom is universal, and intelligence idiosyncratic.
And while wisdom yields calmness, intelligence much of the time
leads to discontentment.That's particularly worth remembering.  A physicist friend recently
told me half his department was on Prozac.  Perhaps if we acknowledge
that some amount of frustration is inevitable in certain kinds
of work, we can mitigate its effects.  Perhaps we can box it up and
put it away some of the time, instead of letting it flow together
with everyday sadness to produce what seems an alarmingly large
pool.  At the very least, we can avoid being discontented about
being discontented.If you feel exhausted, it's not necessarily because there's something
wrong with you.  Maybe you're just running fast.Notes[1]
Gauss was supposedly asked this when he was 10.  Instead of
laboriously adding together the numbers like the other students,
he saw that they consisted of 50 pairs that each summed to 101 (100
+ 1, 99 + 2, etc), and that he could just multiply 101 by 50 to get
the answer, 5050.[2]
A variant is that intelligence is the ability to solve problems,
and wisdom the judgement to know how to use those solutions.   But
while this is certainly an important relationship between wisdom
and intelligence, it's not the distinction between them.  Wisdom
is useful in solving problems too, and intelligence can help in
deciding what to do with the solutions.[3]
In judging both intelligence and wisdom we have to factor out
some knowledge. People who know the combination of a safe will be
better at opening it than people who don't, but no one would say
that was a test of intelligence or wisdom.But knowledge overlaps with wisdom and probably also intelligence.
A knowledge of human nature is certainly part of wisdom.  So where
do we draw the line?Perhaps the solution is to discount knowledge that at some point
has a sharp drop in utility.  For example, understanding French
will help you in a large number of situations, but its value drops
sharply as soon as no one else involved knows French.  Whereas the
value of understanding vanity would decline more gradually.The knowledge whose utility drops sharply is the kind that has
little relation to other knowledge.  This includes mere conventions,
like languages and safe combinations, and also what we'd call
"random" facts, like movie stars' birthdays, or how to distinguish
1956 from 1957 Studebakers.[4]
People seeking some single thing called "wisdom" have been
fooled by grammar.  Wisdom is just knowing the right thing to do,
and there are a hundred and one different qualities that help in
that.  Some, like selflessness, might come from meditating in an
empty room, and others, like a knowledge of human nature, might
come from going to drunken parties.Perhaps realizing this will help dispel the cloud of semi-sacred
mystery that surrounds wisdom in so many people's eyes.  The mystery
comes mostly from looking for something that doesn't exist.  And
the reason there have historically been so many different schools
of thought about how to achieve wisdom is that they've focused on
different components of it.When I use the word "wisdom" in this essay, I mean no more than
whatever collection of qualities helps people make the right choice
in a wide variety of situations.[5]
Even in English, our sense of the word "intelligence" is
surprisingly recent.  Predecessors like "understanding" seem to
have had a broader meaning.[6]
There is of course some uncertainty about how closely the remarks
attributed to Confucius and Socrates resemble their actual opinions.
I'm using these names as we use the name "Homer," to mean the
hypothetical people who said the things attributed to them.[7]
Analects VII:36, Fung trans.Some translators use "calm" instead of "happy."  One source of
difficulty here is that present-day English speakers have a different
idea of happiness from many older societies.  Every language probably
has a word meaning "how one feels when things are going well," but
different cultures react differently when things go well.  We react
like children, with smiles and laughter.  But in a more reserved
society, or in one where life was tougher, the reaction might be a
quiet contentment.[8]
It may have been Andrew Wiles, but I'm not sure.  If anyone
remembers such an interview, I'd appreciate hearing from you.[9]
Confucius claimed proudly that he had never invented
anything—that he had simply passed on an accurate account of
ancient traditions.  [Analects VII:1] It's hard for us now to
appreciate how important a duty it must have been in preliterate
societies to remember and pass on the group's accumulated knowledge.
Even in Confucius's time it still seems to have been the first duty
of the scholar.[10]
The bias toward wisdom in ancient philosophy may be exaggerated
by the fact that, in both Greece and China, many of the first
philosophers (including Confucius and Plato) saw themselves as
teachers of administrators, and so thought disproportionately about
such matters.  The few people who did invent things, like storytellers,
must have seemed an outlying data point that could be ignored.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston,
and Robert Morris for reading drafts of this.January 2003(This article is derived from a keynote talk at the fall 2002 meeting
of NEPLS.)Visitors to this country are often surprised to find that
Americans like to begin a conversation by asking "what do you do?"
I've never liked this question.  I've rarely had a
neat answer to it.  But I think I have finally solved the problem.
Now, when someone asks me what I do, I look them straight
in the eye and say "I'm designing a 
new dialect of Lisp."   
I recommend this answer to anyone who doesn't like being asked what
they do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.
I'm just designing one, in the same way that someone might design
a building or a chair or a new typeface.
I'm not trying to discover anything new.  I just want
to make a language that will be good to program in.  In some ways,
this assumption makes life a lot easier.The difference between design and research seems to be a question
of new versus good.  Design doesn't have to be new, but it has to  
be good.  Research doesn't have to be good, but it has to be new.
I think these two paths converge at the top: the best design
surpasses its predecessors by using new ideas, and the best research
solves problems that are not only new, but actually worth solving.
So ultimately we're aiming for the same destination, just approaching
it from different directions.What I'm going to talk about today is what your target looks like
from the back.  What do you do differently when you treat
programming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.
Design begins by asking, who is this
for and what do they need from it?  A good architect,
for example, does not begin by creating a design that he then
imposes on the users, but by studying the intended users and figuring
out what they need.Notice I said "what they need," not "what they want."  I don't mean
to give the impression that working as a designer means working as 
a sort of short-order cook, making whatever the client tells you
to.  This varies from field to field in the arts, but
I don't think there is any field in which the best work is done by
the people who just make exactly what the customers tell them to.The customer is always right in
the sense that the measure of good design is how well it works
for the user.  If you make a novel that bores everyone, or a chair
that's horribly uncomfortable to sit in, then you've done a bad
job, period.  It's no defense to say that the novel or the chair  
is designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making
what the user tells you to.  Users don't know what all the choices
are, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design
for the user, but you have to design what the user needs, not simply  
what he says he wants.
It's much like being a doctor.  You can't just treat a patient's
symptoms.  When a patient tells you his symptoms, you have to figure
out what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the
practice of good design can be derived, and around which most design
issues center.If good design must do what the user needs, who is the user?  When
I say that design must be for users, I don't mean to imply that good 
design aims at some kind of  
lowest common denominator.  You can pick any group of users you
want.  If you're designing a tool, for example, you can design it
for anyone from beginners to experts, and what's good design
for one group might be bad for another.  The point
is, you have to pick some group of users.  I don't think you can
even talk about good or bad design except with
reference to some intended user.You're most likely to get good design if the intended users include
the designer himself.  When you design something
for a group that doesn't include you, it tends to be for people
you consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,
seems inevitably to corrupt the designer.
I suspect that very few housing
projects in the US were designed by architects who expected to live
in them.   You can see the same thing
in programming languages.  C, Lisp, and Smalltalk were created for
their own designers to use.  Cobol, Ada, and Java, were created   
for other people to use.If you think you're designing something for idiots, the odds are
that you're not designing something good, even for idiots.
Even if you're designing something for the most sophisticated
users, though, you're still designing for humans.  It's different 
in research.  In math you
don't choose abstractions because they're
easy for humans to understand; you choose whichever make the
proof shorter.  I think this is true for the sciences generally.
Scientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is
all about people.  The human body is a strange
thing, but when you're designing a chair,
that's what you're designing for, and there's no way around it.
All the arts have to pander to the interests and limitations
of humans.   In painting, for example, all other things being
equal a painting with people in it will be more interesting than
one without.  It is not merely an accident of history that
the great paintings of the Renaissance are all full of people.
If they hadn't been, painting as a medium wouldn't have the prestige
that it does.Like it or not, programming languages are also for people,
and I suspect the human brain is just as lumpy and idiosyncratic
as the human body.  Some ideas are easy for people to grasp
and some aren't.  For example, we seem to have a very limited
capacity for dealing with detail.  It's this fact that makes
programing languages a good idea in the first place; if we
could handle the detail, we could just program in machine
language.Remember, too, that languages are not
primarily a form for finished programs, but something that
programs have to be developed in.  Anyone in the arts could
tell you that you might want different mediums for the
two situations.  Marble, for example, is a nice, durable
medium for finished ideas, but a hopelessly inflexible one
for developing new ideas.A program, like a proof,
is a pruned version of a tree that in the past has had
false starts branching off all over it.  So the test of
a language is not simply how clean the finished program looks
in it, but how clean the path to the finished program was.
A design choice that gives you elegant finished programs
may not give you an elegant design process.  For example, 
I've written a few macro-defining macros full of nested
backquotes that look now like little gems, but writing them
took hours of the ugliest trial and error, and frankly, I'm still
not entirely sure they're correct.We often act as if the test of a language were how good
finished programs look in it.
It seems so convincing when you see the same program
written in two languages, and one version is much shorter.
When you approach the problem from the direction of the
arts, you're less likely to depend on this sort of
test.  You don't want to end up with a programming
language like marble.For example, it is a huge win in developing software to
have an interactive toplevel, what in Lisp is called a
read-eval-print loop.  And when you have one this has
real effects on the design of the language.  It would not
work well for a language where you have to declare
variables before using them, for example.  When you're
just typing expressions into the toplevel, you want to be 
able to set x to some value and then start doing things
to x.  You don't want to have to declare the type of x
first.  You may dispute either of the premises, but if
a language has to have a toplevel to be convenient, and
mandatory type declarations are incompatible with a
toplevel, then no language that makes type declarations  
mandatory could be convenient to program in.In practice, to get good design you have to get close, and stay
close, to your users.  You have to calibrate your ideas on actual
users constantly, especially in the beginning.  One of the reasons
Jane Austen's novels are so good is that she read them out loud to
her family.  That's why she never sinks into self-indulgently arty
descriptions of landscapes,
or pretentious philosophizing.  (The philosophy's there, but it's
woven into the story instead of being pasted onto it like a label.)
If you open an average "literary" novel and imagine reading it out loud
to your friends as something you'd written, you'll feel all too
keenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.
Actually, there are several ideas mixed together in the concept of
Worse is Better, which is why people are still arguing about
whether worse
is actually better or not.  But one of the main ideas in that
mix is that if you're building something new, you should get a
prototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.
Instead of getting a prototype out quickly and gradually refining
it, you try to create the complete, finished, product in one long
touchdown pass.  As far as I know, this is a
recipe for disaster.  Countless startups destroyed themselves this
way during the Internet bubble.  I've never heard of a case
where it worked.What people outside the software world may not realize is that
Worse is Better is found throughout the arts.
In drawing, for example, the idea was discovered during the
Renaissance.  Now almost every drawing teacher will tell you that
the right way to get an accurate drawing is not to
work your way slowly around the contour of an object, because errors will
accumulate and you'll find at the end that the lines don't meet.
Instead you should draw a few quick lines in roughly the right place,
and then gradually refine this initial sketch.In most fields, prototypes
have traditionally been made out of different materials.
Typefaces to be cut in metal were initially designed  
with a brush on paper.  Statues to be cast in bronze   
were modelled in wax.  Patterns to be embroidered on tapestries
were drawn on paper with ink wash.  Buildings to be
constructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it
first became popular in the fifteenth century, was that you
could actually make the finished work from the prototype.
You could make a preliminary drawing if you wanted to, but you
weren't held to it; you could work out all the details, and
even make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to
be just a model; you can refine it into the finished product.
I think you should always do this when you can.  It lets you
take advantage of new insights you have along the way.  But
perhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people
don't talk more about it.  One of my first
drawing teachers told me: if you're bored when you're
drawing something, the drawing will look boring.
For example, suppose you have to draw a building, and you
decide to draw each brick individually.  You can do this
if you want, but if you get bored halfway through and start
making the bricks mechanically instead of observing each one,   
the drawing will look worse than if you had merely suggested
the bricks.Building something by gradually refining a prototype is good
for morale because it keeps you engaged.  In software, my  
rule is: always have working code.  If you're writing
something that you'll be able to test in an hour, then you
have the prospect of an immediate reward to motivate you.
The same is true in the arts, and particularly in oil painting.
Most painters start with a blurry sketch and gradually
refine it.
If you work this way, then in principle
you never have to end the day with something that actually
looks unfinished.  Indeed, there is even a saying among
painters: "A painting is never finished, you just stop
working on it."  This idea will be familiar to anyone who
has worked on software.Morale is another reason that it's hard to design something
for an unsophisticated user.   It's hard to stay interested in
something you don't like yourself.  To make something  
good, you have to be thinking, "wow, this is really great,"
not "what a piece of shit; those fools will love it."Design means making things for humans.  But it's not just the
user who's human.  The designer is human too.Notice all this time I've been talking about "the designer."
Design usually has to be under the control of a single person to
be any good.   And yet it seems to be possible for several people
to collaborate on a research project.  This seems to
me one of the most interesting differences between research and
design.There have been famous instances of collaboration in the arts,
but most of them seem to have been cases of molecular bonding rather
than nuclear fusion.  In an opera it's common for one person to
write the libretto and another to write the music.   And during the Renaissance, 
journeymen from northern
Europe were often employed to do the landscapes in the
backgrounds of Italian paintings.  But these aren't true collaborations.
They're more like examples of Robert Frost's
"good fences make good neighbors."  You can stick instances
of good design together, but within each individual project,
one person has to be in control.I'm not saying that good design requires that one person think
of everything.  There's nothing more valuable than the advice
of someone whose judgement you trust.  But after the talking is
done, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  
design can't?  This is an interesting question.  I don't 
know the answer.  Perhaps,
if design and research converge, the best research is also
good design, and in fact can't be done by collaborators.
A lot of the most famous scientists seem to have worked alone.
But I don't know enough to say whether there
is a pattern here.  It could be simply that many famous scientists
worked when collaboration was less common.Whatever the story is in the sciences, true collaboration
seems to be vanishingly rare in the arts.  Design by committee is a
synonym for bad design.  Why is that so?  Is there some way to
beat this limitation?I'm inclined to think there isn't-- that good design requires
a dictator.  One reason is that good design has to   
be all of a piece.  Design is not just for humans, but
for individual humans.  If a design represents an idea that  
fits in one person's head, then the idea will fit in the user's
head too.Related:May 2004When people care enough about something to do it well, those who
do it best tend to be far better than everyone else.  There's a
huge gap between Leonardo and second-rate contemporaries like
Borgognone.  You see the same gap between Raymond Chandler and the
average writer of detective novels.  A top-ranked professional chess
player could play ten thousand games against an ordinary club player
without losing once.Like chess or painting or writing novels, making money is a very
specialized skill.   But for some reason we treat this skill
differently.  No one complains when a few people surpass all the
rest at playing chess or writing novels, but when a few people make
more money than the rest, we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other
skill.  What causes people to react so strongly when the skill is
making money?I think there are three reasons we treat making money as different:
the misleading model of wealth we learn as children; the disreputable
way in which, till recently, most fortunes were accumulated; and
the worry that great variations in income are somehow bad for
society.  As far as I can tell, the first is mistaken, the second
outdated, and the third empirically false.  Could it be that, in a
modern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric
sockets.  I didn't realize there were power plants out there
generating it.  Likewise, it doesn't occur to most kids that wealth
is something that has to be generated.  It seems to be something
that flows from parents.Because of the circumstances in which they encounter it, children
tend to misunderstand wealth.  They confuse it with money.  They
think that there is a fixed amount of it.  And they think of it as
something that's distributed by authorities (and so should be
distributed equally), rather than something that has to be created
(and might be created unequally).In fact, wealth is not money.  Money is just a convenient way of
trading one form of wealth for another.  Wealth is the underlying
stuff—the goods and services we buy.  When you travel to a
rich or poor country, you don't have to look at people's bank
accounts to tell which kind you're in.  You can see
wealth—in buildings and streets, in the clothes and the health
of the people.Where does wealth come from?  People make it.  This was easier to
grasp when most people lived on farms, and made many of the things
they wanted with their own hands.  Then you could see in the house,
the herds, and the granary the wealth that each family created.  It
was obvious then too that the wealth of the world was not a fixed
quantity that had to be shared out, like slices of a pie.  If you
wanted more wealth, you could make it.This is just as true today, though few of us create wealth directly
for ourselves (except for a few vestigial domestic tasks).  Mostly
we create wealth for other people in exchange for money, which we
then trade for the forms of wealth we want. 
[1]Because kids are unable to create wealth, whatever they have has
to be given to them.  And when wealth is something you're given,
then of course it seems that it should be distributed equally.
[2]
As in most families it is.  The kids see to that.  "Unfair," they
cry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you
want something, you either have to make it, or do something of
equivalent value for someone else, in order to get them to give you
enough money to buy it.  In the real world, wealth is (except for
a few specialists like thieves and speculators) something you have
to create, not something that's distributed by Daddy.  And since
the ability and desire to create it vary from person to person,
it's not made equally.You get paid by doing or making something people want, and those
who make more money are often simply better at doing what people
want.  Top actors make a lot more money than B-list actors.  The
B-list actors might be almost as charismatic, but when people go
to the theater and look at the list of movies playing, they want
that extra oomph that the big stars have.Doing what people want is not the only way to get money, of course.
You could also rob banks, or solicit bribes, or establish a monopoly.
Such tricks account for some variation in wealth, and indeed for
some of the biggest individual fortunes, but they are not the root
cause of variation in income.  The root cause of variation in income,
as Occam's Razor implies, is the same as the root cause of variation
in every other human skill.In the United States, the CEO of a large public company makes about
100 times as much as the average person. 
[3]
Basketball players
make about 128 times as much, and baseball players 72 times as much.
Editorials quote this kind of statistic with horror.  But I have
no trouble imagining that one person could be 100 times as productive
as another.  In ancient Rome the price of slaves varied by
a factor of 50 depending on their skills. 
[4]
And that's without
considering motivation, or the extra leverage in productivity that
you can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early
Christian writers, arguing from first principles about whether the
Earth was round, when they could just walk outside and check.
[5]
How much someone's work is worth is not a policy question.  It's
something the market already determines."Are they really worth 100 of us?" editorialists ask.  Depends on
what you mean by worth.  If you mean worth in the sense of what
people will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there
not others whose incomes really do reflect the wealth they generate?
Steve Jobs saved a company that was in a terminal decline.  And not
merely in the way a turnaround specialist does, by cutting costs;
he had to decide what Apple's next products should be.  Few others
could have done it.  And regardless of the case with CEOs, it's
hard to see how anyone could argue that the salaries of professional
basketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really
generate so much more wealth than another.  The key to this mystery
is to revisit that question, are they really worth 100 of us?
Would a basketball team trade one of their players for 100
random people?  What would Apple's next product look like if you
replaced Steve Jobs with a committee of 100 random people? 
[6]
These
things don't scale linearly.  Perhaps the CEO or the professional
athlete has only ten times (whatever that means) the skill and
determination of an ordinary person.  But it makes all the difference
that it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid,
what are we really saying?  In a free market, prices are determined
by what buyers want.  People like baseball more than  poetry, so
baseball players make more than poets.  To say that a certain kind
of work is underpaid is thus identical with saying that people want
the wrong things.Well, of course people want the wrong things.  It seems odd to be
surprised by that.  And it seems even odder to say that it's
unjust that certain kinds of work are underpaid. 
[7]
Then
you're saying that it's unjust that people want the wrong things.
It's  lamentable that people prefer reality TV and corndogs to
Shakespeare and steamed vegetables, but unjust?  That seems like
saying that blue is heavy, or that up is circular.The appearance of the word "unjust" here is the unmistakable spectral
signature of the Daddy Model.  Why else would this idea occur in
this odd context?  Whereas if the speaker were still operating on
the Daddy Model, and saw wealth as something that flowed from a
common source and had to be shared out, rather than something
generated by doing what other people wanted, this is exactly what
you'd get on noticing that some people made much more than others.When we talk about "unequal distribution of income," we should
also ask, where does that income come from?
[8]
Who made the wealth
it represents?  Because to the extent that income varies simply
according to how much wealth people create, the distribution may
be unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth
alarming is that for most of human history the usual way to accumulate
a fortune was to steal it: in pastoral societies by cattle raiding;
in agricultural societies by appropriating others' estates in times
of war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates
confiscated from the losers.  In England in the 1060s, when William
the Conqueror distributed the estates of the defeated Anglo-Saxon
nobles to his followers, the conflict was military.  By the 1530s,
when Henry VIII distributed the estates of the monasteries to his
followers, it was mostly political. 
[9]
But the principle was the
same.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials
used taxation instead of confiscation.  But here too we see the
same principle: the way to get rich was not to create wealth, but
to serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class.
Now we think of the middle class as people who are neither rich nor
poor, but originally they were a distinct group.  In a feudal
society, there are just two classes: a warrior aristocracy, and the
serfs who work their estates.  The middle class were a new, third
group who lived in towns and supported themselves by manufacturing
and trade.Starting in the tenth and eleventh centuries, petty nobles and
former serfs banded together in towns that gradually became powerful
enough to ignore the local feudal lords. 
[10]
Like serfs, the middle
class made a living largely by creating wealth.  (In port cities
like Genoa and Pisa, they also engaged in piracy.) But unlike serfs
they had an incentive to create a lot of it.  Any wealth a serf
created belonged to his master.  There was not much point in making
more than you could hide.  Whereas the independence of the townsmen
allowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as
a whole started to get richer very rapidly.  Nearly everything we
have was created by the middle class.  Indeed, the other two classes
have effectively disappeared in industrial societies, and their
names been given to either end of the middle class.  (In the original
sense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation
definitively replaced corruption as the best way to get rich.  In
England, at least, corruption only became unfashionable (and in
fact only started to be called "corruption") when there started to
be other, faster ways to get rich.Seventeenth-century England was much like the third world today,
in that government office was a recognized route to wealth.  The
great fortunes of that time still derived more from what we would
now call corruption than from commerce. 
[11]
By the nineteenth
century that had changed.  There continued to be bribes, as there
still are everywhere, but politics had by then been left to men who
were driven more by vanity than greed.  Technology had made it
possible to create wealth faster than you could steal it.  The
prototypical rich man of the nineteenth century was not a courtier
but an industrialist.With the rise of the middle class, wealth stopped being a zero-sum
game.  Jobs and Wozniak didn't have to make us poor to make themselves
rich.  Quite the opposite: they created things that made our lives
materially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth
was to steal it, we tend to be suspicious of rich people.  Idealistic
undergraduates find their unconsciously preserved child's model of
wealth confirmed by eminent writers of the past.  It is a case of
the mistaken meeting the outdated."Behind every great fortune, there is a crime," Balzac wrote.  Except
he didn't.  What he actually said was that a great fortune with no
apparent cause was probably due to a crime well enough executed
that it had been forgotten.  If we were talking about Europe in
1000, or most of the third world today, the standard misquotation
would be spot on.  But Balzac lived in nineteenth-century France,
where the Industrial Revolution was well advanced.  He knew you
could make a fortune without stealing it.  After all, he did himself,
as a popular novelist.
[12]Only a few countries (by no coincidence, the richest ones) have
reached this stage.  In most, corruption still has the upper hand.
In most, the fastest way to get wealth is by stealing it.  And so
when we see increasing differences in income in a rich country,
there is a tendency to worry that it's sliding back toward becoming
another Venezuela.  I think the opposite is happening. I think
you're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?  It will
certainly increase the gap between the productive and the unproductive.
That's the whole point of technology.   With a tractor an energetic
farmer could plow six times as much land in a day as he could with
a team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In
high school I made money by mowing lawns and scooping ice cream at
Baskin-Robbins.  This was the only kind of work available at the
time.  Now high school kids could write software or design web
sites.  But only some of them will; the rest will still be scooping
ice cream.I remember very vividly when in 1985 improved technology made it
possible for me to buy a computer of my own.  Within months I was
using it to make money as a freelance programmer.  A few years
before, I couldn't have done this.  A few years before, there was
no such thing as a freelance programmer.  But Apple created
wealth, in the form of powerful, inexpensive computers, and programmers
immediately set to work using it to create more.As this example suggests, the rate at which technology increases
our productive capacity is probably exponential, rather than linear.
So we should expect to see ever-increasing variation in individual
productivity as time goes on.   Will that increase the gap between
rich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to
decrease other gaps.  A hundred years ago, the rich led a different
kind of life from ordinary people.  They lived in houses
full of servants, wore elaborately uncomfortable clothes, and
travelled about in carriages drawn by teams of horses which themselves
required their own houses and servants.  Now, thanks to technology,
the rich live more like the average person.Cars are a good example of why.  It's possible to buy expensive,
handmade cars that cost hundreds of thousands of dollars.  But there
is not much point.  Companies make more money by building a large
number of ordinary cars than a small number of expensive ones.  So
a company making a mass-produced car can afford to spend a lot more
on its design.  If you buy a custom-made car, something will always
be breaking.  The only point of buying one now is to advertise that
you can.Or consider watches.  Fifty years ago, by spending a lot of money
on a watch you could get better performance.  When watches had
mechanical movements, expensive watches kept better time.  Not any
more.  Since the invention of the quartz movement, an ordinary Timex
is more accurate than a Patek Philippe costing hundreds of thousands
of dollars.
[13]
Indeed, as with expensive cars, if you're determined
to spend a lot of money on a watch, you have to put up with some
inconvenience to do it: as well as keeping worse time, mechanical
watches have to be wound.The only thing technology can't cheapen is brand.  Which is precisely
why we hear ever more about it.  Brand is the residue left as the
substantive differences between rich and poor evaporate.  But what
label you have on your stuff is a much smaller matter than having
it versus not having it.  In 1900, if you kept a carriage, no one
asked what year or brand it was.  If you had one, you were rich.
And if you weren't rich, you took the omnibus or walked.  Now even
the poorest Americans drive cars, and it is only because we're so
well trained by advertising that we can even recognize the especially
expensive ones.
[14]The same pattern has played out in industry after industry.  If
there is enough demand for something, technology will make it cheap
enough to sell in large volumes, and the mass-produced versions
will be, if not better, at least more convenient.
[15]
And there
is nothing the rich like more than convenience.  The rich people I
know drive the same cars, wear the same clothes, have the same kind
of furniture, and eat the same foods as my other friends.  Their
houses are in different neighborhoods, or if in the same neighborhood
are different sizes, but within them life is similar.  The houses
are made using the same construction techniques and contain much
the same objects.  It's inconvenient to do something expensive and
custom.The rich spend their time more like everyone else too.  Bertie
Wooster seems long gone.  Now, most people who are rich enough not
to work do anyway.  It's not just social pressure that makes them;
idleness is lonely and demoralizing.Nor do we have the social distinctions there were a hundred years
ago.   The novels and etiquette manuals of that period read now
like descriptions of some strange tribal society.  "With respect
to the continuance of friendships..." hints Mrs. Beeton's Book
of Household Management (1880), "it may be found necessary, in
some cases, for a mistress to relinquish, on assuming the responsibility
of a household, many of those commenced in the earlier part of her
life." A woman who married a rich man was expected to drop friends
who didn't.  You'd seem a barbarian if you behaved that way today.
You'd also have a very boring life.  People still tend to segregate
themselves somewhat, but much more on the basis of education than
wealth.
[16]Materially and socially, technology seems to be decreasing the gap
between the rich and the poor, not increasing it.  If Lenin walked
around the offices of a company like Yahoo or Intel or Cisco, he'd
think communism had won.  Everyone would be wearing the same clothes,
have the same kind of office (or rather, cubicle) with the same
furnishings, and address one another by their first names instead
of by honorifics.  Everything would seem exactly as he'd predicted,
until he looked at their bank accounts.  Oops.Is it a problem if technology increases that gap?  It doesn't seem
to be so far.  As it increases the gap in income, it seems to
decrease most other gaps.Alternative to an AxiomOne often hears a policy criticized on the grounds that it would
increase the income gap between rich and poor.  As if it were an
axiom that this would be bad.  It might be true that increased
variation in income would be bad, but I don't see how we can say
it's axiomatic.Indeed, it may even be false, in industrial democracies.  In a
society of serfs and warlords, certainly, variation in income is a
sign of an underlying problem.  But serfdom is not the only cause
of variation in income.  A 747 pilot doesn't make 40 times as much
as a checkout clerk because he is a warlord who somehow holds her
in thrall.  His skills are simply much more valuable.I'd like to propose an alternative idea: that in a modern society,
increasing variation in income is a sign of health.  Technology
seems to increase the variation in productivity at faster than
linear rates.  If we don't see corresponding variation in income,
there are three possible explanations: (a) that technical innovation
has stopped, (b) that the people who would create the most wealth
aren't doing it, or (c) that they aren't getting paid for it.I think we can safely say that (a) and (b) would be bad.  If you
disagree, try living for a year using only the resources available
to the average Frankish nobleman in 800, and report back to us.
(I'll be generous and not send you back to the stone age.)The only option, if you're going to have an increasingly prosperous
society without increasing variation in income, seems to be (c),
that people will create a lot of wealth without being paid for it.
That Jobs and Wozniak, for example, will cheerfully work 20-hour
days to produce the Apple computer for a society that allows them,
after taxes, to keep just enough of their income to match what they
would have made working 9 to 5 at a big company.Will people create wealth if they can't get paid for it?  Only if
it's fun.  People will write operating systems for free.  But they
won't install them, or take support calls, or train customers to
use them.  And at least 90% of the work that even the highest tech
companies do is of this second, unedifying kind.All the unfun kinds of wealth creation slow dramatically in a society
that confiscates private fortunes.  We can confirm this empirically.
Suppose you hear a strange noise that you think may be due to a
nearby fan.  You turn the fan off, and the noise stops.  You turn
the fan back on, and the noise starts again.  Off, quiet.  On,
noise.  In the absence of other information, it would seem the noise
is caused by the fan.At various times and places in history, whether you could accumulate
a fortune by creating wealth has been turned on and off.  Northern
Italy in 800, off (warlords would steal it).  Northern Italy in
1100, on.  Central France in 1100, off (still feudal).  England in
1800, on.  England in 1974, off (98% tax on investment income).
United States in 1974, on.  We've even had a twin study: West
Germany, on;  East Germany, off.  In every case, the creation of
wealth seems to appear and disappear like the noise of a fan as you
switch on and off the prospect of keeping it.There is some momentum involved.  It probably takes at least a
generation to turn people into East Germans (luckily for England).
But if it were merely a fan we were studying, without all the extra
baggage that comes from the controversial topic of wealth, no one
would have any doubt that the fan was causing the noise.If you suppress variations in income, whether by stealing private
fortunes, as feudal rulers used to do, or by taxing them away, as
some modern governments have done, the result always seems to be
the same.    Society as a whole ends up poorer.If I had a choice of living in a society where I was materially
much better off than I am now, but was among the poorest, or in one
where I was the richest, but much worse off than I am now, I'd take
the first option.  If I had children, it would arguably be immoral
not to.  It's absolute poverty you want to avoid, not relative
poverty.  If, as the evidence so far implies, you have to have one
or the other in your society, take relative poverty.You need rich people in your society not so much because in spending
their money they create jobs, but because of what they have to do
to get rich.  I'm not talking about the trickle-down effect
here.  I'm not saying that if you let Henry Ford get rich, he'll
hire you as a waiter at his next party.  I'm saying that he'll make
you a tractor to replace your horse.Notes[1]
Part of the reason this subject is so contentious is that some
of those most vocal on the subject of wealth—university
students, heirs, professors, politicians, and journalists—have
the least experience creating it.  (This phenomenon will be familiar
to anyone who has overheard conversations about sports in a bar.)Students are mostly still on the parental dole, and have not stopped
to think about where that money comes from.  Heirs will be on the
parental dole for life.  Professors and politicians live within
socialist eddies of the economy, at one remove from the creation
of wealth, and are paid a flat rate regardless of how hard they
work.  And journalists as part of their professional code segregate
themselves from the revenue-collecting half of the businesses they
work for (the ad sales department).  Many of these people never
come face to face with the fact that the money they receive represents
wealth—wealth that, except in the case of journalists, someone
else created earlier.  They live in a world in which income is
doled out by a central authority according to some abstract notion
of fairness (or randomly, in the case of heirs), rather than given
by other people in return for something they wanted, so it may seem
to them unfair that things don't work the same in the rest of the
economy.(Some professors do create a great deal of wealth for
society.  But the money they're paid isn't a quid pro quo.
It's more in the nature of an investment.)[2]
When one reads about the origins of the Fabian Society, it
sounds like something cooked up by the high-minded Edwardian
child-heroes of Edith Nesbit's The Wouldbegoods.[3]
According to a study by the Corporate Library, the median total
compensation, including salary, bonus, stock grants, and the exercise
of stock options, of S&P 500 CEOs in 2002 was $3.65 million.
According to Sports Illustrated, the average NBA player's
salary during the 2002-03 season was $4.54 million, and the average
major league baseball player's salary at the start of the 2003
season was $2.56 million.  According to the Bureau of Labor
Statistics, the mean annual wage in the US in 2002 was $35,560.[4]
In the early empire the price of an ordinary adult slave seems
to have been about 2,000 sestertii (e.g. Horace, Sat. ii.7.43).
A servant girl cost 600 (Martial vi.66), while Columella (iii.3.8)
says that a skilled vine-dresser was worth 8,000.  A doctor, P.
Decimus Eros Merula, paid 50,000 sestertii for his freedom (Dessau,
Inscriptiones 7812).  Seneca (Ep. xxvii.7) reports
that one Calvisius Sabinus paid 100,000 sestertii apiece for slaves
learned in the Greek classics.  Pliny (Hist. Nat. vii.39)
says that the highest price paid for a slave up to his time was
700,000 sestertii, for the linguist (and presumably teacher) Daphnis,
but that this had since been exceeded by actors buying their own
freedom.Classical Athens saw a similar variation in prices.  An ordinary
laborer was worth about 125 to 150 drachmae.  Xenophon (Mem.
ii.5) mentions prices ranging from 50 to 6,000 drachmae (for the
manager of a silver mine).For more on the economics of ancient slavery see:Jones, A. H. M., "Slavery in the Ancient World," Economic History
Review, 2:9 (1956), 185-199, reprinted in Finley, M. I. (ed.),
Slavery in Classical Antiquity, Heffer, 1964.[5]
Eratosthenes (276—195 BC) used shadow lengths in different
cities to estimate the Earth's circumference.  He was off by only
about 2%.[6]
No, and Windows, respectively.[7]
One of the biggest divergences between the Daddy Model and
reality is the valuation of hard work.  In the Daddy Model, hard
work is in itself deserving.  In reality, wealth is measured by
what one delivers, not how much effort it costs.  If I paint someone's
house, the owner shouldn't pay me extra for doing it with a toothbrush.It will seem to someone still implicitly operating on the Daddy
Model that it is unfair when someone works hard and doesn't get
paid much.  To help clarify the matter, get rid of everyone else
and put our worker on a desert island, hunting and gathering fruit.
If he's bad at it he'll work very hard and not end up with much
food.  Is this unfair?  Who is being unfair to him?[8]
Part of the reason for the tenacity of the Daddy Model may be
the dual meaning of "distribution." When economists talk about
"distribution of income," they mean statistical distribution.  But
when you use the phrase frequently, you can't help associating it
with the other sense of the word (as in e.g. "distribution of alms"),
and thereby subconsciously seeing wealth as something that flows
from some central tap.  The word "regressive" as applied to tax
rates has a similar effect, at least on me; how can anything
regressive be good?[9]
"From the beginning of the reign Thomas Lord Roos was an assiduous
courtier of the young Henry VIII and was soon to reap the rewards.
In 1525 he was made a Knight of the Garter and given the Earldom
of Rutland.  In the thirties his support of the breach with Rome,
his zeal in crushing the Pilgrimage of Grace, and his readiness to
vote the death-penalty in the succession of spectacular treason
trials that punctuated Henry's erratic matrimonial progress made
him an obvious candidate for grants of monastic property."Stone, Lawrence, Family and Fortune: Studies in Aristocratic
Finance in the Sixteenth and Seventeenth Centuries, Oxford
University Press, 1973, p. 166.[10]
There is archaeological evidence for large settlements earlier,
but it's hard to say what was happening in them.Hodges, Richard and David Whitehouse, Mohammed, Charlemagne and
the Origins of Europe, Cornell University Press, 1983.[11]
William Cecil and his son Robert were each in turn the most
powerful minister of the crown, and both used their position to
amass fortunes among the largest of their times.  Robert in particular
took bribery to the point of treason.  "As Secretary of State and
the leading advisor to King James on foreign policy, [he] was a
special recipient of favour, being offered large bribes by the Dutch
not to make peace with Spain, and large bribes by Spain to make
peace." (Stone, op. cit., p. 17.)[12]
Though Balzac made a lot of money from writing, he was notoriously
improvident and was troubled by debts all his life.[13]
A Timex will gain or lose about.5 seconds per day.  The most
accurate mechanical watch, the Patek Philippe 10 Day Tourbillon,
is rated at -1.5 to +2 seconds.  Its retail price is about $220,000.[14]
If asked to choose which was more expensive, a well-preserved
1989 Lincoln Town Car ten-passenger limousine ($5,000) or a 2004
Mercedes S600 sedan ($122,000), the average Edwardian might well
guess wrong.[15]
To say anything meaningful about income trends, you have to
talk about real income, or income as measured in what it can buy.
But the usual way of calculating real income ignores much of the
growth in wealth over time, because it depends on a consumer price
index created by bolting end to end a series of numbers that are
only locally accurate, and that don't include the prices of new
inventions until they become so common that their prices stabilize.So while we might think it was very much better to live in a world
with antibiotics or air travel or an electric power grid than
without, real income statistics calculated in the usual way will
prove to us that we are only slightly richer for having these things.Another approach would be to ask, if you were going back to the
year x in a time machine, how much would you have to spend on trade
goods to make your fortune?  For example, if you were going back
to 1970 it would certainly be less than $500, because the processing
power you can get for $500 today would have been worth at least
$150 million in 1970.  The function goes asymptotic fairly quickly,
because for times over a hundred years or so you could get all you
needed in present-day trash.  In 1800 an empty plastic drink bottle
with a screw top would have seemed a miracle of workmanship.[16]
Some will say this amounts to the same thing, because the rich
have better opportunities for education.  That's a valid point.  It
is still possible, to a degree, to buy your kids' way into top
colleges by sending them to private schools that in effect hack the
college admissions process.According to a 2002 report by the National Center for Education
Statistics, about 1.7% of American kids attend private, non-sectarian
schools.  At Princeton, 36% of the class of 2007 came from such
schools.  (Interestingly, the number at Harvard is significantly
lower, about 28%.)  Obviously this is a huge loophole.  It does at
least seem to be closing, not widening.Perhaps the designers of admissions processes should take a lesson
from the example of computer security, and instead of just assuming
that their system can't be hacked, measure the degree to which it
is.

April 2009I usually avoid politics, but since we now seem to have an administration that's open to suggestions, I'm going to risk making one.  The single biggest thing the government could do to increase the number of startups in this country is a policy that would cost nothing: establish a new class of visa for startup founders.The biggest constraint on the number of new startups that get created in the US is not tax policy or employment law or even Sarbanes-Oxley.  It's that we won't let the people who want to start them into the country.Letting just 10,000 startup founders into the country each year could have a visible effect on the economy.  If we assume 4 people per startup, which is probably an overestimate, that's 2500 new companies.  Each year.  They wouldn't all grow as big as Google, but out of 2500 some would come close.By definition these 10,000 founders wouldn't be taking jobs from Americans: it could be part of the terms of the visa that they couldn't work for existing companies, only new ones they'd founded.  In fact they'd cause there to be 
more jobs for Americans, because the companies they started would hire more employees as they grew.The tricky part might seem to be how one defined a startup. But that could be solved quite easily: let the market decide.  Startup investors work hard to find the best startups.  The government could not do better than to piggyback on their expertise, and use investment by recognized startup investors as the test of whether a company was a real startup.How would the government decide who's a startup investor?  The same way they decide what counts as a university for student visas. We'll establish our own accreditation procedure. We know who one another are.10,000 people is a drop in the bucket by immigration standards, but would represent a huge increase in the pool of startup founders.  I think this would have such a visible effect on the economy that it would make the legislator who introduced the bill famous.  The only way to know for sure would be to try it, and that would cost practically nothing.
Thanks to Trevor Blackwell, Paul Buchheit, Jeff Clavier, David Hornik, Jessica Livingston, Greg Mcadoo, Aydin Senkut, and Fred Wilson for reading drafts of this.Related:January 2016Life is short, as everyone knows. When I was a kid I used to wonder
about this. Is life actually short, or are we really complaining
about its finiteness?  Would we be just as likely to feel life was
short if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped
wondering about it.  Then I had kids.  That gave me a way to answer
the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,
into discrete quantities. You only get 52 weekends with your 2 year
old.  If Christmas-as-magic lasts from say ages 3 to 10, you only
get to watch your child experience it 8 times.  And while it's
impossible to say what is a lot or a little of a continuous quantity
like time, 8 is not a lot of something.  If you had a handful of 8
peanuts, or a shelf of 8 books to choose from, the quantity would
definitely seem limited, no matter what your lifespan was.Ok, so life actually is short.  Does it make any difference to know
that?It has for me.  It means arguments of the form "Life is too short
for x" have great force.  It's not just a figure of speech to say
that life is too short for something.  It's not just a synonym for
annoying.  If you find yourself thinking that life is too short for
something, you should try to eliminate it if you can.When I ask myself what I've found life is too short for, the word
that pops into my head is "bullshit." I realize that answer is
somewhat tautological.  It's almost the definition of bullshit that
it's the stuff that life is too short for.  And yet bullshit does
have a distinctive character.  There's something fake about it.
It's the junk food of experience.
[1]If you ask yourself what you spend your time on that's bullshit,
you probably already know the answer.  Unnecessary meetings, pointless
disputes, bureaucracy, posturing, dealing with other people's
mistakes, traffic jams, addictive but unrewarding pastimes.There are two ways this kind of thing gets into your life: it's
either forced on you, or it tricks you.  To some extent you have to
put up with the bullshit forced on you by circumstances.  You need
to make money, and making money consists mostly of errands.  Indeed,
the law of supply and demand insures that: the more rewarding some
kind of work is, the cheaper people will do it.  It may be that
less bullshit is forced on you than you think, though.  There has
always been a stream of people who opt out of the default grind and
go live somewhere where opportunities are fewer in the conventional
sense, but life feels more authentic.  This could become more common.You can do it on a smaller scale without moving.  The amount of
time you have to spend on bullshit varies between employers.  Most
large organizations (and many small ones) are steeped in it.  But
if you consciously prioritize bullshit avoidance over other factors
like money and prestige, you can probably find employers that will
waste less of your time.If you're a freelancer or a small company, you can do this at the
level of individual customers.  If you fire or avoid toxic customers,
you can decrease the amount of bullshit in your life by more than
you decrease your income.But while some amount of bullshit is inevitably forced on you, the
bullshit that sneaks into your life by tricking you is no one's
fault but your own.  And yet the bullshit you choose may be harder
to eliminate than the bullshit that's forced on you.  Things that
lure you into wasting your time have to be really good at
tricking you.  An example that will be familiar to a lot of people
is arguing online.  When someone
contradicts you, they're in a sense attacking you. Sometimes pretty
overtly.  Your instinct when attacked is to defend yourself.  But
like a lot of instincts, this one wasn't designed for the world we
now live in.  Counterintuitive as it feels, it's better most of
the time not to defend yourself.  Otherwise these people are literally
taking your life.
[2]Arguing online is only incidentally addictive. There are more
dangerous things than that. As I've written before, one byproduct
of technical progress is that things we like tend to become more
addictive.  Which means we will increasingly have to make a conscious
effort to avoid addictions  to stand outside ourselves and ask "is
this how I want to be spending my time?"As well as avoiding bullshit, one should actively seek out things
that matter.  But different things matter to different people, and
most have to learn what matters to them.  A few are lucky and realize
early on that they love math or taking care of animals or writing,
and then figure out a way to spend a lot of time doing it.  But
most people start out with a life that's a mix of things that
matter and things that don't, and only gradually learn to distinguish
between them.For the young especially, much of this confusion is induced by the
artificial situations they find themselves in. In middle school and
high school, what the other kids think of you seems the most important
thing in the world.  But when you ask adults what they got wrong
at that age, nearly all say they cared too much what other kids
thought of them.One heuristic for distinguishing stuff that matters is to ask
yourself whether you'll care about it in the future.  Fake stuff
that matters usually has a sharp peak of seeming to matter.  That's
how it tricks you.  The area under the curve is small, but its shape
jabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would
call "important."  Having coffee with a friend matters.  You won't
feel later like that was a waste of time.One great thing about having small children is that they make you
spend time on things that matter: them. They grab your sleeve as
you're staring at your phone and say "will you play with me?" And
odds are that is in fact the bullshit-minimizing option.If life is short, we should expect its shortness to take us by
surprise. And that is just what tends to happen.  You take things
for granted, and then they're gone.  You think you can always write
that book, or climb that mountain, or whatever, and then you realize
the window has closed.  The saddest windows close when other people
die. Their lives are short too.  After my mother died, I wished I'd
spent more time with her.  I lived as if she'd always be there.
And in her typical quiet way she encouraged that illusion.  But an
illusion it was. I think a lot of people make the same mistake I
did.The usual way to avoid being taken by surprise by something is to
be consciously aware of it.  Back when life was more precarious,
people used to be aware of death to a degree that would now seem a
bit morbid.  I'm not sure why, but it doesn't seem the right answer
to be constantly reminding oneself of the grim reaper hovering at
everyone's shoulder.  Perhaps a better solution is to look at the
problem from the other end. Cultivate a habit of impatience about
the things you most want to do. Don't wait before climbing that
mountain or writing that book or visiting your mother.  You don't
need to be constantly reminding yourself why you shouldn't wait.
Just don't wait.I can think of two more things one does when one doesn't have much
of something: try to get more of it, and savor what one has.  Both
make sense here.How you live affects how long you live.  Most people could do better.
Me among them.But you can probably get even more effect by paying closer attention
to the time you have.  It's easy to let the days rush by.  The
"flow" that imaginative people love so much has a darker cousin
that prevents you from pausing to savor life amid the daily slurry
of errands and alarms.  One of the most striking things I've read
was not in a book, but the title of one: James Salter's Burning
the Days.It is possible to slow time somewhat. I've gotten better at it.
Kids help.  When you have small children, there are a lot of moments
so perfect that you can't help noticing.It does help too to feel that you've squeezed everything out of
some experience.  The reason I'm sad about my mother is not just
that I miss her but that I think of all the things we could have
done that we didn't.  My oldest son will be 7 soon.  And while I
miss the 3 year old version of him, I at least don't have any regrets
over what might have been.  We had the best time a daddy and a 3
year old ever had.Relentlessly prune bullshit, don't wait to do things that matter,
and savor the time you have.  That's what you do when life is short.Notes[1]
At first I didn't like it that the word that came to mind was
one that had other meanings.  But then I realized the other meanings
are fairly closely related.  Bullshit in the sense of things you
waste your time on is a lot like intellectual bullshit.[2]
I chose this example deliberately as a note to self.  I get
attacked a lot online.  People tell the craziest lies about me.
And I have so far done a pretty mediocre job of suppressing the
natural human inclination to say "Hey, that's not true!"Thanks to Jessica Livingston and Geoff Ralston for reading drafts
of this.October 2015This will come as a surprise to a lot of people, but in some cases
it's possible to detect bias in a selection process without knowing
anything about the applicant pool.  Which is exciting because among
other things it means third parties can use this technique to detect
bias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least
a random sample of the applicants that were selected, (b) their
subsequent performance is measured, and (c) the groups of
applicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What
it means for a selection process to be biased against applicants
of type x is that it's harder for them to make it through.  Which
means applicants of type x have to be better to get selected than
applicants not of type x.
[1]
Which means applicants of type x
who do make it through the selection process will outperform other
successful applicants.  And if the performance of all the successful
applicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid
one.  And in particular it must not be invalidated by the bias you're
trying to measure.
But there are some domains where performance can be measured, and
in those detecting bias is straightforward. Want to know if the
selection process was biased against some type of applicant?  Check
whether they outperform the others.  This is not just a heuristic
for detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased
against female founders. This would be easy to detect: among their
portfolio companies, do startups with female founders outperform
those without?  A couple months ago, one VC firm (almost certainly
unintentionally) published a study showing bias of this type. First
Round Capital found that among its portfolio companies, startups
with female founders outperformed
those without by 63%. 
[2]The reason I began by saying that this technique would come as a
surprise to many people is that we so rarely see analyses of this
type.  I'm sure it will come as a surprise to First Round that they
performed one. I doubt anyone there realized that by limiting their
sample to their own portfolio, they were producing a study not of
startup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The
information needed to conduct such studies is increasingly available.
Data about who applies for things is usually closely guarded by the
organizations selecting them, but nowadays data about who gets
selected is often publicly available to anyone who takes the trouble
to aggregate it.
Notes[1]
This technique wouldn't work if the selection process looked
for different things from different types of applicants—for
example, if an employer hired men based on their ability but women
based on their appearance.[2]
As Paul Buchheit points out, First Round excluded their most 
successful investment, Uber, from the study.  And while it 
makes sense to exclude outliers from some types of studies, 
studies of returns from startup investing, which is all about 
hitting outliers, are not one of them.
Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading
drafts of this.February 2021Before college the two main things I worked on, outside of school,
were writing and programming. I didn't write essays. I wrote what
beginning writers were supposed to write then, and probably still
are: short stories. My stories were awful. They had hardly any plot,
just characters with strong feelings, which I imagined made them
deep.The first programs I tried writing were on the IBM 1401 that our
school district used for what was then called "data processing."
This was in 9th grade, so I was 13 or 14. The school district's
1401 happened to be in the basement of our junior high school, and
my friend Rich Draves and I got permission to use it. It was like
a mini Bond villain's lair down there, with all these alien-looking
machines  CPU, disk drives, printer, card reader  sitting up
on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to
type programs on punch cards, then stack them in the card reader
and press a button to load the program into memory and run it. The
result would ordinarily be to print something on the spectacularly
loud printer.I was puzzled by the 1401. I couldn't figure out what to do with
it. And in retrospect there's not much I could have done with it.
The only form of input to programs was data stored on punched cards,
and I didn't have any data stored on punched cards. The only other
option was to do things that didn't rely on any input, like calculate
approximations of pi, but I didn't know enough math to do anything
interesting of that type. So I'm not surprised I can't remember any
programs I wrote, because they can't have done much. My clearest
memory is of the moment I learned it was possible for programs not
to terminate, when one of mine didn't. On a machine without
time-sharing, this was a social as well as a technical error, as
the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a
computer sitting right in front of you, on a desk, that could respond
to your keystrokes as it was running instead of just churning through
a stack of punch cards and then stopping. 
[1]The first of my friends to get a microcomputer built it himself.
It was sold as a kit by Heathkit. I remember vividly how impressed
and envious I felt watching him sitting in front of it, typing
programs right into the computer.Computers were expensive in those days and it took me years of
nagging before I convinced my father to buy one, a TRS-80, in about
1980. The gold standard then was the Apple II, but a TRS-80 was
good enough. This was when I really started programming. I wrote
simple games, a program to predict how high my model rockets would
fly, and a word processor that my father used to write at least one
book. There was only room in memory for about 2 pages of text, so
he'd write 2 pages at a time and then print them out, but it was a
lot better than a typewriter.Though I liked programming, I didn't plan to study it in college.
In college I was going to study philosophy, which sounded much more
powerful. It seemed, to my naive high school self, to be the study
of the ultimate truths, compared to which the things studied in
other fields would be mere domain knowledge. What I discovered when
I got to college was that the other fields took up so much of the
space of ideas that there wasn't much left for these supposed
ultimate truths. All that seemed left for philosophy were edge cases
that people in other fields felt could safely be ignored.I couldn't have put this into words when I was 18. All I knew at
the time was that I kept taking philosophy courses and they kept
being boring. So I decided to switch to AI.AI was in the air in the mid 1980s, but there were two things
especially that made me want to work on it: a novel by Heinlein
called The Moon is a Harsh Mistress, which featured an intelligent
computer called Mike, and a PBS documentary that showed Terry
Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh
Mistress, so I don't know how well it has aged, but when I read it
I was drawn entirely into its world. It seemed only a matter of
time before we'd have Mike, and when I saw Winograd using SHRDLU,
it seemed like that time would be a few years at most. All you had
to do was teach SHRDLU more words.There weren't any classes in AI at Cornell then, not even graduate
classes, so I started trying to teach myself. Which meant learning
Lisp, since in those days Lisp was regarded as the language of AI.
The commonly used programming languages then were pretty primitive,
and programmers' ideas correspondingly so. The default language at
Cornell was a Pascal-like language called PL/I, and the situation
was similar elsewhere. Learning Lisp expanded my concept of a program
so fast that it was years before I started to have a sense of where
the new limits were. This was more like it; this was what I had
expected college to do. It wasn't happening in a class, like it was
supposed to, but that was ok. For the next couple years I was on a
roll. I knew what I was going to do.For my undergraduate thesis, I reverse-engineered SHRDLU. My God
did I love working on that program. It was a pleasing bit of code,
but what made it even more exciting was my belief  hard to imagine
now, but not unique in 1985  that it was already climbing the
lower slopes of intelligence.I had gotten into a program at Cornell that didn't make you choose
a major. You could take whatever classes you liked, and choose
whatever you liked to put on your degree. I of course chose "Artificial
Intelligence." When I got the actual physical diploma, I was dismayed
to find that the quotes had been included, which made them read as
scare-quotes. At the time this bothered me, but now it seems amusingly
accurate, for reasons I was about to discover.I applied to 3 grad schools: MIT and Yale, which were renowned for
AI at the time, and Harvard, which I'd visited because Rich Draves
went there, and was also home to Bill Woods, who'd invented the
type of parser I used in my SHRDLU clone. Only Harvard accepted me,
so that was where I went.I don't remember the moment it happened, or if there even was a
specific moment, but during the first year of grad school I realized
that AI, as practiced at the time, was a hoax. By which I mean the
sort of AI in which a program that's told "the dog is sitting on
the chair" translates this into some formal representation and adds
it to the list of things it knows.What these programs really showed was that there's a subset of
natural language that's a formal language. But a very proper subset.
It was clear that there was an unbridgeable gap between what they
could do and actually understanding natural language. It was not,
in fact, simply a matter of teaching SHRDLU more words. That whole
way of doing AI, with explicit data structures representing concepts,
was not going to work. Its brokenness did, as so often happens,
generate a lot of opportunities to write papers about various
band-aids that could be applied to it, but it was never going to
get us Mike.So I looked around to see what I could salvage from the wreckage
of my plans, and there was Lisp. I knew from experience that Lisp
was interesting for its own sake and not just for its association
with AI, even though that was the main reason people cared about
it at the time. So I decided to focus on Lisp. In fact, I decided
to write a book about Lisp hacking. It's scary to think how little
I knew about Lisp hacking when I started writing that book. But
there's nothing like writing a book about something to help you
learn it. The book, On Lisp, wasn't published till 1993, but I wrote
much of it in grad school.Computer Science is an uneasy alliance between two halves, theory
and systems. The theory people prove things, and the systems people
build things. I wanted to build things. I had plenty of respect for
theory  indeed, a sneaking suspicion that it was the more admirable
of the two halves  but building things seemed so much more exciting.The problem with systems work, though, was that it didn't last.
Any program you wrote today, no matter how good, would be obsolete
in a couple decades at best. People might mention your software in
footnotes, but no one would actually use it. And indeed, it would
seem very feeble work. Only people with a sense of the history of
the field would even realize that, in its time, it had been good.There were some surplus Xerox Dandelions floating around the computer
lab at one point. Anyone who wanted one to play around with could
have one. I was briefly tempted, but they were so slow by present
standards; what was the point? No one else wanted one either, so
off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would
last.In this dissatisfied state I went in 1988 to visit Rich Draves at
CMU, where he was in grad school. One day I went to visit the
Carnegie Institute, where I'd spent a lot of time as a kid. While
looking at a painting there I realized something that might seem
obvious, but was a big surprise to me. There, right on the wall,
was something you could make that would last. Paintings didn't
become obsolete. Some of the best ones were hundreds of years old.And moreover this was something you could make a living doing. Not
as easily as you could by writing software, of course, but I thought
if you were really industrious and lived really cheaply, it had to
be possible to make enough to survive. And as an artist you could
be truly independent. You wouldn't have a boss, or even need to get
research funding.I had always liked looking at paintings. Could I make them? I had
no idea. I'd never imagined it was even possible. I knew intellectually
that people made art  that it didn't just appear spontaneously
 but it was as if the people who made it were a different species.
They either lived long ago or were mysterious geniuses doing strange
things in profiles in Life magazine. The idea of actually being
able to make art, to put that verb before that noun, seemed almost
miraculous.That fall I started taking art classes at Harvard. Grad students
could take classes in any department, and my advisor, Tom Cheatham,
was very easy going. If he even knew about the strange classes I
was taking, he never said anything.So now I was in a PhD program in computer science, yet planning to
be an artist, yet also genuinely in love with Lisp hacking and
working away at On Lisp. In other words, like many a grad student,
I was working energetically on multiple projects that were not my
thesis.I didn't see a way out of this situation. I didn't want to drop out
of grad school, but how else was I going to get out? I remember
when my friend Robert Morris got kicked out of Cornell for writing
the internet worm of 1988, I was envious that he'd found such a
spectacular way to get out of grad school.Then one day in April 1990 a crack appeared in the wall. I ran into
professor Cheatham and he asked if I was far enough along to graduate
that June. I didn't have a word of my dissertation written, but in
what must have been the quickest bit of thinking in my life, I
decided to take a shot at writing one in the 5 weeks or so that
remained before the deadline, reusing parts of On Lisp where I
could, and I was able to respond, with no perceptible delay "Yes,
I think so. I'll give you something to read in a few days."I picked applications of continuations as the topic. In retrospect
I should have written about macros and embedded languages. There's
a whole world there that's barely been explored. But all I wanted
was to get out of grad school, and my rapidly written dissertation
sufficed, just barely.Meanwhile I was applying to art schools. I applied to two: RISD in
the US, and the Accademia di Belli Arti in Florence, which, because
it was the oldest art school, I imagined would be good. RISD accepted
me, and I never heard back from the Accademia, so off to Providence
I went.I'd applied for the BFA program at RISD, which meant in effect that
I had to go to college again. This was not as strange as it sounds,
because I was only 25, and art schools are full of people of different
ages. RISD counted me as a transfer sophomore and said I had to do
the foundation that summer. The foundation means the classes that
everyone has to take in fundamental subjects like drawing, color,
and design.Toward the end of the summer I got a big surprise: a letter from
the Accademia, which had been delayed because they'd sent it to
Cambridge England instead of Cambridge Massachusetts, inviting me
to take the entrance exam in Florence that fall. This was now only
weeks away. My nice landlady let me leave my stuff in her attic. I
had some money saved from consulting work I'd done in grad school;
there was probably enough to last a year if I lived cheaply. Now
all I had to do was learn Italian.Only stranieri (foreigners) had to take this entrance exam. In
retrospect it may well have been a way of excluding them, because
there were so many stranieri attracted by the idea of studying
art in Florence that the Italian students would otherwise have been
outnumbered. I was in decent shape at painting and drawing from the
RISD foundation that summer, but I still don't know how I managed
to pass the written exam. I remember that I answered the essay
question by writing about Cezanne, and that I cranked up the
intellectual level as high as I could to make the most of my limited
vocabulary. 
[2]I'm only up to age 25 and already there are such conspicuous patterns.
Here I was, yet again about to attend some august institution in
the hopes of learning about some prestigious subject, and yet again
about to be disappointed. The students and faculty in the painting
department at the Accademia were the nicest people you could imagine,
but they had long since arrived at an arrangement whereby the
students wouldn't require the faculty to teach anything, and in
return the faculty wouldn't require the students to learn anything.
And at the same time all involved would adhere outwardly to the
conventions of a 19th century atelier. We actually had one of those
little stoves, fed with kindling, that you see in 19th century
studio paintings, and a nude model sitting as close to it as possible
without getting burned. Except hardly anyone else painted her besides
me. The rest of the students spent their time chatting or occasionally
trying to imitate things they'd seen in American art magazines.Our model turned out to live just down the street from me. She made
a living from a combination of modelling and making fakes for a
local antique dealer. She'd copy an obscure old painting out of a
book, and then he'd take the copy and maltreat it to make it look
old. 
[3]While I was a student at the Accademia I started painting still
lives in my bedroom at night. These paintings were tiny, because
the room was, and because I painted them on leftover scraps of
canvas, which was all I could afford at the time. Painting still
lives is different from painting people, because the subject, as
its name suggests, can't move. People can't sit for more than about
15 minutes at a time, and when they do they don't sit very still.
So the traditional m.o. for painting people is to know how to paint
a generic person, which you then modify to match the specific person
you're painting. Whereas a still life you can, if you want, copy
pixel by pixel from what you're seeing. You don't want to stop
there, of course, or you get merely photographic accuracy, and what
makes a still life interesting is that it's been through a head.
You want to emphasize the visual cues that tell you, for example,
that the reason the color changes suddenly at a certain point is
that it's the edge of an object. By subtly emphasizing such things
you can make paintings that are more realistic than photographs not
just in some metaphorical sense, but in the strict information-theoretic
sense. 
[4]I liked painting still lives because I was curious about what I was
seeing. In everyday life, we aren't consciously aware of much we're
seeing. Most visual perception is handled by low-level processes
that merely tell your brain "that's a water droplet" without telling
you details like where the lightest and darkest points are, or
"that's a bush" without telling you the shape and position of every
leaf. This is a feature of brains, not a bug. In everyday life it
would be distracting to notice every leaf on every bush. But when
you have to paint something, you have to look more closely, and
when you do there's a lot to see. You can still be noticing new
things after days of trying to paint something people usually take
for granted, just as you can  after
days of trying to write an essay about something people usually
take for granted.This is not the only way to paint. I'm not 100% sure it's even a
good way to paint. But it seemed a good enough bet to be worth
trying.Our teacher, professor Ulivi, was a nice guy. He could see I worked
hard, and gave me a good grade, which he wrote down in a sort of
passport each student had. But the Accademia wasn't teaching me
anything except Italian, and my money was running out, so at the
end of the first year I went back to the US.I wanted to go back to RISD, but I was now broke and RISD was very
expensive, so I decided to get a job for a year and then return to
RISD the next fall. I got one at a company called Interleaf, which
made software for creating documents. You mean like Microsoft Word?
Exactly. That was how I learned that low end software tends to eat
high end software. But Interleaf still had a few years to live yet.
[5]Interleaf had done something pretty bold. Inspired by Emacs, they'd
added a scripting language, and even made the scripting language a
dialect of Lisp. Now they wanted a Lisp hacker to write things in
it. This was the closest thing I've had to a normal job, and I
hereby apologize to my boss and coworkers, because I was a bad
employee. Their Lisp was the thinnest icing on a giant C cake, and
since I didn't know C and didn't want to learn it, I never understood
most of the software. Plus I was terribly irresponsible. This was
back when a programming job meant showing up every day during certain
working hours. That seemed unnatural to me, and on this point the
rest of the world is coming around to my way of thinking, but at
the time it caused a lot of friction. Toward the end of the year I
spent much of my time surreptitiously working on On Lisp, which I
had by this time gotten a contract to publish.The good part was that I got paid huge amounts of money, especially
by art student standards. In Florence, after paying my part of the
rent, my budget for everything else had been $7 a day. Now I was
getting paid more than 4 times that every hour, even when I was
just sitting in a meeting. By living cheaply I not only managed to
save enough to go back to RISD, but also paid off my college loans.I learned some useful things at Interleaf, though they were mostly
about what not to do. I learned that it's better for technology
companies to be run by product people than sales people (though
sales is a real skill and people who are good at it are really good
at it), that it leads to bugs when code is edited by too many people,
that cheap office space is no bargain if it's depressing, that
planned meetings are inferior to corridor conversations, that big,
bureaucratic customers are a dangerous source of money, and that
there's not much overlap between conventional office hours and the
optimal time for hacking, or conventional offices and the optimal
place for it.But the most important thing I learned, and which I used in both
Viaweb and Y Combinator, is that the low end eats the high end:
that it's good to be the "entry level" option, even though that
will be less prestigious, because if you're not, someone else will
be, and will squash you against the ceiling. Which in turn means
that prestige is a danger sign.When I left to go back to RISD the next fall, I arranged to do
freelance work for the group that did projects for customers, and
this was how I survived for the next several years. When I came
back to visit for a project later on, someone told me about a new
thing called HTML, which was, as he described it, a derivative of
SGML. Markup language enthusiasts were an occupational hazard at
Interleaf and I ignored him, but this HTML thing later became a big
part of my life.In the fall of 1992 I moved back to Providence to continue at RISD.
The foundation had merely been intro stuff, and the Accademia had
been a (very civilized) joke. Now I was going to see what real art
school was like. But alas it was more like the Accademia than not.
Better organized, certainly, and a lot more expensive, but it was
now becoming clear that art school did not bear the same relationship
to art that medical school bore to medicine. At least not the
painting department. The textile department, which my next door
neighbor belonged to, seemed to be pretty rigorous. No doubt
illustration and architecture were too. But painting was post-rigorous.
Painting students were supposed to express themselves, which to the
more worldly ones meant to try to cook up some sort of distinctive
signature style.A signature style is the visual equivalent of what in show business
is known as a "schtick": something that immediately identifies the
work as yours and no one else's. For example, when you see a painting
that looks like a certain kind of cartoon, you know it's by Roy
Lichtenstein. So if you see a big painting of this type hanging in
the apartment of a hedge fund manager, you know he paid millions
of dollars for it. That's not always why artists have a signature
style, but it's usually why buyers pay a lot for such work.
[6]There were plenty of earnest students too: kids who "could draw"
in high school, and now had come to what was supposed to be the
best art school in the country, to learn to draw even better. They
tended to be confused and demoralized by what they found at RISD,
but they kept going, because painting was what they did. I was not
one of the kids who could draw in high school, but at RISD I was
definitely closer to their tribe than the tribe of signature style
seekers.I learned a lot in the color class I took at RISD, but otherwise I
was basically teaching myself to paint, and I could do that for
free. So in 1993 I dropped out. I hung around Providence for a bit,
and then my college friend Nancy Parmet did me a big favor. A
rent-controlled apartment in a building her mother owned in New
York was becoming vacant. Did I want it? It wasn't much more than
my current place, and New York was supposed to be where the artists
were. So yes, I wanted it!
[7]Asterix comics begin by zooming in on a tiny corner of Roman Gaul
that turns out not to be controlled by the Romans. You can do
something similar on a map of New York City: if you zoom in on the
Upper East Side, there's a tiny corner that's not rich, or at least
wasn't in 1993. It's called Yorkville, and that was my new home.
Now I was a New York artist  in the strictly technical sense of
making paintings and living in New York.I was nervous about money, because I could sense that Interleaf was
on the way down. Freelance Lisp hacking work was very rare, and I
didn't want to have to program in another language, which in those
days would have meant C++ if I was lucky. So with my unerring nose
for financial opportunity, I decided to write another book on Lisp.
This would be a popular book, the sort of book that could be used
as a textbook. I imagined myself living frugally off the royalties
and spending all my time painting. (The painting on the cover of
this book, ANSI Common Lisp, is one that I painted around this
time.)The best thing about New York for me was the presence of Idelle and
Julian Weber. Idelle Weber was a painter, one of the early
photorealists, and I'd taken her painting class at Harvard. I've
never known a teacher more beloved by her students. Large numbers
of former students kept in touch with her, including me. After I
moved to New York I became her de facto studio assistant.She liked to paint on big, square canvases, 4 to 5 feet on a side.
One day in late 1994 as I was stretching one of these monsters there
was something on the radio about a famous fund manager. He wasn't
that much older than me, and was super rich. The thought suddenly
occurred to me: why don't I become rich? Then I'll be able to work
on whatever I want.Meanwhile I'd been hearing more and more about this new thing called
the World Wide Web. Robert Morris showed it to me when I visited
him in Cambridge, where he was now in grad school at Harvard. It
seemed to me that the web would be a big deal. I'd seen what graphical
user interfaces had done for the popularity of microcomputers. It
seemed like the web would do the same for the internet.If I wanted to get rich, here was the next train leaving the station.
I was right about that part. What I got wrong was the idea. I decided
we should start a company to put art galleries online. I can't
honestly say, after reading so many Y Combinator applications, that
this was the worst startup idea ever, but it was up there. Art
galleries didn't want to be online, and still don't, not the fancy
ones. That's not how they sell. I wrote some software to generate
web sites for galleries, and Robert wrote some to resize images and
set up an http server to serve the pages. Then we tried to sign up
galleries. To call this a difficult sale would be an understatement.
It was difficult to give away. A few galleries let us make sites
for them for free, but none paid us.Then some online stores started to appear, and I realized that
except for the order buttons they were identical to the sites we'd
been generating for galleries. This impressive-sounding thing called
an "internet storefront" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready copy
of ANSI Common Lisp to the publishers, we started trying to write
software to build online stores. At first this was going to be
normal desktop software, which in those days meant Windows software.
That was an alarming prospect, because neither of us knew how to
write Windows software or wanted to learn. We lived in the Unix
world. But we decided we'd at least try writing a prototype store
builder on Unix. Robert wrote a shopping cart, and I wrote a new
site generator for stores  in Lisp, of course.We were working out of Robert's apartment in Cambridge. His roommate
was away for big chunks of time, during which I got to sleep in his
room. For some reason there was no bed frame or sheets, just a
mattress on the floor. One morning as I was lying on this mattress
I had an idea that made me sit up like a capital L. What if we ran
the software on the server, and let users control it by clicking
on links? Then we'd never have to write anything to run on users'
computers. We could generate the sites on the same server we'd serve
them from. Users wouldn't need anything more than a browser.This kind of software, known as a web app, is common now, but at
the time it wasn't clear that it was even possible. To find out,
we decided to try making a version of our store builder that you
could control through the browser. A couple days later, on August
12, we had one that worked. The UI was horrible, but it proved you
could build a whole store through the browser, without any client
software or typing anything into the command line on the server.Now we felt like we were really onto something. I had visions of a
whole new generation of software working this way. You wouldn't
need versions, or ports, or any of that crap. At Interleaf there
had been a whole group called Release Engineering that seemed to
be at least as big as the group that actually wrote the software.
Now you could just update the software right on the server.We started a new company we called Viaweb, after the fact that our
software worked via the web, and we got $10,000 in seed funding
from Idelle's husband Julian. In return for that and doing the
initial legal work and giving us business advice, we gave him 10%
of the company. Ten years later this deal became the model for Y
Combinator's. We knew founders needed something like this, because
we'd needed it ourselves.At this stage I had a negative net worth, because the thousand
dollars or so I had in the bank was more than counterbalanced by
what I owed the government in taxes. (Had I diligently set aside
the proper proportion of the money I'd made consulting for Interleaf?
No, I had not.) So although Robert had his graduate student stipend,
I needed that seed funding to live on.We originally hoped to launch in September, but we got more ambitious
about the software as we worked on it. Eventually we managed to
build a WYSIWYG site builder, in the sense that as you were creating
pages, they looked exactly like the static ones that would be
generated later, except that instead of leading to static pages,
the links all referred to closures stored in a hash table on the
server.It helped to have studied art, because the main goal of an online
store builder is to make users look legit, and the key to looking
legit is high production values. If you get page layouts and fonts
and colors right, you can make a guy running a store out of his
bedroom look more legit than a big company.(If you're curious why my site looks so old-fashioned, it's because
it's still made with this software. It may look clunky today, but
in 1996 it was the last word in slick.)In September, Robert rebelled. "We've been working on this for a
month," he said, "and it's still not done." This is funny in
retrospect, because he would still be working on it almost 3 years
later. But I decided it might be prudent to recruit more programmers,
and I asked Robert who else in grad school with him was really good.
He recommended Trevor Blackwell, which surprised me at first, because
at that point I knew Trevor mainly for his plan to reduce everything
in his life to a stack of notecards, which he carried around with
him. But Rtm was right, as usual. Trevor turned out to be a
frighteningly effective hacker.It was a lot of fun working with Robert and Trevor. They're the two
most independent-minded people 
I know, and in completely different
ways. If you could see inside Rtm's brain it would look like a
colonial New England church, and if you could see inside Trevor's
it would look like the worst excesses of Austrian Rococo.We opened for business, with 6 stores, in January 1996. It was just
as well we waited a few months, because although we worried we were
late, we were actually almost fatally early. There was a lot of
talk in the press then about ecommerce, but not many people actually
wanted online stores.
[8]There were three main parts to the software: the editor, which
people used to build sites and which I wrote, the shopping cart,
which Robert wrote, and the manager, which kept track of orders and
statistics, and which Trevor wrote. In its time, the editor was one
of the best general-purpose site builders. I kept the code tight
and didn't have to integrate with any other software except Robert's
and Trevor's, so it was quite fun to work on. If all I'd had to do
was work on this software, the next 3 years would have been the
easiest of my life. Unfortunately I had to do a lot more, all of
it stuff I was worse at than programming, and the next 3 years were
instead the most stressful.There were a lot of startups making ecommerce software in the second
half of the 90s. We were determined to be the Microsoft Word, not
the Interleaf. Which meant being easy to use and inexpensive. It
was lucky for us that we were poor, because that caused us to make
Viaweb even more inexpensive than we realized. We charged $100 a
month for a small store and $300 a month for a big one. This low
price was a big attraction, and a constant thorn in the sides of
competitors, but it wasn't because of some clever insight that we
set the price low. We had no idea what businesses paid for things.
$300 a month seemed like a lot of money to us.We did a lot of things right by accident like that. For example,
we did what's now called "doing things that 
don't scale," although
at the time we would have described it as "being so lame that we're
driven to the most desperate measures to get users." The most common
of which was building stores for them. This seemed particularly
humiliating, since the whole raison d'etre of our software was that
people could use it to make their own stores. But anything to get
users.We learned a lot more about retail than we wanted to know. For
example, that if you could only have a small image of a man's shirt
(and all images were small then by present standards), it was better
to have a closeup of the collar than a picture of the whole shirt.
The reason I remember learning this was that it meant I had to
rescan about 30 images of men's shirts. My first set of scans were
so beautiful too.Though this felt wrong, it was exactly the right thing to be doing.
Building stores for users taught us about retail, and about how it
felt to use our software. I was initially both mystified and repelled
by "business" and thought we needed a "business person" to be in
charge of it, but once we started to get users, I was converted,
in much the same way I was converted to 
fatherhood once I had kids.
Whatever users wanted, I was all theirs. Maybe one day we'd have
so many users that I couldn't scan their images for them, but in
the meantime there was nothing more important to do.Another thing I didn't get at the time is that 
growth rate is the
ultimate test of a startup. Our growth rate was fine. We had about
70 stores at the end of 1996 and about 500 at the end of 1997. I
mistakenly thought the thing that mattered was the absolute number
of users. And that is the thing that matters in the sense that
that's how much money you're making, and if you're not making enough,
you might go out of business. But in the long term the growth rate
takes care of the absolute number. If we'd been a startup I was
advising at Y Combinator, I would have said: Stop being so stressed
out, because you're doing fine. You're growing 7x a year. Just don't
hire too many more people and you'll soon be profitable, and then
you'll control your own destiny.Alas I hired lots more people, partly because our investors wanted
me to, and partly because that's what startups did during the
Internet Bubble. A company with just a handful of employees would
have seemed amateurish. So we didn't reach breakeven until about
when Yahoo bought us in the summer of 1998. Which in turn meant we
were at the mercy of investors for the entire life of the company.
And since both we and our investors were noobs at startups, the
result was a mess even by startup standards.It was a huge relief when Yahoo bought us. In principle our Viaweb
stock was valuable. It was a share in a business that was profitable
and growing rapidly. But it didn't feel very valuable to me; I had
no idea how to value a business, but I was all too keenly aware of
the near-death experiences we seemed to have every few months. Nor
had I changed my grad student lifestyle significantly since we
started. So when Yahoo bought us it felt like going from rags to
riches. Since we were going to California, I bought a car, a yellow
1998 VW GTI. I remember thinking that its leather seats alone were
by far the most luxurious thing I owned.The next year, from the summer of 1998 to the summer of 1999, must
have been the least productive of my life. I didn't realize it at
the time, but I was worn out from the effort and stress of running
Viaweb. For a while after I got to California I tried to continue
my usual m.o. of programming till 3 in the morning, but fatigue
combined with Yahoo's prematurely aged
culture and grim cube farm
in Santa Clara gradually dragged me down. After a few months it
felt disconcertingly like working at Interleaf.Yahoo had given us a lot of options when they bought us. At the
time I thought Yahoo was so overvalued that they'd never be worth
anything, but to my astonishment the stock went up 5x in the next
year. I hung on till the first chunk of options vested, then in the
summer of 1999 I left. It had been so long since I'd painted anything
that I'd half forgotten why I was doing this. My brain had been
entirely full of software and men's shirts for 4 years. But I had
done this to get rich so I could paint, I reminded myself, and now
I was rich, so I should go paint.When I said I was leaving, my boss at Yahoo had a long conversation
with me about my plans. I told him all about the kinds of pictures
I wanted to paint. At the time I was touched that he took such an
interest in me. Now I realize it was because he thought I was lying.
My options at that point were worth about $2 million a month. If I
was leaving that kind of money on the table, it could only be to
go and start some new startup, and if I did, I might take people
with me. This was the height of the Internet Bubble, and Yahoo was
ground zero of it. My boss was at that moment a billionaire. Leaving
then to start a new startup must have seemed to him an insanely,
and yet also plausibly, ambitious plan.But I really was quitting to paint, and I started immediately.
There was no time to lose. I'd already burned 4 years getting rich.
Now when I talk to founders who are leaving after selling their
companies, my advice is always the same: take a vacation. That's
what I should have done, just gone off somewhere and done nothing
for a month or two, but the idea never occurred to me.So I tried to paint, but I just didn't seem to have any energy or
ambition. Part of the problem was that I didn't know many people
in California. I'd compounded this problem by buying a house up in
the Santa Cruz Mountains, with a beautiful view but miles from
anywhere. I stuck it out for a few more months, then in desperation
I went back to New York, where unless you understand about rent
control you'll be surprised to hear I still had my apartment, sealed
up like a tomb of my old life. Idelle was in New York at least, and
there were other people trying to paint there, even though I didn't
know any of them.When I got back to New York I resumed my old life, except now I was
rich. It was as weird as it sounds. I resumed all my old patterns,
except now there were doors where there hadn't been. Now when I was
tired of walking, all I had to do was raise my hand, and (unless
it was raining) a taxi would stop to pick me up. Now when I walked
past charming little restaurants I could go in and order lunch. It
was exciting for a while. Painting started to go better. I experimented
with a new kind of still life where I'd paint one painting in the
old way, then photograph it and print it, blown up, on canvas, and
then use that as the underpainting for a second still life, painted
from the same objects (which hopefully hadn't rotted yet).Meanwhile I looked for an apartment to buy. Now I could actually
choose what neighborhood to live in. Where, I asked myself and
various real estate agents, is the Cambridge of New York? Aided by
occasional visits to actual Cambridge, I gradually realized there
wasn't one. Huh.Around this time, in the spring of 2000, I had an idea. It was clear
from our experience with Viaweb that web apps were the future. Why
not build a web app for making web apps? Why not let people edit
code on our server through the browser, and then host the resulting
applications for them?
[9]
You could run all sorts of services
on the servers that these applications could use just by making an
API call: making and receiving phone calls, manipulating images,
taking credit card payments, etc.I got so excited about this idea that I couldn't think about anything
else. It seemed obvious that this was the future. I didn't particularly
want to start another company, but it was clear that this idea would
have to be embodied as one, so I decided to move to Cambridge and
start it. I hoped to lure Robert into working on it with me, but
there I ran into a hitch. Robert was now a postdoc at MIT, and
though he'd made a lot of money the last time I'd lured him into
working on one of my schemes, it had also been a huge time sink.
So while he agreed that it sounded like a plausible idea, he firmly
refused to work on it.Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had
worked for Viaweb, and two undergrads who wanted summer jobs, and
we got to work trying to build what it's now clear is about twenty
companies and several open source projects worth of software. The
language for defining applications would of course be a dialect of
Lisp. But I wasn't so naive as to assume I could spring an overt
Lisp on a general audience; we'd hide the parentheses, like Dylan
did.By then there was a name for the kind of company Viaweb was, an
"application service provider," or ASP. This name didn't last long
before it was replaced by "software as a service," but it was current
for long enough that I named this new company after it: it was going
to be called Aspra.I started working on the application builder, Dan worked on network
infrastructure, and the two undergrads worked on the first two
services (images and phone calls). But about halfway through the
summer I realized I really didn't want to run a company  especially
not a big one, which it was looking like this would have to be. I'd
only started Viaweb because I needed the money. Now that I didn't
need money anymore, why was I doing this? If this vision had to be
realized as a company, then screw the vision. I'd build a subset
that could be done as an open source project.Much to my surprise, the time I spent working on this stuff was not
wasted after all. After we started Y Combinator, I would often
encounter startups working on parts of this new architecture, and
it was very useful to have spent so much time thinking about it and
even trying to write some of it.The subset I would build as an open source project was the new Lisp,
whose parentheses I now wouldn't even have to hide. A lot of Lisp
hackers dream of building a new Lisp, partly because one of the
distinctive features of the language is that it has dialects, and
partly, I think, because we have in our minds a Platonic form of
Lisp that all existing dialects fall short of. I certainly did. So
at the end of the summer Dan and I switched to working on this new
dialect of Lisp, which I called Arc, in a house I bought in Cambridge.The following spring, lightning struck. I was invited to give a
talk at a Lisp conference, so I gave one about how we'd used Lisp
at Viaweb. Afterward I put a postscript file of this talk online,
on paulgraham.com, which I'd created years before using Viaweb but
had never used for anything. In one day it got 30,000 page views.
What on earth had happened? The referring urls showed that someone
had posted it on Slashdot.
[10]Wow, I thought, there's an audience. If I write something and put
it on the web, anyone can read it. That may seem obvious now, but
it was surprising then. In the print era there was a narrow channel
to readers, guarded by fierce monsters known as editors. The only
way to get an audience for anything you wrote was to get it published
as a book, or in a newspaper or magazine. Now anyone could publish
anything.This had been possible in principle since 1993, but not many people
had realized it yet. I had been intimately involved with building
the infrastructure of the web for most of that time, and a writer
as well, and it had taken me 8 years to realize it. Even then it
took me several years to understand the implications. It meant there
would be a whole new generation of 
essays.
[11]In the print era, the channel for publishing essays had been
vanishingly small. Except for a few officially anointed thinkers
who went to the right parties in New York, the only people allowed
to publish essays were specialists writing about their specialties.
There were so many essays that had never been written, because there
had been no way to publish them. Now they could be, and I was going
to write them.
[12]I've worked on several different things, but to the extent there
was a turning point where I figured out what to work on, it was
when I started publishing essays online. From then on I knew that
whatever else I did, I'd always write essays too.I knew that online essays would be a 
marginal medium at first.
Socially they'd seem more like rants posted by nutjobs on their
GeoCities sites than the genteel and beautifully typeset compositions
published in The New Yorker. But by this point I knew enough to
find that encouraging instead of discouraging.One of the most conspicuous patterns I've noticed in my life is how
well it has worked, for me at least, to work on things that weren't
prestigious. Still life has always been the least prestigious form
of painting. Viaweb and Y Combinator both seemed lame when we started
them. I still get the glassy eye from strangers when they ask what
I'm writing, and I explain that it's an essay I'm going to publish
on my web site. Even Lisp, though prestigious intellectually in
something like the way Latin is, also seems about as hip.It's not that unprestigious types of work are good per se. But when
you find yourself drawn to some kind of work despite its current
lack of prestige, it's a sign both that there's something real to
be discovered there, and that you have the right kind of motives.
Impure motives are a big danger for the ambitious. If anything is
going to lead you astray, it will be the desire to impress people.
So while working on things that aren't prestigious doesn't guarantee
you're on the right track, it at least guarantees you're not on the
most common type of wrong one.Over the next several years I wrote lots of essays about all kinds
of different topics. O'Reilly reprinted a collection of them as a
book, called Hackers & Painters after one of the essays in it. I
also worked on spam filters, and did some more painting. I used to
have dinners for a group of friends every thursday night, which
taught me how to cook for groups. And I bought another building in
Cambridge, a former candy factory (and later, twas said, porn
studio), to use as an office.One night in October 2003 there was a big party at my house. It was
a clever idea of my friend Maria Daniels, who was one of the thursday
diners. Three separate hosts would all invite their friends to one
party. So for every guest, two thirds of the other guests would be
people they didn't know but would probably like. One of the guests
was someone I didn't know but would turn out to like a lot: a woman
called Jessica Livingston. A couple days later I asked her out.Jessica was in charge of marketing at a Boston investment bank.
This bank thought it understood startups, but over the next year,
as she met friends of mine from the startup world, she was surprised
how different reality was. And how colorful their stories were. So
she decided to compile a book of 
interviews with startup founders.When the bank had financial problems and she had to fire half her
staff, she started looking for a new job. In early 2005 she interviewed
for a marketing job at a Boston VC firm. It took them weeks to make
up their minds, and during this time I started telling her about
all the things that needed to be fixed about venture capital. They
should make a larger number of smaller investments instead of a
handful of giant ones, they should be funding younger, more technical
founders instead of MBAs, they should let the founders remain as
CEO, and so on.One of my tricks for writing essays had always been to give talks.
The prospect of having to stand up in front of a group of people
and tell them something that won't waste their time is a great
spur to the imagination. When the Harvard Computer Society, the
undergrad computer club, asked me to give a talk, I decided I would
tell them how to start a startup. Maybe they'd be able to avoid the
worst of the mistakes we'd made.So I gave this talk, in the course of which I told them that the
best sources of seed funding were successful startup founders,
because then they'd be sources of advice too. Whereupon it seemed
they were all looking expectantly at me. Horrified at the prospect
of having my inbox flooded by business plans (if I'd only known),
I blurted out "But not me!" and went on with the talk. But afterward
it occurred to me that I should really stop procrastinating about
angel investing. I'd been meaning to since Yahoo bought us, and now
it was 7 years later and I still hadn't done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects
we could work on together. I missed working with them, and it seemed
like there had to be something we could collaborate on.As Jessica and I were walking home from dinner on March 11, at the
corner of Garden and Walker streets, these three threads converged.
Screw the VCs who were taking so long to make up their minds. We'd
start our own investment firm and actually implement the ideas we'd
been talking about. I'd fund it, and Jessica could quit her job and
work for it, and we'd get Robert and Trevor as partners too.
[13]Once again, ignorance worked in our favor. We had no idea how to
be angel investors, and in Boston in 2005 there were no Ron Conways
to learn from. So we just made what seemed like the obvious choices,
and some of the things we did turned out to be novel.There are multiple components to Y Combinator, and we didn't figure
them all out at once. The part we got first was to be an angel firm.
In those days, those two words didn't go together. There were VC
firms, which were organized companies with people whose job it was
to make investments, but they only did big, million dollar investments.
And there were angels, who did smaller investments, but these were
individuals who were usually focused on other things and made
investments on the side. And neither of them helped founders enough
in the beginning. We knew how helpless founders were in some respects,
because we remembered how helpless we'd been. For example, one thing
Julian had done for us that seemed to us like magic was to get us
set up as a company. We were fine writing fairly difficult software,
but actually getting incorporated, with bylaws and stock and all
that stuff, how on earth did you do that? Our plan was not only to
make seed investments, but to do for startups everything Julian had
done for us.YC was not organized as a fund. It was cheap enough to run that we
funded it with our own money. That went right by 99% of readers,
but professional investors are thinking "Wow, that means they got
all the returns." But once again, this was not due to any particular
insight on our part. We didn't know how VC firms were organized.
It never occurred to us to try to raise a fund, and if it had, we
wouldn't have known where to start.
[14]The most distinctive thing about YC is the batch model: to fund a
bunch of startups all at once, twice a year, and then to spend three
months focusing intensively on trying to help them. That part we
discovered by accident, not merely implicitly but explicitly due
to our ignorance about investing. We needed to get experience as
investors. What better way, we thought, than to fund a whole bunch
of startups at once? We knew undergrads got temporary jobs at tech
companies during the summer. Why not organize a summer program where
they'd start startups instead? We wouldn't feel guilty for being
in a sense fake investors, because they would in a similar sense
be fake founders. So while we probably wouldn't make much money out
of it, we'd at least get to practice being investors on them, and
they for their part would probably have a more interesting summer
than they would working at Microsoft.We'd use the building I owned in Cambridge as our headquarters.
We'd all have dinner there once a week  on tuesdays, since I was
already cooking for the thursday diners on thursdays  and after
dinner we'd bring in experts on startups to give talks.We knew undergrads were deciding then about summer jobs, so in a
matter of days we cooked up something we called the Summer Founders
Program, and I posted an 
announcement 
on my site, inviting undergrads
to apply. I had never imagined that writing essays would be a way
to get "deal flow," as investors call it, but it turned out to be
the perfect source.
[15]
We got 225 applications for the Summer
Founders Program, and we were surprised to find that a lot of them
were from people who'd already graduated, or were about to that
spring. Already this SFP thing was starting to feel more serious
than we'd intended.We invited about 20 of the 225 groups to interview in person, and
from those we picked 8 to fund. They were an impressive group. That
first batch included reddit, Justin Kan and Emmett Shear, who went
on to found Twitch, Aaron Swartz, who had already helped write the
RSS spec and would a few years later become a martyr for open access,
and Sam Altman, who would later become the second president of YC.
I don't think it was entirely luck that the first batch was so good.
You had to be pretty bold to sign up for a weird thing like the
Summer Founders Program instead of a summer job at a legit place
like Microsoft or Goldman Sachs.The deal for startups was based on a combination of the deal we did
with Julian ($10k for 10%) and what Robert said MIT grad students
got for the summer ($6k). We invested $6k per founder, which in the
typical two-founder case was $12k, in return for 6%. That had to
be fair, because it was twice as good as the deal we ourselves had
taken. Plus that first summer, which was really hot, Jessica brought
the founders free air conditioners.
[16]Fairly quickly I realized that we had stumbled upon the way to scale
startup funding. Funding startups in batches was more convenient
for us, because it meant we could do things for a lot of startups
at once, but being part of a batch was better for the startups too.
It solved one of the biggest problems faced by founders: the
isolation. Now you not only had colleagues, but colleagues who
understood the problems you were facing and could tell you how they
were solving them.As YC grew, we started to notice other advantages of scale. The
alumni became a tight community, dedicated to helping one another,
and especially the current batch, whose shoes they remembered being
in. We also noticed that the startups were becoming one another's
customers. We used to refer jokingly to the "YC GDP," but as YC
grows this becomes less and less of a joke. Now lots of startups
get their initial set of customers almost entirely from among their
batchmates.I had not originally intended YC to be a full-time job. I was going
to do three things: hack, write essays, and work on YC. As YC grew,
and I grew more excited about it, it started to take up a lot more
than a third of my attention. But for the first few years I was
still able to work on other things.In the summer of 2006, Robert and I started working on a new version
of Arc. This one was reasonably fast, because it was compiled into
Scheme. To test this new Arc, I wrote Hacker News in it. It was
originally meant to be a news aggregator for startup founders and
was called Startup News, but after a few months I got tired of
reading about nothing but startups. Plus it wasn't startup founders
we wanted to reach. It was future startup founders. So I changed
the name to Hacker News and the topic to whatever engaged one's
intellectual curiosity.HN was no doubt good for YC, but it was also by far the biggest
source of stress for me. If all I'd had to do was select and help
founders, life would have been so easy. And that implies that HN
was a mistake. Surely the biggest source of stress in one's work
should at least be something close to the core of the work. Whereas
I was like someone who was in pain while running a marathon not
from the exertion of running, but because I had a blister from an
ill-fitting shoe. When I was dealing with some urgent problem during
YC, there was about a 60% chance it had to do with HN, and a 40%
chance it had do with everything else combined.
[17]As well as HN, I wrote all of YC's internal software in Arc. But
while I continued to work a good deal in Arc, I gradually stopped
working on Arc, partly because I didn't have time to, and partly
because it was a lot less attractive to mess around with the language
now that we had all this infrastructure depending on it. So now my
three projects were reduced to two: writing essays and working on
YC.YC was different from other kinds of work I've done. Instead of
deciding for myself what to work on, the problems came to me. Every
6 months there was a new batch of startups, and their problems,
whatever they were, became our problems. It was very engaging work,
because their problems were quite varied, and the good founders
were very effective. If you were trying to learn the most you could
about startups in the shortest possible time, you couldn't have
picked a better way to do it.There were parts of the job I didn't like. Disputes between cofounders,
figuring out when people were lying to us, fighting with people who
maltreated the startups, and so on. But I worked hard even at the
parts I didn't like. I was haunted by something Kevin Hale once
said about companies: "No one works harder than the boss." He meant
it both descriptively and prescriptively, and it was the second
part that scared me. I wanted YC to be good, so if how hard I worked
set the upper bound on how hard everyone else worked, I'd better
work very hard.One day in 2010, when he was visiting California for interviews,
Robert Morris did something astonishing: he offered me unsolicited
advice. I can only remember him doing that once before. One day at
Viaweb, when I was bent over double from a kidney stone, he suggested
that it would be a good idea for him to take me to the hospital.
That was what it took for Rtm to offer unsolicited advice. So I
remember his exact words very clearly. "You know," he said, "you
should make sure Y Combinator isn't the last cool thing you do."At the time I didn't understand what he meant, but gradually it
dawned on me that he was saying I should quit. This seemed strange
advice, because YC was doing great. But if there was one thing rarer
than Rtm offering advice, it was Rtm being wrong. So this set me
thinking. It was true that on my current trajectory, YC would be
the last thing I did, because it was only taking up more of my
attention. It had already eaten Arc, and was in the process of
eating essays too. Either YC was my life's work or I'd have to leave
eventually. And it wasn't, so I would.In the summer of 2012 my mother had a stroke, and the cause turned
out to be a blood clot caused by colon cancer. The stroke destroyed
her balance, and she was put in a nursing home, but she really
wanted to get out of it and back to her house, and my sister and I
were determined to help her do it. I used to fly up to Oregon to
visit her regularly, and I had a lot of time to think on those
flights. On one of them I realized I was ready to hand YC over to
someone else.I asked Jessica if she wanted to be president, but she didn't, so
we decided we'd try to recruit Sam Altman. We talked to Robert and
Trevor and we agreed to make it a complete changing of the guard.
Up till that point YC had been controlled by the original LLC we
four had started. But we wanted YC to last for a long time, and to
do that it couldn't be controlled by the founders. So if Sam said
yes, we'd let him reorganize YC. Robert and I would retire, and
Jessica and Trevor would become ordinary partners.When we asked Sam if he wanted to be president of YC, initially he
said no. He wanted to start a startup to make nuclear reactors.
But I kept at it, and in October 2013 he finally agreed. We decided
he'd take over starting with the winter 2014 batch. For the rest
of 2013 I left running YC more and more to Sam, partly so he could
learn the job, and partly because I was focused on my mother, whose
cancer had returned.She died on January 15, 2014. We knew this was coming, but it was
still hard when it did.I kept working on YC till March, to help get that batch of startups
through Demo Day, then I checked out pretty completely. (I still
talk to alumni and to new startups working on things I'm interested
in, but that only takes a few hours a week.)What should I do next? Rtm's advice hadn't included anything about
that. I wanted to do something completely different, so I decided
I'd paint. I wanted to see how good I could get if I really focused
on it. So the day after I stopped working on YC, I started painting.
I was rusty and it took a while to get back into shape, but it was
at least completely engaging.
[18]I spent most of the rest of 2014 painting. I'd never been able to
work so uninterruptedly before, and I got to be better than I had
been. Not good enough, but better. Then in November, right in the
middle of a painting, I ran out of steam. Up till that point I'd
always been curious to see how the painting I was working on would
turn out, but suddenly finishing this one seemed like a chore. So
I stopped working on it and cleaned my brushes and haven't painted
since. So far anyway.I realize that sounds rather wimpy. But attention is a zero sum
game. If you can choose what to work on, and you choose a project
that's not the best one (or at least a good one) for you, then it's
getting in the way of another project that is. And at 50 there was
some opportunity cost to screwing around.I started writing essays again, and wrote a bunch of new ones over
the next few months. I even wrote a couple that 
weren't about
startups. Then in March 2015 I started working on Lisp again.The distinctive thing about Lisp is that its core is a language
defined by writing an interpreter in itself. It wasn't originally
intended as a programming language in the ordinary sense. It was
meant to be a formal model of computation, an alternative to the
Turing machine. If you want to write an interpreter for a language
in itself, what's the minimum set of predefined operators you need?
The Lisp that John McCarthy invented, or more accurately discovered,
is an answer to that question.
[19]McCarthy didn't realize this Lisp could even be used to program
computers till his grad student Steve Russell suggested it. Russell
translated McCarthy's interpreter into IBM 704 machine language,
and from that point Lisp started also to be a programming language
in the ordinary sense. But its origins as a model of computation
gave it a power and elegance that other languages couldn't match.
It was this that attracted me in college, though I didn't understand
why at the time.McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions.
It was missing a lot of things you'd want in a programming language.
So these had to be added, and when they were, they weren't defined
using McCarthy's original axiomatic approach. That wouldn't have
been feasible at the time. McCarthy tested his interpreter by
hand-simulating the execution of programs. But it was already getting
close to the limit of interpreters you could test that way  indeed,
there was a bug in it that McCarthy had overlooked. To test a more
complicated interpreter, you'd have had to run it, and computers
then weren't powerful enough.Now they are, though. Now you could continue using McCarthy's
axiomatic approach till you'd defined a complete programming language.
And as long as every change you made to McCarthy's Lisp was a
discoveredness-preserving transformation, you could, in principle,
end up with a complete language that had this quality. Harder to
do than to talk about, of course, but if it was possible in principle,
why not try? So I decided to take a shot at it. It took 4 years,
from March 26, 2015 to October 12, 2019. It was fortunate that I
had a precisely defined goal, or it would have been hard to keep
at it for so long.I wrote this new Lisp, called Bel, 
in itself in Arc. That may sound
like a contradiction, but it's an indication of the sort of trickery
I had to engage in to make this work. By means of an egregious
collection of hacks I managed to make something close enough to an
interpreter written in itself that could actually run. Not fast,
but fast enough to test.I had to ban myself from writing essays during most of this time,
or I'd never have finished. In late 2015 I spent 3 months writing
essays, and when I went back to working on Bel I could barely
understand the code. Not so much because it was badly written as
because the problem is so convoluted. When you're working on an
interpreter written in itself, it's hard to keep track of what's
happening at what level, and errors can be practically encrypted
by the time you get them.So I said no more essays till Bel was done. But I told few people
about Bel while I was working on it. So for years it must have
seemed that I was doing nothing, when in fact I was working harder
than I'd ever worked on anything. Occasionally after wrestling for
hours with some gruesome bug I'd check Twitter or HN and see someone
asking "Does Paul Graham still code?"Working on Bel was hard but satisfying. I worked on it so intensively
that at any given time I had a decent chunk of the code in my head
and could write more there. I remember taking the boys to the
coast on a sunny day in 2015 and figuring out how to deal with some
problem involving continuations while I watched them play in the
tide pools. It felt like I was doing life right. I remember that
because I was slightly dismayed at how novel it felt. The good news
is that I had more moments like this over the next few years.In the summer of 2016 we moved to England. We wanted our kids to
see what it was like living in another country, and since I was a
British citizen by birth, that seemed the obvious choice. We only
meant to stay for a year, but we liked it so much that we still
live there. So most of Bel was written in England.In the fall of 2019, Bel was finally finished. Like McCarthy's
original Lisp, it's a spec rather than an implementation, although
like McCarthy's Lisp it's a spec expressed as code.Now that I could write essays again, I wrote a bunch about topics
I'd had stacked up. I kept writing essays through 2020, but I also
started to think about other things I could work on. How should I
choose what to do? Well, how had I chosen what to work on in the
past? I wrote an essay for myself to answer that question, and I
was surprised how long and messy the answer turned out to be. If
this surprised me, who'd lived it, then I thought perhaps it would
be interesting to other people, and encouraging to those with
similarly messy lives. So I wrote a more detailed version for others
to read, and this is the last sentence of it.
Notes[1]
My experience skipped a step in the evolution of computers:
time-sharing machines with interactive OSes. I went straight from
batch processing to microcomputers, which made microcomputers seem
all the more exciting.[2]
Italian words for abstract concepts can nearly always be
predicted from their English cognates (except for occasional traps
like polluzione). It's the everyday words that differ. So if you
string together a lot of abstract concepts with a few simple verbs,
you can make a little Italian go a long way.[3]
I lived at Piazza San Felice 4, so my walk to the Accademia
went straight down the spine of old Florence: past the Pitti, across
the bridge, past Orsanmichele, between the Duomo and the Baptistery,
and then up Via Ricasoli to Piazza San Marco. I saw Florence at
street level in every possible condition, from empty dark winter
evenings to sweltering summer days when the streets were packed with
tourists.[4]
You can of course paint people like still lives if you want
to, and they're willing. That sort of portrait is arguably the apex
of still life painting, though the long sitting does tend to produce
pained expressions in the sitters.[5]
Interleaf was one of many companies that had smart people and
built impressive technology, and yet got crushed by Moore's Law.
In the 1990s the exponential growth in the power of commodity (i.e.
Intel) processors rolled up high-end, special-purpose hardware and
software companies like a bulldozer.[6]
The signature style seekers at RISD weren't specifically
mercenary. In the art world, money and coolness are tightly coupled.
Anything expensive comes to be seen as cool, and anything seen as
cool will soon become equally expensive.[7]
Technically the apartment wasn't rent-controlled but
rent-stabilized, but this is a refinement only New Yorkers would
know or care about. The point is that it was really cheap, less
than half market price.[8]
Most software you can launch as soon as it's done. But when
the software is an online store builder and you're hosting the
stores, if you don't have any users yet, that fact will be painfully
obvious. So before we could launch publicly we had to launch
privately, in the sense of recruiting an initial set of users and
making sure they had decent-looking stores.[9]
We'd had a code editor in Viaweb for users to define their
own page styles. They didn't know it, but they were editing Lisp
expressions underneath. But this wasn't an app editor, because the
code ran when the merchants' sites were generated, not when shoppers
visited them.[10]
This was the first instance of what is now a familiar experience,
and so was what happened next, when I read the comments and found
they were full of angry people. How could I claim that Lisp was
better than other languages? Weren't they all Turing complete?
People who see the responses to essays I write sometimes tell me
how sorry they feel for me, but I'm not exaggerating when I reply
that it has always been like this, since the very beginning. It
comes with the territory. An essay must tell readers things they
don't already know, and some 
people dislike being told such things.[11]
People put plenty of stuff on the internet in the 90s of
course, but putting something online is not the same as publishing
it online. Publishing online means you treat the online version as
the (or at least a) primary version.[12]
There is a general lesson here that our experience with Y
Combinator also teaches: Customs continue to constrain you long
after the restrictions that caused them have disappeared. Customary
VC practice had once, like the customs about publishing essays,
been based on real constraints. Startups had once been much more
expensive to start, and proportionally rare. Now they could be cheap
and common, but the VCs' customs still reflected the old world,
just as customs about writing essays still reflected the constraints
of the print era.Which in turn implies that people who are independent-minded (i.e.
less influenced by custom) will have an advantage in fields affected
by rapid change (where customs are more likely to be obsolete).Here's an interesting point, though: you can't always predict which
fields will be affected by rapid change. Obviously software and
venture capital will be, but who would have predicted that essay
writing would be?[13]
Y Combinator was not the original name. At first we were
called Cambridge Seed. But we didn't want a regional name, in case
someone copied us in Silicon Valley, so we renamed ourselves after
one of the coolest tricks in the lambda calculus, the Y combinator.I picked orange as our color partly because it's the warmest, and
partly because no VC used it. In 2005 all the VCs used staid colors
like maroon, navy blue, and forest green, because they were trying
to appeal to LPs, not founders. The YC logo itself is an inside
joke: the Viaweb logo had been a white V on a red circle, so I made
the YC logo a white Y on an orange square.[14]
YC did become a fund for a couple years starting in 2009,
because it was getting so big I could no longer afford to fund it
personally. But after Heroku got bought we had enough money to go
back to being self-funded.[15]
I've never liked the term "deal flow," because it implies
that the number of new startups at any given time is fixed. This
is not only false, but it's the purpose of YC to falsify it, by
causing startups to be founded that would not otherwise have existed.[16]
She reports that they were all different shapes and sizes,
because there was a run on air conditioners and she had to get
whatever she could, but that they were all heavier than she could
carry now.[17]
Another problem with HN was a bizarre edge case that occurs
when you both write essays and run a forum. When you run a forum,
you're assumed to see if not every conversation, at least every
conversation involving you. And when you write essays, people post
highly imaginative misinterpretations of them on forums. Individually
these two phenomena are tedious but bearable, but the combination
is disastrous. You actually have to respond to the misinterpretations,
because the assumption that you're present in the conversation means
that not responding to any sufficiently upvoted misinterpretation
reads as a tacit admission that it's correct. But that in turn
encourages more; anyone who wants to pick a fight with you senses
that now is their chance.[18]
The worst thing about leaving YC was not working with Jessica
anymore. We'd been working on YC almost the whole time we'd known
each other, and we'd neither tried nor wanted to separate it from
our personal lives, so leaving was like pulling up a deeply rooted
tree.[19]
One way to get more precise about the concept of invented vs
discovered is to talk about space aliens. Any sufficiently advanced
alien civilization would certainly know about the Pythagorean
theorem, for example. I believe, though with less certainty, that
they would also know about the Lisp in McCarthy's 1960 paper.But if so there's no reason to suppose that this is the limit of
the language that might be known to them. Presumably aliens need
numbers and errors and I/O too. So it seems likely there exists at
least one path out of McCarthy's Lisp along which discoveredness
is preserved.Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel
Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj
Taggar for reading drafts of this.September 2007In high school I decided I was going to study philosophy in college.
I had several motives, some more honorable than others.  One of the
less honorable was to shock people.  College was regarded as job
training where I grew up, so studying philosophy seemed an impressively
impractical thing to do.  Sort of like slashing holes in your clothes
or putting a safety pin through your ear, which were other forms
of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying
philosophy would be a shortcut straight to wisdom.  All the people
majoring in other things would just end up with a bunch of domain
knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you
wouldn't find those in our high school library.  But I tried to
read Plato and Aristotle.  I doubt I believed I understood them,
but they sounded like they were talking about something important.
I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned
a lot in the calculus class, but I didn't learn much in Philosophy
101.  And yet my plan to study philosophy remained intact.  It was
my fault I hadn't learned anything.  I hadn't read the books we
were assigned carefully enough.  I'd give Berkeley's Principles
of Human Knowledge another shot in college.  Anything so admired
and so difficult to read must have something in it, if one could
only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have
a nice edition of his collected works.  Will I ever read it?  Seems
unlikely.The difference between then and now is that now I understand why
Berkeley is probably not worth trying to understand.  I think I see
now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It
didn't work out as I'd hoped.  I didn't learn any magical truths
compared to which everything else was mere domain knowledge.  But
I do at least know now why I didn't.  Philosophy doesn't really
have a subject matter in the way math or history or most other
university subjects do.  There is no core of knowledge one must
master.  The closest you come to that is a knowledge of what various
individual philosophers have said about different topics over the
years.  Few were sufficiently correct that people have forgotten
who discovered what they discovered.Formal logic has some subject matter. I took several classes in
logic.  I don't know if I learned anything from them.
[1]
It does seem to me very important to be able to flip ideas around in
one's head: to see when two ideas don't fully cover the space of
possibilities, or when one idea is the same as another but with a
couple things changed.  But did studying logic teach me the importance
of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The
most dramatic I learned immediately, in the first semester of
freshman year, in a class taught by Sydney Shoemaker.  I learned
that I don't exist.  I am (and you are) a collection of cells that
lurches around driven by various forces, and calls itself I.  But
there's no central, indivisible thing that your identity goes with.
You could conceivably lose half your brain and live.  Which means
your brain could conceivably be split into two halves and each
transplanted into different bodies.  Imagine waking up after such
an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life
are fuzzy, and break down if pushed too hard.  Even a concept as
dear to us as I.  It took me a while to grasp this, but when I
did it was fairly sudden, like someone in the nineteenth century
grasping evolution and realizing the story of creation they'd been
told as a child was all wrong. 
[2]
Outside of math there's a limit
to how far you can push words; in fact, it would not be a bad
definition of math to call it the study of terms that have precise
meanings.  Everyday words are inherently imprecise.  They work well
enough in everyday life that you don't notice.  Words seem to work,
just as Newtonian physics seems to.  But you can always make them
break if you push them far enough.I would say that this has been, unfortunately for philosophy, the
central fact of philosophy.  Most philosophical debates are not
merely afflicted by but driven by confusions over words.  Do we
have free will?  Depends what you mean by "free." Do abstract ideas
exist?  Depends what you mean by "exist."Wittgenstein is popularly credited with the idea that most philosophical
controversies are due to confusions over language.  I'm not sure
how much credit to give him.  I suspect a lot of people realized
this, but reacted simply by not studying philosophy, rather than
becoming philosophy professors.How did things get this way?  Can something people have spent
thousands of years studying really be a waste of time?  Those are
interesting questions.  In fact, some of the most interesting
questions you can ask about philosophy.  The most valuable way to
approach the current philosophical tradition may be neither to get
lost in pointless speculations like Berkeley, nor to shut them down
like Wittgenstein, but to study it as an example of reason gone
wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.
What we know of their predecessors comes from fragments and references
in later works; their doctrines could be described as speculative
cosmology that occasionally strays into analysis.  Presumably they
were driven by whatever makes people in every other society invent
cosmologies.
[3]With Socrates, Plato, and particularly Aristotle, this tradition
turned a corner.  There started to be a lot more analysis.  I suspect
Plato and Aristotle were encouraged in this by progress in math.
Mathematicians had by then shown that you could figure things out
in a much more conclusive way than by making up fine sounding stories
about them.  
[4]People talk so much about abstractions now that we don't realize
what a leap it must have been when they first started to.  It was
presumably many thousands of years between when people first started
describing things as hot or cold and when someone asked "what is
heat?"  No doubt it was a very gradual process.  We don't know if
Plato or Aristotle were the first to ask any of the questions they
did.  But their works are the oldest we have that do this on a large
scale, and there is a freshness (not to say naivete) about them
that suggests some of the questions they asked were new to them,
at least.Aristotle in particular reminds me of the phenomenon that happens
when people discover something new, and are so excited by it that
they race through a huge percentage of the newly discovered territory
in one lifetime.  If so, that's evidence of how new this kind of
thinking was. 
[5]This is all to explain how Plato and Aristotle can be very impressive
and yet naive and mistaken.  It was impressive even to ask the
questions they did.  That doesn't mean they always came up with
good answers.  It's not considered insulting to say that ancient
Greek mathematicians were naive in some respects, or at least lacked
some concepts that would have made their lives easier.  So I hope
people will not be too offended if I propose that ancient philosophers
were similarly naive.  In particular, they don't seem to have fully
grasped what I earlier called the central fact of philosophy: that
words break if you push them too far."Much to the surprise of the builders of the first digital computers,"
Rod Brooks wrote, "programs written for them usually did not work."
[6]
Something similar happened when people first started trying
to talk about abstractions.  Much to their surprise, they didn't
arrive at answers they agreed upon.  In fact, they rarely seemed
to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at
too low a resolution.The proof of how useless some of their answers turned out to be is
how little effect they have.  No one after reading Aristotle's
Metaphysics does anything differently as a result.
[7]Surely I'm not claiming that ideas have to have practical applications
to be interesting?  No, they may not have to.  Hardy's boast that
number theory had no use whatsoever wouldn't disqualify it.  But
he turned out to be mistaken.  In fact, it's suspiciously hard to
find a field of math that truly has no practical use.  And Aristotle's
explanation of the ultimate goal of philosophy in Book A of the
Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.
The examples he gives are convincing: an ordinary worker builds
things a certain way out of habit; a master craftsman can do more
because he grasps the underlying principles.  The trend is clear:
the more general the knowledge, the more admirable it is.  But then
he makes a mistake—possibly the most important mistake in the
history of philosophy.  He has noticed that theoretical knowledge
is often acquired for its own sake, out of curiosity, rather than
for any practical need.  So he proposes there are two kinds of
theoretical knowledge: some that's useful in practical matters and
some that isn't.  Since people interested in the latter are interested
in it for its own sake, it must be more noble.  So he sets as his
goal in the Metaphysics the exploration of knowledge that has no
practical use.  Which means no alarms go off when he takes on grand
but vaguely understood questions and ends up getting lost in a sea
of words.His mistake was to confuse motive and result.  Certainly, people
who want a deep understanding of something are often driven by
curiosity rather than any practical need.  But that doesn't mean
what they end up learning is useless.  It's very valuable in practice
to have a deep understanding of what you're doing; even if you're
never called on to solve advanced problems, you can see shortcuts
in the solution of simple ones, and your knowledge won't break down
in edge cases, as it would if you were relying on formulas you
didn't understand.  Knowledge is power.  That's what makes theoretical
knowledge prestigious.  It's also what causes smart people to be
curious about certain things and not others; our DNA is not so
disinterested as we might think.So while ideas don't have to have immediate practical applications
to be interesting, the kinds of things we find interesting will
surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was
partly that he set off with contradictory aims: to explore the most
abstract ideas, guided by the assumption that they were useless.
He was like an explorer looking for a territory to the north of
him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future
explorers, he sent them off in the wrong direction as well. 
[8]
Perhaps worst of all, he protected them from both the criticism of
outsiders and the promptings of their own inner compass by establishing
the principle that the most noble sort of theoretical knowledge had
to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from
it turned out to be worth keeping; the bulk of it has had no effect
at all.  The Metaphysics is among the least read of all famous
books.  It's not hard to understand the way Newton's Principia
is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately
that was not the conclusion Aristotle's successors derived from
works like the Metaphysics. 
[9]
Soon after, the western world
fell on intellectual hard times.  Instead of version 1s to be
superseded, the works of Plato and Aristotle became revered texts
to be mastered and discussed.  And so things remained for a shockingly
long time.  It was not till around 1600 (in Europe, where the center
of gravity had shifted by then) that one found people confident
enough to treat Aristotle's work as a catalog of mistakes.  And
even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little
progress there was in math between Hellenistic times and the
Renaissance.In the intervening years an unfortunate idea took hold:  that it
was not only acceptable to produce works like the Metaphysics,
but that it was a particularly prestigious line of work, done by a
class of people called philosophers.  No one thought to go back and
debug Aristotle's motivating argument.  And so instead of correcting
the problem Aristotle discovered by falling into it—that you can
easily get lost if you talk too loosely about very abstract ideas—they 
continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract
new readers.  Traditional philosophy occupies a kind of singularity
in this respect.  If you write in an unclear way about big ideas,
you produce something that seems tantalizingly attractive to
inexperienced but intellectually ambitious students.  Till one knows
better, it's hard to distinguish something that's hard to understand
because the writer was unclear in his own mind from something like
a mathematical proof that's hard to understand because the ideas
it represents are hard to understand.  To someone who hasn't learned
the difference, traditional philosophy seems extremely attractive:
as hard (and therefore impressive) as math, yet broader in scope.
That was what lured me in as a high school student.This singularity is even more singular in having its own defense
built in.  When things are hard to understand, people who suspect
they're nonsense generally keep quiet.  There's no way to prove a
text is meaningless.  The closest you can get is to show that the
official judges of some class of texts can't distinguish them from
placebos. 
[10]And so instead of denouncing philosophy, most people who suspected
it was a waste of time just studied other things.  That alone is
fairly damning evidence, considering philosophy's claims.  It's
supposed to be about the ultimate truths. Surely all smart people
would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might
have corrected them, they tended to be self-perpetuating.  Bertrand
Russell wrote in a letter in 1912:

  Hitherto the people attracted to philosophy have been mostly those
  who loved the big generalizations, which were all wrong, so that
  few people with exact minds have taken up the subject.
[11]

His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery
that most previous philosophy was a waste of time, which judging
from the circumstantial evidence must have been made by every smart
person who studied a little philosophy and declined to pursue it
further, but for how he acted in response.
[12]
Instead of quietly
switching to another field, he made a fuss, from inside.  He was
Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein
gave it. 
[13]
Later in life he spent a lot of time talking about
how words worked.  Since that seems to be allowed, that's what a
lot of philosophers do now.  Meanwhile, sensing a vacuum in the
metaphysical speculation department, the people who used to do
literary criticism have been edging Kantward, under new names like
"literary theory," "critical theory," and when they're feeling
ambitious, plain "theory."  The writing is the familiar word salad:

  Gender is not like some of the other grammatical modes which
  express precisely a mode of conception without any reality that
  corresponds to the conceptual mode, and consequently do not express
  precisely something in reality by which the intellect could be
  moved to conceive a thing the way it does, even where that motive
  is not something in the thing as such.
  [14]

The singularity I've described is not going away.  There's a market
for writing that sounds impressive and can't be disproven. There
will always be both supply and demand.  So if one group abandons
this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.
Perhaps we should do what Aristotle meant to do, instead of what
he did.  The goal he announces in the Metaphysics seems one worth
pursuing: to discover the most general truths.  That sounds good.
But instead of trying to discover them because they're useless,
let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised
criterion, applicability, as a guide to keep us from wondering
off into a swamp of abstractions.  Instead of trying to answer the
question:

  What are the most general truths?

let's try to answer the question

  Of all the useful things we can say, which are the most general?

The test of utility I propose is whether we cause people who read
what we've written to do anything differently afterward.  Knowing
we have to give definite (if implicit) advice will keep us from
straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a
different direction.As an example of a useful, general idea, consider that of the
controlled experiment.  There's an idea that has turned out to be
widely applicable.  Some might say it's part of science, but it's
not part of any specific science; it's literally meta-physics (in
our sense of "meta").   The idea of evolution is another. It turns
out to have quite broad applications—for example, in genetic
algorithms and even product design.  Frankfurt's distinction between
lying and bullshitting seems a promising recent example.
[15]These seem to me what philosophy should look like: quite general
observations that would cause someone who understood them to do
something differently.Such observations will necessarily be about things that are imprecisely
defined.  Once you start using words with precise meanings, you're
doing math.  So starting from utility won't entirely solve the
problem I described above—it won't flush out the metaphysical
singularity.  But it should help.  It gives people with good
intentions a new roadmap into abstraction.  And they may thereby
produce things that make the writing of the people with bad intentions
look bad by comparison.One drawback of this approach is that it won't produce the sort of
writing that gets you tenure.  And not just because it's not currently
the fashion.  In order to get tenure in any field you must not
arrive at conclusions that members of tenure committees can disagree
with.  In practice there are two kinds of solutions to this problem.
In math and the sciences, you can prove what you're saying, or at
any rate adjust your conclusions so you're not claiming anything
false ("6 of 8 subjects had lower blood pressure after the treatment").
In the humanities you can either avoid drawing any definite conclusions
(e.g. conclude that an issue is a complex one), or draw conclusions
so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either
of these routes.  At best you'll be able to achieve the essayist's
standard of proof, not the mathematician's or the experimentalist's.
And yet you won't be able to meet the usefulness test without
implying definite and fairly broadly applicable conclusions.  Worse
still, the usefulness test will tend to produce results that annoy
people: there's no use in telling people things they already believe,
and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting
to general plus useful by starting with useful and cranking up the
generality may be unsuitable for junior professors trying to get
tenure, but it's better for everyone else, including professors who
already have it.  This side of the mountain is a nice gradual slope.
You can start by writing things that are useful but very specific,
and then gradually make them more general.  Joe's has good burritos.
What makes a good burrito?  What makes good food?  What makes
anything good?  You can take as long as you want.  You don't have
to get all the way to the top of the mountain.  You don't have to
tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an
encouraging thought.  The field is a lot younger than it seems.
Though the first philosophers in the western tradition lived about
2500 years ago, it would be misleading to say the field is 2500
years old, because for most of that time the leading practitioners
weren't doing much more than writing commentaries on Plato or
Aristotle while watching over their shoulders for the next invading
army.  In the times when they weren't, philosophy was hopelessly
intermingled with religion.  It didn't shake itself free till a
couple hundred years ago, and even then was afflicted by the
structural problems I've described above.  If I say this, some will
say it's a ridiculously overbroad and uncharitable generalization,
and others will say it's old news, but here goes: judging from their
works, most philosophers up to the present have been wasting their
time.  So in a sense the field is still at the first step. 
[16]That sounds a preposterous claim to make.  It won't seem so
preposterous in 10,000 years.  Civilization always seems old, because
it's always the oldest it's ever been.  The only way to say whether
something is really old or not is by looking at structural evidence,
and structurally philosophy is young; it's still reeling from the
unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot
more to discover.Notes
[1]
In practice formal logic is not much use, because despite
some progress in the last 150 years we're still only able to formalize
a small percentage of statements.  We may never do that much better,
for the same reason 1980s-style "knowledge representation" could
never have worked; many statements may have no representation more
concise than a huge, analog brain state.[2]
It was harder for Darwin's contemporaries to grasp this than
we can easily imagine.  The story of creation in the Bible is not
just a Judeo-Christian concept; it's roughly what everyone must
have believed since before people were people.  The hard part of
grasping evolution was to realize that species weren't, as they
seem to be, unchanging, but had instead evolved from different,
simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized
country encounters the idea of evolution for the first time as an
adult.  Everyone's taught about it as a child, either as truth or
heresy.[3]
Greek philosophers before Plato wrote in verse.  This must
have affected what they said.  If you try to write about the nature
of the world in verse, it inevitably turns into incantation.  Prose
lets you be more precise, and more tentative.[4]
Philosophy is like math's
ne'er-do-well brother.  It was born when Plato and Aristotle looked
at the works of their predecessors and said in effect "why can't
you be more like your brother?"  Russell was still saying the same
thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy
the imprecise half.  It's probably inevitable that philosophy will
suffer by comparison, because there's no lower bound to its precision.
Bad math is merely boring, whereas bad philosophy is nonsense.  And
yet there are some good ideas in the imprecise half.[5]
Aristotle's best work was in logic and zoology, both of which
he can  be said to have invented.  But the most dramatic departure
from his predecessors was a new, much more analytical style of
thinking.  He was arguably the first scientist.[6]
Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.
94.[7]
Some would say we depend on Aristotle more than we realize,
because his ideas were one of the ingredients in our common culture.
Certainly a lot of the words we use have a connection with Aristotle,
but it seems a bit much to suggest that we wouldn't have the concept
of the essence of something or the distinction between matter and
form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to
diff European culture with Chinese: what ideas did European culture
have in 1800 that Chinese culture didn't, in virtue of Aristotle's
contribution?[8]
The meaning of the word "philosophy" has changed over time.
In ancient times it covered a broad range of topics, comparable in
scope to our "scholarship" (though without the methodological
implications).  Even as late as Newton's time it included what we
now call "science."  But core of the subject today is still what
seemed to Aristotle the core: the attempt to discover the most
general truths.Aristotle didn't call this "metaphysics."  That name got assigned
to it because the books we now call the Metaphysics came after
(meta = after) the Physics in the standard edition of Aristotle's
works compiled by Andronicus of Rhodes three centuries later.  What
we call "metaphysics" Aristotle called "first philosophy."[9]
Some of Aristotle's immediate successors may have realized
this, but it's hard to say because most of their works are lost.[10]
Sokal, Alan, "Transgressing the Boundaries: Toward a Transformative
Hermeneutics of Quantum Gravity," Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's
aligned with some axe the audience already has to grind.  If this
is so we should find it's most popular with groups that are (or
feel) weak.  The powerful don't need its reassurance.[11]
Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,
p. 75.[12]
A preliminary result, that all metaphysics between Aristotle
and 1783 had been a waste of time, is due to I. Kant.[13]
Wittgenstein asserted a sort of mastery to which the inhabitants
of early 20th century Cambridge seem to have been peculiarly
vulnerable—perhaps partly because so many had been raised religious
and then stopped believing, so had a vacant space in their heads
for someone to tell them what to do (others chose Marx or Cardinal
Newman), and partly because a quiet, earnest place like Cambridge
in that era had no natural immunity to messianic figures, just as
European politics then had no natural immunity to dictators.[14]
This is actually from the Ordinatio of Duns Scotus (ca.
1300), with "number" replaced by "gender."  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,
1963, p. 92.[15]
Frankfurt, Harry, On Bullshit,  Princeton University Press,
2005.[16]
Some introductions to philosophy now take the line that
philosophy is worth studying as a process rather than for any
particular truths you'll learn.  The philosophers whose works they
cover would be rolling in their graves at that.  They hoped they
were doing more than serving as examples of how to argue: they hoped
they were getting results.  Most were wrong, but it doesn't seem
an impossible hope.This argument seems to me like someone in 1500 looking at the lack
of results achieved by alchemy and saying its value was as a process.
No, they were going about it wrong.  It turns out it is possible
to transmute lead into gold (though not economically at current
energy prices), but the route to that knowledge was to
backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, 
Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.November 2022Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I wasn't responsible
for my actions. I gradually lost hope of that, but the puzzle
remained: How do you reconcile being a machine made of matter with
the feeling that you're free to choose what you do?
[1]The best way to explain the answer may be to start with a slightly
wrong version, and then fix it. The wrong version is: You can do
what you want, but you can't want what you want. Yes, you can control
what you do, but you'll do what you want, and you can't control
that.The reason this is mistaken is that people do sometimes change what
they want. People who don't want to want something — drug addicts,
for example — can sometimes make themselves stop wanting it. And
people who want to want something — who want to like classical
music, or broccoli — sometimes succeed.So we modify our initial statement: You can do what you want, but
you can't want to want what you want.That's still not quite true. It's possible to change what you want
to want. I can imagine someone saying "I decided to stop wanting
to like classical music." But we're getting closer to the truth.
It's rare for people to change what they want to want, and the more
"want to"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more "want
to"s in much the same way we can get arbitrarily close to 1 by adding
more 9s to a string of 9s following a decimal point. In practice
three or four "want to"s must surely be enough. It's hard even to
envision what it would mean to change what you want to want to want
to want, let alone actually do it.So one way to express the correct answer is to use a regular
expression. You can do what you want, but there's some statement
of the form "you can't (want to)* want what you want" that's true.
Ultimately you get back to a want that you don't control.
[2]
Notes[1]
I didn't know when I was 9 that matter might behave randomly,
but I don't think it affects the problem much. Randomness destroys
the ghost in the machine as effectively as determinism.[2]
If you don't like using an expression, you can make the same
point using higher-order desires: There is some n such that you
don't control your nth-order desires.
Thanks to Trevor Blackwell,
Jessica Livingston, Robert Morris, and
Michael Nielsen for reading drafts of this.September 2017The most valuable insights are both general and surprising. 
F = ma for example. But general and surprising is a hard
combination to achieve. That territory tends to be picked
clean, precisely because those insights are so valuable.Ordinarily, the best that people can do is one without the
other: either surprising without being general (e.g.
gossip), or general without being surprising (e.g.
platitudes).Where things get interesting is the moderately valuable
insights.  You get those from small additions of whichever
quality was missing.  The more common case is a small
addition of generality: a piece of gossip that's more than
just gossip, because it teaches something interesting about
the world. But another less common approach is to focus on
the most general ideas and see if you can find something new
to say about them. Because these start out so general, you
only need a small delta of novelty to produce a useful
insight.A small delta of novelty is all you'll be able to get most
of the time. Which means if you take this route, your ideas
will seem a lot like ones that already exist. Sometimes
you'll find you've merely rediscovered an idea that did
already exist.  But don't be discouraged.  Remember the huge
multiplier that kicks in when you do manage to think of
something even a little new.Corollary: the more general the ideas you're talking about,
the less you should worry about repeating yourself.  If you
write enough, it's inevitable you will.  Your brain is much
the same from year to year and so are the stimuli that hit
it. I feel slightly bad when I find I've said something
close to what I've said before, as if I were plagiarizing
myself. But rationally one shouldn't.  You won't say
something exactly the same way the second time, and that
variation increases the chance you'll get that tiny but
critical delta of novelty.And of course, ideas beget ideas.  (That sounds 
familiar.)
An idea with a small amount of novelty could lead to one
with more. But only if you keep going. So it's doubly
important not to let yourself be discouraged by people who
say there's not much new about something you've discovered.
"Not much new" is a real achievement when you're talking
about the most general ideas. It's not true that there's nothing new under the sun.  There
are some domains where there's almost nothing new.  But
there's a big difference between nothing and almost nothing,
when it's multiplied by the area under the sun.
Thanks to Sam Altman, Patrick Collison, and Jessica
Livingston for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




November 2005Does "Web 2.0" mean anything?  Till recently I thought it didn't,
but the truth turns out to be more complicated.  Originally, yes,
it was meaningless.  Now it seems to have acquired a meaning.  And
yet those who dislike the term are probably right, because if it
means what I think it does, we don't need it.I first heard the phrase "Web 2.0" in the name of the Web 2.0
conference in 2004.  At the time it was supposed to mean using "the
web as a platform," which I took to refer to web-based applications.
[1]So I was surprised at a conference this summer when Tim O'Reilly
led a session intended to figure out a definition of "Web 2.0."
Didn't it already mean using the web as a platform?  And if it
didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase "Web 2.0" first
arose in "a brainstorming session between
O'Reilly and Medialive International." What is Medialive International?
"Producers of technology tradeshows and conferences," according to
their site.  So presumably that's what this brainstorming session
was about.  O'Reilly wanted to organize a conference about the web,
and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a
new version of the web.  They just wanted to make the point
that the web mattered again.  It was a kind of semantic deficit
spending: they knew new things were coming, and the "2.0" referred
to whatever those might turn out to be.And they were right.  New things were coming.  But the new version
number led to some awkwardness in the short term.  In the process
of developing the pitch for the first conference, someone must have
decided they'd better take a stab at explaining what that "2.0"
referred to.  Whatever it meant, "the web as a platform" was at
least not too constricting.The story about "Web 2.0" meaning the web as a platform didn't live
much past the first conference.  By the second conference, what
"Web 2.0" seemed to mean was something about democracy.  At least,
it did when people wrote about it online.  The conference itself
didn't seem very grassroots.  It cost $2800, so the only people who
could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article
about the conference in Wired News spoke of "throngs of
geeks."  When a friend of mine asked Ryan about this, it was news
to him.  He said he'd originally written something like "throngs
of VCs and biz dev guys" but had later shortened it just to "throngs,"
and that this must have in turn been expanded by the editors into
"throngs of geeks."  After all, a Web 2.0 conference would presumably
be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a   
suit, a sight so alien I couldn't parse it at first.  I saw
him walk by and said to one of the O'Reilly people "that guy looks
just like Tim.""Oh, that's Tim.  He bought a suit."
I ran after him, and sure enough, it was.  He explained that he'd
just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows
during the Bubble, full of prowling VCs looking for the next hot
startup.  There was that same odd atmosphere created by a large  
number of people determined not to miss out.  Miss out on what?
They didn't know.  Whatever was going to happen—whatever Web 2.0
turned out to be.I wouldn't quite call it "Bubble 2.0" just because VCs are eager
to invest again.  The Internet is a genuinely big deal.  The bust
was as much an overreaction as
the boom.  It's to be expected that once we started to pull out of
the bust, there would be a lot of growth in this area, just as there
was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO
market is gone.  Venture investors
are driven by exit strategies.  The reason they were funding all  
those laughable startups during the late 90s was that they hoped
to sell them to gullible retail investors; they hoped to be laughing
all the way to the bank.  Now that route is closed.  Now the default
exit strategy is to get bought, and acquirers are less prone to
irrational exuberance than IPO investors.  The closest you'll get 
to Bubble valuations is Rupert Murdoch paying $580 million for   
Myspace.  That's only off by a factor of 10 or so.1. AjaxDoes "Web 2.0" mean anything more than the name of a conference
yet?  I don't like to admit it, but it's starting to.  When people
say "Web 2.0" now, I have some idea what they mean.  And the fact
that I both despise the phrase and understand it is the surest proof
that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still
only just bear to use without scare quotes.  Basically, what "Ajax"
means is "Javascript now works."  And that in turn means that
web-based applications can now be made to work much more like desktop
ones.As you read this, a whole new generation
of software is being written to take advantage of Ajax.  There
hasn't been such a wave of new applications since microcomputers
first appeared.  Even Microsoft sees it, but it's too late for them
to do anything more than leak "internal"  
documents designed to give the impression they're on top of this
new trend.In fact the new generation of software is being written way too
fast for Microsoft even to channel it, let alone write their own
in house.  Their only hope now is to buy all the best Ajax startups
before Google does.  And even that's going to be hard, because
Google has as big a head start in buying microstartups as it did
in search a few years ago.  After all, Google Maps, the canonical
Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference
turned out to be partially right: web-based applications are a big
component of Web 2.0.  But I'm convinced they got this right by 
accident.  The Ajax boom didn't start till early 2005, when Google
Maps appeared and the term "Ajax" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several
examples to prove that amateurs can   
surpass professionals, when they have the right kind of system to 
channel their efforts.  Wikipedia
may be the most famous.  Experts have given Wikipedia middling
reviews, but they miss the critical point: it's good enough.  And   
it's free, which means people actually read it.  On the web, articles
you have to pay for might as well not exist.  Even if you were    
willing to pay to read them yourself, you can't link to them.    
They're not part of the conversation.Another place democracy seems to win is in deciding what counts as
news.  I never look at any news site now except Reddit.
[2]
 I know if something major
happens, or someone writes a particularly interesting article, it   
will show up there.  Why bother checking the front page of any
specific paper or magazine?  Reddit's like an RSS feed for the whole
web, with a filter for quality.  Similar sites include Digg, a technology news site that's
rapidly approaching Slashdot in popularity, and del.icio.us, the collaborative
bookmarking network that set off the "tagging" movement.  And whereas
Wikipedia's main appeal is that it's good enough and free, these
sites suggest that voters do a significantly better job than human
editors.The most dramatic example of Web 2.0 democracy is not in the selection
of ideas, but their production.  
I've noticed for a while that the stuff I read on individual people's
sites is as good as or better than the stuff I read in newspapers
and magazines.  And now I have independent evidence: the top links
on Reddit are generally links to individual people's sites rather  
than to magazine articles or news stories.My experience of writing
for magazines suggests an explanation.  Editors.  They control the
topics you can write about, and they can generally rewrite whatever
you produce.  The result is to damp extremes.  Editing yields 95th
percentile writing—95% of articles are improved by it, but 5% are
dragged down.  5% of the time you get "throngs of geeks."On the web, people can publish whatever they want.  Nearly all of
it falls short of the editor-damped writing in print publications.
But the pool of writers is very, very large.  If it's large enough,
the lack of damping means the best writing online should surpass  
the best in print.
[3]  
And now that the web has evolved mechanisms
for selecting good stuff, the web wins net.  Selection beats damping,
for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the  
startups of the Bubble what bloggers are to the print media.  During
the Bubble, a startup meant a company headed by an MBA that was   
blowing through several million dollars of VC money to "get big
fast" in the most literal sense.  Now it means a smaller, younger, more technical group that just      
decided to make something great.  They'll decide later if they want  
to raise VC-scale funding, and if they take it, they'll take it on
their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements
of "Web 2.0."  I also see a third: not to maltreat users.  During
the Bubble a lot of popular sites were quite high-handed with users.
And not just in obvious ways, like making them register, or subjecting
them to annoying ads.  The very design of the average site in the   
late 90s was an abuse.  Many of the most popular sites were loaded
with obtrusive branding that made them slow to load and sent the
user the message: this is our site, not yours.  (There's a physical
analog in the Intel and Microsoft stickers that come on some
laptops.)I think the root of the problem was that sites felt they were giving
something away for free, and till recently a company giving anything
away for free could be pretty high-handed about it.  Sometimes it
reached the point of economic sadism: site owners assumed that the
more pain they caused the user, the more benefit it must be to them.  
The most dramatic remnant of this model may be at salon.com, where   
you can read the beginning of a story, but to get the rest you have
sit through a movie.At Y Combinator we advise all the startups we fund never to lord
it over users.  Never make users register, unless you need to in
order to store something for them.  If you do make users register,   
never make them wait for a confirmation link in an email; in fact,
don't even ask for their email address unless you need it for some
reason.  Don't ask them any unnecessary questions.  Never send them
email unless they explicitly ask for it.  Never frame pages you
link to, or open them in new windows.  If you have a free version 
and a pay version, don't make the free version too restricted.  And
if you find yourself asking "should we allow users to do x?" just 
answer "yes" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups
never to let anyone fly under them, meaning never to let any other
company offer a cheaper, easier solution.  Another way to fly low 
is to give users more power.  Let users do what they want.  If you 
don't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual
songs instead of having to buy whole albums.  The recording industry
hated the idea and resisted it as long as possible.  But it was
obvious what users wanted, so Apple flew under the labels.
[4]
Though really it might be better to describe iTunes as Web 1.5.     
Web 2.0 applied to music would probably mean individual bands giving
away DRMless songs for free.The ultimate way to be nice to users is to give them something for
free that competitors charge for.  During the 90s a lot of people   
probably thought we'd have some working system for micropayments     
by now.  In fact things have gone in the other direction.  The most   
successful sites are the ones that figure out new ways to give stuff
away for free.  Craigslist has largely destroyed the classified ad
sites of the 90s, and OkCupid looks likely to do the same to the
previous generation of dating sites.Serving web pages is very, very cheap.  If you can make even a   
fraction of a cent per page view, you can make a profit.  And
technology for targeting ads continues to improve.  I wouldn't be
surprised if ten years from now eBay had been supplanted by an      
ad-supported freeBay (or, more likely, gBay).Odd as it might<|begin_of_text|>
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
