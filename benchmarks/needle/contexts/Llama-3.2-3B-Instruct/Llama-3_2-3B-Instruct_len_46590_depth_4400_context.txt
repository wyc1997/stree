<|begin_of_text|><|begin_of_text|>December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least
two times, maybe three.  And yet if I had to write down everything
I remember from it, I doubt it would amount to much more than a
page.  Multiply this times several hundred, and I get an uneasy
feeling when I look at my bookshelves. What use is it to read all
these books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent
biography of Hilbert, I figured out if not the answer to this
question, at least something that made me feel better about it.
She writes:

  Hilbert had no patience with mathematical lectures which filled
  the students with facts but did not teach them how to frame a
  problem and solve it. He often used to tell them that "a perfect
  formulation of a problem is already half its solution."

That has always seemed to me an important point, and I was even
more convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A
combination of my own experience and other things I'd read.  None
of which I could at that moment remember!  And eventually I'd forget
that Hilbert had confirmed it too.  But my increased belief in the
importance of this idea would remain something I'd learned from
this book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if
you forget the experience or what you read, its effect on your model
of the world persists.  Your mind is like a compiled program you've
lost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle
is not what I remember from it, but my mental models of the crusades,
Venice, medieval culture, siege warfare, and so on.  Which doesn't
mean I couldn't have read more attentively, but at least the harvest
of reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But
it was a surprise to me and presumably would be to anyone else who
felt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about
forgetting, though.  There are specific implications.For example, reading and experience are usually "compiled" at the
time they happen, using the state of your brain at that time.  The
same book would get compiled differently at different points in
your life.  Which means it is very much worth reading important
books multiple times.  I always used to feel some misgivings about
rereading books.  I unconsciously lumped reading together with work
like carpentry, where having to do something again is a sign you
did it wrong the first time.  Whereas now the phrase "already read"
seems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology
will increasingly make it possible to relive our experiences.  When
people do that today it's usually to enjoy them again (e.g. when
looking at pictures of a trip) or to find the origin of some bug in
their compiled code (e.g. when Stephen Fry succeeded in remembering
the childhood trauma that prevented him from singing).  But as
technologies for recording and playing back your life improve, it
may become common for people to relive experiences without any goal
in mind, simply to learn from them again as one might when rereading
a book.Eventually we may be able not just to play back experiences but
also to index and even edit them. So although not knowing how you
know things may seem part of being human, it may not be.
Thanks to Sam Altman, Jessica Livingston, and Robert Morris for reading 
drafts of this.April 2012A palliative care nurse called Bronnie Ware made a list of the
biggest regrets
of the dying.  Her list seems plausible.  I could see
myself — can see myself — making at least 4 of these
5 mistakes.If you had to compress them into a single piece of advice, it might
be: don't be a cog.  The 5 regrets paint a portrait of post-industrial
man, who shrinks himself into a shape that fits his circumstances,
then turns dutifully till he stops.The alarming thing is, the mistakes that produce these regrets are
all errors of omission.  You forget your dreams, ignore your family,
suppress your feelings, neglect your friends, and forget to be
happy.  Errors of omission are a particularly dangerous type of
mistake, because you make them by default.I would like to avoid making these mistakes.  But how do you avoid
mistakes you make by default?  Ideally you transform your life so
it has other defaults.  But it may not be possible to do that
completely. As long as these mistakes happen by default, you probably
have to be reminded not to make them.  So I inverted the 5 regrets,
yielding a list of 5 commands

   Don't ignore your dreams; don't work too much; say what you
   think; cultivate friendships; be happy.

which I then put at the top of the file I use as a todo list.November 2005In the next few years, venture capital funds will find themselves
squeezed from four directions.  They're already stuck with a seller's
market, because of the huge amounts they raised at the end of the
Bubble and still haven't invested.  This by itself is not the end
of the world.  In fact, it's just a more extreme version of the
norm
in the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because
it's getting so cheap to start a startup.  The four causes: open
source, which makes software free; Moore's law, which makes hardware
geometrically closer to free; the Web, which makes promotion free
if you're good; and better languages, which make development a lot
cheaper.When we started our startup in 1995, the first three were our biggest
expenses.  We had to pay $5000 for the Netscape Commerce Server,
the only software that then supported secure http connections.  We
paid $3000 for a server with a 90 MHz processor and 32 meg of
memory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software
for free; people throw away computers more powerful than our first
server; and if you make something good you can generate ten times
as much traffic by word of mouth online than our first PR firm got
through the print media.And of course another big change for the average startup is that
programming languages have improved-- or rather, the median language has.  At most startups ten years
ago, software development meant ten programmers writing code in
C++.  Now the same work might be done by one or two using Python
or Ruby.During the Bubble, a lot of people predicted that startups would
outsource their development to India.  I think a better model for
the future is David Heinemeier Hansson, who outsourced his development
to a more powerful language instead.  A lot of well-known applications
are now, like BaseCamp, written by just one programmer.  And one
guy is more than 10x cheaper than ten, because (a) he won't waste
any time in meetings, and (b) since he's probably a founder, he can
pay himself nothing.Because starting a startup is so cheap, venture capitalists now
often want to give startups more money than the startups want to
take.  VCs like to invest several million at a time.  But as one
VC told me after a startup he funded would only take about half a
million, "I don't know what we're going to do.  Maybe we'll just
have to give some of it back." Meaning give some of the fund back
to the institutional investors who supplied it, because it wasn't
going to be possible to invest it all.Into this already bad situation comes the third problem: Sarbanes-Oxley.
Sarbanes-Oxley is a law, passed after the Bubble, that drastically
increases the regulatory burden on public companies. And in addition
to the cost of compliance, which is at least two million dollars a
year, the law introduces frightening legal exposure for corporate
officers.  An experienced CFO I know said flatly: "I would not
want to be CFO of a public company now."You might think that responsible corporate governance is an area
where you can't go too far.  But you can go too far in any law, and
this remark convinced me that Sarbanes-Oxley must have.  This CFO
is both the smartest and the most upstanding money guy I know.  If
Sarbanes-Oxley deters people like him from being CFOs of public  
companies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For
all practical purposes, succeeding now equals getting bought.  Which
means VCs are now in the business of finding promising little 2-3
man startups and pumping them up into companies that cost $100
million to acquire.   They didn't mean to be in this business; it's
just what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they
can buy wholesale.  Why should they wait for VCs to make the startups
they want more expensive?  Most of what the VCs add, acquirers don't
want anyway.  The acquirers already have brand recognition and HR
departments.  What they really want is the software and the developers,
and that's what the startup is in the early phase: concentrated
software and developers.Google, typically, seems to have been the first to figure this out.
"Bring us your startups early," said Google's speaker at the Startup School.  They're quite
explicit about it: they like to acquire startups at just the point
where they would do a Series A round.  (The Series A round is the
first round of real VC funding; it usually happens in the first
year.) It is a brilliant strategy, and one that other big technology
companies will no doubt try to duplicate.  Unless they want to have 
still more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the
people there are rich, or expect to be when their options vest.
Ordinary employees find it very hard to recommend an acquisition;
it's just too annoying to see a bunch of twenty year olds get rich
when you're still working for salary.  Even if it's the right thing   
for your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.
They need to do two things, one of which won't surprise them, and  
another that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley  
loosened.  This law was created to prevent future Enrons, not to
destroy the IPO market.  Since the IPO market was practically dead
when it passed, few saw what bad effects it would have.  But now 
that technology has recovered from the last bust, we can see clearly
what a bottleneck Sarbanes-Oxley has become.Startups are fragile plants—seedlings, in fact.  These seedlings
are worth protecting, because they grow into the trees of the
economy.  Much of the economy's growth is their growth.  I think
most politicians realize that.  But they don't realize just how   
fragile startups are, and how easily they can become collateral
damage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very
little noise.  If you step on the toes of the coal industry, you'll
hear about it.  But if you inadvertantly squash the startup industry,
all that happens is that the founders of the next Google stay in 
grad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash  
out partially in the Series A round.  At the moment, when VCs invest
in a startup, all the stock they get is newly issued and all the 
money goes to the company.  They could buy some stock directly from
the founders as well.Most VCs have an almost religious rule against doing this.  They
don't want founders to get a penny till the company is sold or goes
public.  VCs are obsessed with control, and they worry that they'll
have less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock
early would generally be better for the company, because it would
cause the founders' attitudes toward risk to be aligned with the
VCs'.  As things currently work, their attitudes toward risk tend
to be diametrically opposed: the founders, who have nothing, would
prefer a 100% chance of $1 million to a 20% chance of $10 million,
while the VCs can afford to be "rational" and prefer the latter.Whatever they say, the reason founders are selling their companies
early instead of doing Series A rounds is that they get paid up
front.  That first million is just worth so much more than the
subsequent ones.  If founders could sell a little stock early,
they'd be happy to take VC money and bet the rest on a bigger
outcome.So why not let the founders have that first million, or at least
half million?  The VCs would get same number of shares for the   
money.  So what if some of the money would go to the  
founders instead of the company?Some VCs will say this is
unthinkable—that they want all their money to be put to work
growing the company.  But the fact is, the huge size of current VC
investments is dictated by the structure
of VC funds, not the needs of startups.  Often as not these large  
investments go to work destroying the company rather than growing
it.The angel investors who funded our startup let the founders sell
some stock directly to them, and it was a good deal for everyone. 
The angels made a huge return on that investment, so they're happy.
And for us founders it blunted the terrifying all-or-nothingness
of a startup, which in its raw form is more a distraction than a
motivator.If VCs are frightened at the idea of letting founders partially
cash out, let me tell them something still more frightening: you
are now competing directly with Google.
Thanks to Trevor Blackwell, Sarah Harlin, Jessica
Livingston, and Robert Morris for reading drafts of this.April 2006(This essay is derived from a talk at the 2006 
Startup School.)The startups we've funded so far are pretty quick, but they seem
quicker to learn some lessons than others.  I think it's because
some things about startups are kind of counterintuitive.We've now 
invested 
in enough companies that I've learned a trick
for determining which points are the counterintuitive ones:
they're the ones I have to keep repeating.So I'm going to number these points, and maybe with future startups
I'll be able to pull off a form of Huffman coding. I'll make them
all read this, and then instead of nagging them in detail, I'll
just be able to say: number four!
1. Release Early.The thing I probably repeat most is this recipe for a startup: get
a version 1 out fast, then improve it based on users' reactions.By "release early" I don't mean you should release something full
of bugs, but that you should release something minimal.  Users hate
bugs, but they don't seem to mind a minimal version 1, if there's
more coming soon.There are several reasons it pays to get version 1 done fast.  One
is that this is simply the right way to write software, whether for
a startup or not.  I've been repeating that since 1993, and I haven't seen much since to
contradict it.  I've seen a lot of startups die because they were
too slow to release stuff, and none because they were too quick.
[1]One of the things that will surprise you if you build something
popular is that you won't know your users.  Reddit now has almost half a million
unique visitors a month.  Who are all those people?  They have no
idea.  No web startup does.  And since you don't know your users,
it's dangerous to guess what they'll like.  Better to release
something and let them tell you.Wufoo took this to heart and released
their form-builder before the underlying database.  You can't even
drive the thing yet, but 83,000 people came to sit in the driver's
seat and hold the steering wheel.  And Wufoo got valuable feedback
from it: Linux users complained they used too much Flash, so they
rewrote their software not to.  If they'd waited to release everything
at once, they wouldn't have discovered this problem till it was
more deeply wired in.Even if you had no users, it would still be important to release
quickly, because for a startup the initial release acts as a shakedown
cruise.  If anything major is broken-- if the idea's no good,
for example, or the founders hate one another-- the stress of getting
that first version out will expose it.  And if you have such problems
you want to find them early.Perhaps the most important reason to release early, though, is that
it makes you work harder.  When you're working on something that
isn't released, problems are intriguing.  In something that's out
there, problems are alarming.  There is a lot more urgency once you
release.  And I think that's precisely why people put it off.  They
know they'll have to work a lot harder once they do. 
[2]
2. Keep Pumping Out Features.Of course, "release early" has a second component, without which
it would be bad advice.  If you're going to start with something
that doesn't do much, you better improve it fast.What I find myself repeating is "pump out features."  And this rule
isn't just for the initial stages.  This is something all startups
should do for as long as they want to be considered startups.I don't mean, of course, that you should make your application ever
more complex.  By "feature" I mean one unit of hacking-- one quantum
of making users' lives better.As with exercise, improvements beget improvements.  If you run every
day, you'll probably feel like running tomorrow.  But if you skip
running for a couple weeks, it will be an effort to drag yourself
out.  So it is with hacking: the more ideas you implement, the more
ideas you'll have.  You should make your system better at least in
some small way every day or two.This is not just a good way to get development done; it is also a
form of marketing.  Users love a site that's constantly improving.
In fact, users expect a site to improve.  Imagine if you visited a
site that seemed very good, and then returned two months later and
not one thing had changed.  Wouldn't it start to seem lame? 
[3]They'll like you even better when you improve in response to their
comments, because customers are used to companies ignoring them.
If you're the rare exception-- a company that actually listens--
you'll generate fanatical loyalty.  You won't need to advertise,
because your users will do it for you.This seems obvious too, so why do I have to keep repeating it?  I
think the problem here is that people get used to how things are.
Once a product gets past the stage where it has glaring flaws, you
start to get used to it, and gradually whatever features it happens
to have become its identity.  For example, I doubt many people at
Yahoo (or Google for that matter) realized how much better web mail
could be till Paul Buchheit showed them.I think the solution is to assume that anything you've made is far
short of what it could be.  Force yourself, as a sort of intellectual
exercise, to keep thinking of improvements.  Ok, sure, what you
have is perfect.  But if you had to change something, what would
it be?If your product seems finished, there are two possible explanations:
(a) it is finished, or (b) you lack imagination.  Experience suggests
(b) is a thousand times more likely.
3. Make Users Happy.Improving constantly is an instance of a more general rule: make
users happy.  One thing all startups have in common is that they
can't force anyone to do anything.  They can't force anyone to use
their software, and they can't force anyone to do deals with them.
A startup has to sing for its supper.  That's why the successful
ones make great things.  They have to, or die.When you're running a startup you feel like a little bit of debris
blown about by powerful winds.  The most powerful wind is users.
They can either catch you and loft you up into the sky, as they did
with Google, or leave you flat on the pavement, as they do with
most startups.  Users are a fickle wind, but more powerful than any
other.  If they take you up, no competitor can keep you down.As a little piece of debris, the rational thing for you to do is
not to lie flat, but to curl yourself into a shape the wind will
catch.I like the wind metaphor because it reminds you how impersonal the
stream of traffic is.  The vast majority of people who visit your
site will be casual visitors.  It's them you have to design your
site for.  The people who really care will find what they want by
themselves.The median visitor will arrive with their finger poised on the Back
button.  Think about your own experience: most links you
follow lead to something lame.  Anyone who has used the web for
more than a couple weeks has been trained to click on Back after
following a link.  So your site has to say "Wait!  Don't click on
Back.  This site isn't lame.  Look at this, for example."There are two things you have to do to make people pause.  The most
important is to explain, as concisely as possible, what the hell
your site is about.  How often have you visited a site that seemed
to assume you already knew what they did?  For example, the corporate
site that says the
company makes

  enterprise content management solutions for business that enable
  organizations to unify people, content and processes to minimize
  business risk, accelerate time-to-value and sustain lower total
  cost of ownership.

An established company may get away with such an opaque description,
but no startup can.  A startup
should be able to explain in one or two sentences exactly what it
does. 
[4]
And not just to users.  You need this for everyone:
investors, acquirers, partners, reporters, potential employees, and
even current employees.  You probably shouldn't even start a company
to do something that can't be described compellingly in one or two
sentences.The other thing I repeat is to give people everything you've got,
right away.  If you have something impressive, try to put it on the
front page, because that's the only one most visitors will see.
Though indeed there's a paradox here: the more you push the good
stuff toward the front, the more likely visitors are to explore
further. 
[5]In the best case these two suggestions get combined: you tell
visitors what your site is about by showing them.  One of the
standard pieces of advice in fiction writing is "show, don't tell."
Don't say that a character's angry; have him grind his teeth, or
break his pencil in half.  Nothing will explain what your site does
so well as using it.The industry term here is "conversion."  The job of your site is
to convert casual visitors into users-- whatever your definition
of a user is.  You can measure this in your growth rate.  Either
your site is catching on, or it isn't, and you must know which.  If
you have decent growth, you'll win in the end, no matter how obscure
you are now.  And if you don't, you need to fix something.
4. Fear the Right Things.Another thing I find myself saying a lot is "don't worry."  Actually,
it's more often "don't worry about this; worry about that instead."
Startups are right to be paranoid, but they sometimes fear the wrong
things.Most visible disasters are not so alarming as they seem.  Disasters
are normal in a startup: a founder quits, you discover a patent
that covers what you're doing, your servers keep crashing, you run
into an insoluble technical problem, you have to change your name,
a deal falls through-- these are all par for the course.  They won't
kill you unless you let them.Nor will most competitors.  A lot of startups worry "what if Google
builds something like us?"  Actually big companies are not the ones
you have to worry about-- not even Google.  The people at Google
are smart, but no smarter than you; they're not as motivated, because
Google is not going to go out of business if this one product fails;
and even at Google they have a lot of bureaucracy to slow them down.What you should fear, as a startup, is not the established players,
but other startups you don't know exist yet.  They're way more
dangerous than Google because, like you, they're cornered animals.Looking just at existing competitors can give you a false sense of
security.  You should compete against what someone else could be
doing, not just what you can see people doing.  A corollary is that
you shouldn't relax just because you have no visible competitors
yet.  No matter what your idea, there's someone else out there
working on the same thing.That's the downside of it being easier to start a startup: more people
are doing it.  But I disagree with Caterina Fake when she says that
makes this a bad time to start a startup.  More people are starting
startups, but not as many more as could.  Most college graduates
still think they have to get a job.  The average person can't ignore
something that's been beaten into their head since they were three
just because serving web pages recently got a lot cheaper.And in any case, competitors are not the biggest threat.  Way more
startups hose themselves than get crushed by competitors.  There
are a lot of ways to do it, but the three main ones are internal
disputes, inertia, and ignoring users.  Each is, by itself, enough
to kill you.  But if I had to pick the worst, it would be ignoring
users.  If you want a recipe for a startup that's going to die,
here it is: a couple of founders who have some great idea they know
everyone is going to love, and that's what they're going to build,
no matter what.Almost everyone's initial plan is broken.  If companies stuck to
their initial plans, Microsoft would be selling programming languages,
and Apple would be selling printed circuit boards.  In both cases
their customers told them what their business should be-- and they
were smart enough to listen.As Richard Feynman said, the imagination of nature is greater than
the imagination of man.  You'll find more interesting things by
looking at the world than you could ever produce just by thinking.
This principle is very powerful.  It's why the best abstract painting
still falls short of Leonardo, for example.  And it applies to
startups too.  No idea for a product could ever be so clever as the
ones you can discover by smashing a beam of prototypes into a beam
of users.
5. Commitment Is a Self-Fulfilling Prophecy.I now have enough experience with startups to be able to say what
the most important quality is in a startup founder, and it's not
what you might think.  The most important quality in a startup
founder is determination.  Not intelligence-- determination.This is a little depressing.  I'd like to believe Viaweb succeeded
because we were smart, not merely determined.  A lot of people in
the startup world want to believe that.  Not just founders, but
investors too.  They like the idea of inhabiting a world ruled by
intelligence.  And you can tell they really believe this, because
it affects their investment decisions.Time after time VCs invest in startups founded by eminent professors.
This may work in biotech, where a lot of startups simply commercialize
existing research, but in software you want to invest in students,
not professors.  Microsoft, Yahoo, and Google were all founded by
people who dropped out of school to do it.  What students lack in
experience they more than make up in dedication.Of course, if you want to get rich, it's not enough merely to be
determined.  You have to be smart too, right?  I'd like to think
so, but I've had an experience that convinced me otherwise: I spent
several years living in New York.You can lose quite a lot in the brains department and it won't kill
you.  But lose even a little bit in the commitment department, and
that will kill you very rapidly.Running a startup is like walking on your hands: it's possible, but
it requires extraordinary effort.  If an ordinary employee were
asked to do the things a startup founder has to, he'd be very
indignant.  Imagine if you were hired at some big company, and in
addition to writing software ten times faster than you'd ever had
to before, they expected you to answer support calls, administer
the servers, design the web site, cold-call customers, find the
company office space, and go out and get everyone lunch.And to do all this not in the calm, womb-like atmosphere of a big
company, but against a backdrop of constant disasters.  That's the
part that really demands determination.  In a startup, there's
always some disaster happening.  So if you're the least bit inclined
to find an excuse to quit, there's always one right there.But if you lack commitment, chances are it will have been hurting
you long before you actually quit.  Everyone who deals with startups
knows how important commitment is, so if they sense you're ambivalent,
they won't give you much attention.  If you lack commitment, you'll
just find that for some mysterious reason good things happen to
your competitors but not to you.  If you lack commitment, it will
seem to you that you're unlucky.Whereas if you're determined to stick around, people will pay
attention to you, because odds are they'll have to deal with you
later.  You're a local, not just a tourist, so everyone has to come
to terms with you.At Y Combinator we sometimes mistakenly fund teams who have the
attitude that they're going to give this startup thing a shot for
three months, and if something great happens, they'll stick with
it-- "something great" meaning either that someone wants to buy
them or invest millions of dollars in them.  But if this is your
attitude, "something great" is very unlikely to happen to you,
because both acquirers and investors judge you by your level of
commitment.If an acquirer thinks you're going to stick around no matter what,
they'll be more likely to buy you, because if they don't and you
stick around, you'll probably grow, your price will go up, and
they'll be left wishing they'd bought you earlier.  Ditto for
investors.  What really motivates investors, even big VCs, is not
the hope of good returns, but the fear of missing out. 
[6]
So if
you make it clear you're going to succeed no matter what, and the only
reason you need them is to make it happen a little faster, you're
much more likely to get money.You can't fake this.  The only way to convince everyone that you're
ready to fight to the death is actually to be ready to.You have to be the right kind of determined, though.  I carefully
chose the word determined rather than stubborn, because stubbornness
is a disastrous quality in a startup.  You have to be determined,
but flexible, like a running back.  A successful running back doesn't
just put his head down and try to run through people.  He improvises:
if someone appears in front of him, he runs around them; if someone
tries to grab him, he spins out of their grip; he'll even run in
the wrong direction briefly if that will help.  The one thing he'll
never do is stand still. 
[7]
6. There Is Always Room.I was talking recently to a startup founder about whether it might
be good to add a social component to their software.  He said he
didn't think so, because the whole social thing was tapped out.
Really?  So in a hundred years the only social networking sites
will be the Facebook, MySpace, Flickr, and Del.icio.us?  Not likely.There is always room for new stuff.  At every point in history,
even the darkest bits of the dark ages, people were discovering
things that made everyone say "why didn't anyone think of that
before?"  We know this continued to be true up till 2004, when the
Facebook was founded-- though strictly speaking someone else did
think of that.The reason we don't see the opportunities all around us is that we
adjust to however things are, and assume that's how things have to
be.  For example, it would seem crazy to most people to try to make
a better search engine than Google.  Surely that field, at least,
is tapped out.  Really?  In a hundred years-- or even twenty-- are
people still going to search for information using something like
the current Google?  Even Google probably doesn't think that.In particular, I don't think there's any limit to the number of
startups.  Sometimes you hear people saying "All these guys starting
startups now are going to be disappointed. How many little startups
are Google and Yahoo going to buy, after all?" That sounds cleverly
skeptical, but I can prove it's mistaken.  No one proposes that
there's some limit to the number of people who can be employed in
an economy consisting of big, slow-moving companies with a couple
thousand people each.  Why should there be any limit to the number
who could be employed by small, fast-moving companies with ten each?
It seems to me the only limit would be the number of people who
want to work that hard.The limit on the number of startups is not the number that can get
acquired by Google and Yahoo-- though it seems even that should
be unlimited, if the startups were actually worth buying-- but the
amount of wealth that can be created.  And I don't think there's
any limit on that, except cosmological ones.So for all practical purposes, there is no limit to the number of
startups.  Startups make wealth, which means they make things people
want, and if there's a limit on the number of things people want,
we are nowhere near it.  I still don't even have a flying car.
7. Don't Get Your Hopes Up.This is another one I've been repeating since long before Y Combinator.
It was practically the corporate motto at Viaweb.Startup founders are naturally optimistic.  They wouldn't do it
otherwise.  But you should treat your optimism the way you'd treat
the core of a nuclear reactor: as a source of power that's also
very dangerous.  You have to build a shield around it, or it will
fry you.The shielding of a reactor is not uniform; the reactor would be
useless if it were.  It's pierced in a few places to let pipes in.
An optimism shield has to be pierced too.  I think the place to
draw the line is between what you expect of yourself, and what you
expect of other people.  It's ok to be optimistic about what you
can do, but assume the worst about machines and other people.This is particularly necessary in a startup, because you tend to
be pushing the limits of whatever you're doing.  So things don't
happen in the smooth, predictable way they do in the rest of the
world.  Things change suddenly, and usually for the worse.Shielding your optimism is nowhere more important than with deals.
If your startup is doing a deal, just assume it's not going to
happen.  The VCs who say they're going to invest in you aren't.
The company that says they're going to buy you isn't.  The big
customer who wants to use your system in their whole company won't.
Then if things work out you can be pleasantly surprised.The reason I warn startups not to get their hopes up is not to save
them from being disappointed when things fall through.  It's
for a more practical reason: to prevent them from leaning their
company against something that's going to fall over, taking them
with it.For example, if someone says they want to invest in you, there's a
natural tendency to stop looking for other investors.  That's why
people proposing deals seem so positive: they want you to
stop looking.  And you want to stop too, because doing deals is a
pain.  Raising money, in particular, is a huge time sink.  So you
have to consciously force yourself to keep looking.Even if you ultimately do the first deal, it will be to your advantage
to have kept looking, because you'll get better terms.  Deals are
dynamic; unless you're negotiating with someone unusually honest,
there's not a single point where you shake hands and the deal's
done. There are usually a lot of subsidiary questions to be cleared
up after the handshake, and if the other side senses weakness-- if
they sense you need this deal-- they will be very tempted to screw
you in the details.VCs and corp dev guys are professional negotiators.  They're trained
to take advantage of weakness. 
[8]
So while they're often nice
guys, they just can't help it.  And as pros they do this more than
you.  So don't even try to bluff them.  The only way a startup can
have any leverage in a deal is genuinely not to need it.  And if
you don't believe in a deal, you'll be less likely to depend on it.So I want to plant a hypnotic suggestion in your heads: when you
hear someone say the words "we want to invest in you" or "we want
to acquire you," I want the following phrase to appear automatically
in your head: don't get your hopes up.  Just continue running
your company as if this deal didn't exist.  Nothing is more likely
to make it close.The way to succeed in a startup is to focus on the goal of getting
lots of users, and keep walking swiftly toward it while investors
and acquirers scurry alongside trying to wave money in your face.
Speed, not MoneyThe way I've described it, starting a startup sounds pretty stressful.
It is.  When I talk to the founders of the companies we've funded,
they all say the same thing: I knew it would be hard, but I didn't
realize it would be this hard.So why do it?  It would be worth enduring a lot of pain and stress
to do something grand or heroic, but just to make money?  Is making
money really that important?No, not really.  It seems ridiculous to me when people take business
too seriously.  I regard making money as a boring errand to be got
out of the way as soon as possible.  There is nothing grand or
heroic about starting a startup per se.So why do I spend so much time thinking about startups?  I'll tell
you why.  Economically, a startup is best seen not as a way to get
rich, but as a way to work faster.  You have to make a living, and
a startup is a way to get that done quickly, instead of letting it
drag on through your whole life.
[9]We take it for granted most of the time, but human life is fairly
miraculous.  It is also palpably short.  You're given this marvellous
thing, and then poof, it's taken away.  You can see why people
invent gods to explain it.  But even to people who don't believe
in gods, life commands respect.  There are times in most of our
lives when the days go by in a blur, and almost everyone has a
sense, when this happens, of wasting something precious.  As Ben
Franklin said, if you love life, don't waste time, because time is
what life is made of.So no, there's nothing particularly grand about making money.  That's
not what makes startups worth the trouble.  What's important about
startups is the speed.  By compressing the dull but necessary task
of making a living into the smallest possible time, you show respect
for life, and there is something grand about that.Notes[1]
Startups can die from releasing something full of bugs, and not
fixing them fast enough, but I don't know of any that died from
releasing something stable but minimal very early, then promptly
improving it.[2]
I know this is why I haven't released Arc.  The moment I do,
I'll have people nagging me for features.[3]
A web site is different from a book or movie or desktop application
in this respect.  Users judge a site not as a single snapshot, but
as an animation with multiple frames.  Of the two, I'd say the rate of
improvement is more important to users than where you currently
are.[4]
It should not always tell this to users, however.  For example,
MySpace is basically a replacement mall for mallrats.  But it was
wiser for them, initially, to pretend that the site was about bands.[5]
Similarly, don't make users register to try your site.  Maybe
what you have is so valuable that visitors should gladly register
to get at it.  But they've been trained to expect the opposite.
Most of the things they've tried on the web have sucked-- and
probably especially those that made them register.[6]
VCs have rational reasons for behaving this way. They don't
make their money (if they make money) off their median investments.
In a typical fund, half the companies fail, most of the rest generate
mediocre returns, and one or two "make the fund" by succeeding
spectacularly.  So if they miss just a few of the most promising
opportunities, it could hose the whole fund.[7]
The attitude of a running back doesn't translate to soccer.
Though it looks great when a forward dribbles past multiple defenders,
a player who persists in trying such things will do worse in the
long term than one who passes.[8]
The reason Y Combinator never negotiates valuations
is that we're not professional negotiators, and don't want to turn
into them.[9]
There are two ways to do 
work you love: (a) to make money, then work
on what you love, or (b) to get a job where you get paid to work on
stuff you love.  In practice the first phases of both
consist mostly of unedifying schleps, and in (b) the second phase is less
secure.Thanks to Sam Altman, Trevor Blackwell, Beau Hartshorne, Jessica 
Livingston, and Robert Morris for reading drafts of this.September 2017The most valuable insights are both general and surprising. 
F = ma for example. But general and surprising is a hard
combination to achieve. That territory tends to be picked
clean, precisely because those insights are so valuable.Ordinarily, the best that people can do is one without the
other: either surprising without being general (e.g.
gossip), or general without being surprising (e.g.
platitudes).Where things get interesting is the moderately valuable
insights.  You get those from small additions of whichever
quality was missing.  The more common case is a small
addition of generality: a piece of gossip that's more than
just gossip, because it teaches something interesting about
the world. But another less common approach is to focus on
the most general ideas and see if you can find something new
to say about them. Because these start out so general, you
only need a small delta of novelty to produce a useful
insight.A small delta of novelty is all you'll be able to get most
of the time. Which means if you take this route, your ideas
will seem a lot like ones that already exist. Sometimes
you'll find you've merely rediscovered an idea that did
already exist.  But don't be discouraged.  Remember the huge
multiplier that kicks in when you do manage to think of
something even a little new.Corollary: the more general the ideas you're talking about,
the less you should worry about repeating yourself.  If you
write enough, it's inevitable you will.  Your brain is much
the same from year to year and so are the stimuli that hit
it. I feel slightly bad when I find I've said something
close to what I've said before, as if I were plagiarizing
myself. But rationally one shouldn't.  You won't say
something exactly the same way the second time, and that
variation increases the chance you'll get that tiny but
critical delta of novelty.And of course, ideas beget ideas.  (That sounds 
familiar.)
An idea with a small amount of novelty could lead to one
with more. But only if you keep going. So it's doubly
important not to let yourself be discouraged by people who
say there's not much new about something you've discovered.
"Not much new" is a real achievement when you're talking
about the most general ideas. It's not true that there's nothing new under the sun.  There
are some domains where there's almost nothing new.  But
there's a big difference between nothing and almost nothing,
when it's multiplied by the area under the sun.
Thanks to Sam Altman, Patrick Collison, and Jessica
Livingston for reading drafts of this.February 2007A few days ago I finally figured out something I've wondered about
for 25 years: the relationship between wisdom and intelligence.
Anyone can see they're not the same by the number of people who are
smart, but not very wise.  And yet intelligence and wisdom do seem
related.  How?What is wisdom?  I'd say it's knowing what to do in a lot of
situations.  I'm not trying to make a deep point here about the
true nature of wisdom, just to figure out how we use the word.  A
wise person is someone who usually knows the right thing to do.And yet isn't being smart also knowing what to do in certain
situations?  For example, knowing what to do when the teacher tells
your elementary school class to add all the numbers from 1 to 100?
[1]Some say wisdom and intelligence apply to different types of
problems—wisdom to human problems and intelligence to abstract
ones.  But that isn't true.  Some wisdom has nothing to do with
people: for example, the wisdom of the engineer who knows certain
structures are less prone to failure than others.  And certainly
smart people can find clever solutions to human problems as well
as abstract ones. 
[2]Another popular explanation is that wisdom comes from experience
while intelligence is innate.  But people are not simply wise in
proportion to how much experience they have.  Other things must
contribute to wisdom besides experience, and some may be innate: a
reflective disposition, for example.Neither of the conventional explanations of the difference between
wisdom and intelligence stands up to scrutiny.  So what is the
difference?  If we look at how people use the words "wise" and
"smart," what they seem to mean is different shapes of performance.Curve"Wise" and "smart" are both ways of saying someone knows what to
do.  The difference is that "wise" means one has a high average
outcome across all situations, and "smart" means one does spectacularly
well in a few.  That is, if you had a graph in which the x axis
represented situations and the y axis the outcome, the graph of the
wise person would be high overall, and the graph of the smart person
would have high peaks.The distinction is similar to the rule that one should judge talent
at its best and character at its worst.  Except you judge intelligence
at its best, and wisdom by its average.  That's how the two are
related: they're the two different senses in which the same curve
can be high.So a wise person knows what to do in most situations, while a smart
person knows what to do in situations where few others could.  We
need to add one more qualification: we should ignore cases where
someone knows what to do because they have inside information. 
[3]
But aside from that, I don't think we can get much more specific
without starting to be mistaken.Nor do we need to.  Simple as it is, this explanation predicts, or
at least accords with, both of the conventional stories about the
distinction between wisdom and intelligence.  Human problems are
the most common type, so being good at solving those is key in
achieving a high average outcome.   And it seems natural that a
high average outcome depends mostly on experience, but that dramatic
peaks can only be achieved by people with certain rare, innate
qualities; nearly anyone can learn to be a good swimmer, but to be
an Olympic swimmer you need a certain body type.This explanation also suggests why wisdom is such an elusive concept:
there's no such thing.  "Wise" means something—that one is
on average good at making the right choice.  But giving the name
"wisdom" to the supposed quality that enables one to do that doesn't
mean such a thing exists.  To the extent "wisdom" means anything,
it refers to a grab-bag of qualities as various as self-discipline,
experience, and empathy.  
[4]Likewise, though "intelligent" means something, we're asking for
trouble if we insist on looking for a single thing called "intelligence."
And whatever its components, they're not all innate.  We use the
word "intelligent" as an indication of ability: a smart person can
grasp things few others could.  It does seem likely there's some
inborn predisposition to intelligence (and wisdom too), but this
predisposition is not itself intelligence.One reason we tend to think of intelligence as inborn is that people
trying to measure it have concentrated on the aspects of it that
are most measurable.  A quality that's inborn will obviously be
more convenient to work with than one that's influenced by experience,
and thus might vary in the course of a study.  The problem comes
when we drag the word "intelligence" over onto what they're measuring.
If they're measuring something inborn, they can't be measuring
intelligence.  Three year olds aren't smart.   When we describe one
as smart, it's shorthand for "smarter than other three year olds."SplitPerhaps it's a technicality to point out that a predisposition to
intelligence is not the same as intelligence.  But it's an important
technicality, because it reminds us that we can become smarter,
just as we can become wiser.The alarming thing is that we may have to choose between the two.If wisdom and intelligence are the average and peaks of the same
curve, then they converge as the number of points on the curve
decreases.  If there's just one point, they're identical: the average
and maximum are the same.  But as the number of points increases,
wisdom and intelligence diverge.  And historically the number of
points on the curve seems to have been increasing: our ability is
tested in an ever wider range of situations.In the time of Confucius and Socrates, people seem to have regarded
wisdom, learning, and intelligence as more closely related than we
do.  Distinguishing between "wise" and "smart" is a modern habit.
[5]
And the reason we do is that they've been diverging.  As knowledge
gets more specialized, there are more points on the curve, and the
distinction between the spikes and the average becomes sharper,
like a digital image rendered with more pixels.One consequence is that some old recipes may have become obsolete.
At the very least we have to go back and figure out if they were
really recipes for wisdom or intelligence.  But the really striking
change, as intelligence and wisdom drift apart, is that we may have
to decide which we prefer.  We may not be able to optimize for both
simultaneously.Society seems to have voted for intelligence.  We no longer admire
the sage—not the way people did two thousand years ago.  Now
we admire the genius.  Because in fact the distinction we began
with has a rather brutal converse: just as you can be smart without
being very wise, you can be wise without being very smart.  That
doesn't sound especially admirable.  That gets you James Bond, who
knows what to do in a lot of situations, but has to rely on Q for
the ones involving math.Intelligence and wisdom are obviously not mutually exclusive.  In
fact, a high average may help support high peaks.  But there are
reasons to believe that at some point you have to choose between
them.  One is the example of very smart people, who are so often
unwise that in popular culture this now seems to be regarded as the
rule rather than the exception.  Perhaps the absent-minded professor
is wise in his way, or wiser than he seems, but he's not wise in
the way Confucius or Socrates wanted people to be. 
[6]NewFor both Confucius and Socrates, wisdom, virtue, and happiness were
necessarily related.  The wise man was someone who knew what the
right choice was and always made it; to be the right choice, it had
to be morally right; he was therefore always happy, knowing he'd
done the best he could.  I can't think of many ancient philosophers
who would have disagreed with that, so far as it goes."The superior man is always happy; the small man sad," said Confucius.
[7]Whereas a few years ago I read an interview with a mathematician
who said that most nights he went to bed discontented, feeling he
hadn't made enough progress.  
[8]
The Chinese and Greek words we
translate as "happy" didn't mean exactly what we do by it, but
there's enough overlap that this remark contradicts them.Is the mathematician a small man because he's discontented?  No;
he's just doing a kind of work that wasn't very common in Confucius's
day.Human knowledge seems to grow fractally.  Time after time, something
that seemed a small and uninteresting area—experimental error,
even—turns out, when examined up close, to have as much in
it as all knowledge up to that point.  Several of the fractal buds
that have exploded since ancient times involve inventing and
discovering new things.  Math, for example, used to be something a
handful of people did part-time.  Now it's the career of thousands.
And in work that involves making new things, some old rules don't
apply.Recently I've spent some time advising people, and there I find the
ancient rule still works: try to understand the situation as well
as you can, give the best advice you can based on your experience,
and then don't worry about it, knowing you did all you could.  But
I don't have anything like this serenity when I'm writing an essay.
Then I'm worried.  What if I run out of ideas?  And when I'm writing,
four nights out of five I go to bed discontented, feeling I didn't
get enough done.Advising people and writing are fundamentally different types of
work.  When people come to you with a problem and you have to figure
out the right thing to do, you don't (usually) have to invent
anything.  You just weigh the alternatives and try to judge which
is the prudent choice.  But prudence can't tell me what sentence
to write next.  The search space is too big.Someone like a judge or a military officer can in much of his work
be guided by duty, but duty is no guide in making things.  Makers
depend on something more precarious: inspiration.  And like most
people who lead a precarious existence, they tend to be worried,
not contented.  In that respect they're more like the small man of
Confucius's day, always one bad harvest (or ruler) away from
starvation. Except instead of being at the mercy of weather and
officials, they're at the mercy of their own imagination.LimitsTo me it was a relief just to realize it might be ok to be discontented.
The idea that a successful person should be happy has thousands of
years of momentum behind it.  If I was any good, why didn't I have
the easy confidence winners are supposed to have?  But that, I now
believe, is like a runner asking "If I'm such a good athlete, why
do I feel so tired?" Good runners still get tired; they just get
tired at higher speeds.People whose work is to invent or discover things are in the same
position as the runner.  There's no way for them to do the best
they can, because there's no limit to what they could do.  The
closest you can come is to compare yourself to other people.  But
the better you do, the less this matters.  An undergrad who gets
something published feels like a star.  But for someone at the top
of the field, what's the test of doing well?  Runners can at least
compare themselves to others doing exactly the same thing; if you
win an Olympic gold medal, you can be fairly content, even if you
think you could have run a bit faster.  But what is a novelist to
do?Whereas if you're doing the kind of work in which problems are
presented to you and you have to choose between several alternatives,
there's an upper bound on your performance: choosing the best every
time.  In ancient societies, nearly all work seems to have been of
this type.  The peasant had to decide whether a garment was worth
mending, and the king whether or not to invade his neighbor, but
neither was expected to invent anything.  In principle they could
have; the king could have invented firearms, then invaded his
neighbor.  But in practice innovations were so rare that they weren't
expected of you, any more than goalkeepers are expected to score
goals. 
[9]
In practice, it seemed as if there was a correct decision
in every situation, and if you made it you'd done your job perfectly,
just as a goalkeeper who prevents the other team from scoring is
considered to have played a perfect game.In this world, wisdom seemed paramount.  
[10]
Even now, most people
do work in which problems are put before them and they have to
choose the best alternative.  But as knowledge has grown more
specialized, there are more and more types of work in which people
have to make up new things, and in which performance is therefore
unbounded.  Intelligence has become increasingly important relative
to wisdom because there is more room for spikes.RecipesAnother sign we may have to choose between intelligence and wisdom
is how different their recipes are.  Wisdom seems to come largely
from curing childish qualities, and intelligence largely from
cultivating them.Recipes for wisdom, particularly ancient ones, tend to have a
remedial character.  To achieve wisdom one must cut away all the
debris that fills one's head on emergence from childhood, leaving
only the important stuff.  Both self-control and experience have
this effect: to eliminate the random biases that come from your own
nature and from the circumstances of your upbringing respectively.
That's not all wisdom is, but it's a large part of it.  Much of
what's in the sage's head is also in the head of every twelve year
old.  The difference is that in the head of the twelve year old
it's mixed together with a lot of random junk.The path to intelligence seems to be through working on hard problems.
You develop intelligence as you might develop muscles, through
exercise.  But there can't be too much compulsion here.  No amount
of discipline can replace genuine curiosity.  So cultivating
intelligence seems to be a matter of identifying some bias in one's
character—some tendency to be interested in certain types of
things—and nurturing it.  Instead of obliterating your
idiosyncrasies in an effort to make yourself a neutral vessel for
the truth, you select one and try to grow it from a seedling into
a tree.The wise are all much alike in their wisdom, but very smart people
tend to be smart in distinctive ways.Most of our educational traditions aim at wisdom. So perhaps one
reason schools work badly is that they're trying to make intelligence
using recipes for wisdom.  Most recipes for wisdom have an element
of subjection.  At the very least, you're supposed to do what the
teacher says.  The more extreme recipes aim to break down your
individuality the way basic training does.  But that's not the route
to intelligence.  Whereas wisdom comes through humility, it may
actually help, in cultivating intelligence, to have a mistakenly
high opinion of your abilities, because that encourages you to keep
working.  Ideally till you realize how mistaken you were.(The reason it's hard to learn new skills late in life is not just
that one's brain is less malleable.  Another probably even worse
obstacle is that one has higher standards.)I realize we're on dangerous ground here.  I'm not proposing the
primary goal of education should be to increase students' "self-esteem."
That just breeds laziness.  And in any case, it doesn't really fool
the kids, not the smart ones.  They can tell at a young age that a
contest where everyone wins is a fraud.A teacher has to walk a narrow path: you want to encourage kids to
come up with things on their own, but you can't simply applaud
everything they produce.  You have to be a good audience: appreciative,
but not too easily impressed.  And that's a lot of work.  You have
to have a good enough grasp of kids' capacities at different ages
to know when to be surprised.That's the opposite of traditional recipes for education.  Traditionally
the student is the audience, not the teacher; the student's job is
not to invent, but to absorb some prescribed body of material.  (The
use of the term "recitation" for sections in some colleges is a
fossil of this.) The problem with these old traditions is that
they're too much influenced by recipes for wisdom.DifferentI deliberately gave this essay a provocative title; of course it's
worth being wise.  But I think it's important to understand the
relationship between intelligence and wisdom, and particularly what
seems to be the growing gap between them.  That way we can avoid
applying rules and standards to intelligence that are really meant
for wisdom.  These two senses of "knowing what to do" are more
different than most people realize.  The path to wisdom is through
discipline, and the path to intelligence through carefully selected
self-indulgence.  Wisdom is universal, and intelligence idiosyncratic.
And while wisdom yields calmness, intelligence much of the time
leads to discontentment.That's particularly worth remembering.  A physicist friend recently
told me half his department was on Prozac.  Perhaps if we acknowledge
that some amount of frustration is inevitable in certain kinds
of work, we can mitigate its effects.  Perhaps we can box it up and
put it away some of the time, instead of letting it flow together
with everyday sadness to produce what seems an alarmingly large
pool.  At the very least, we can avoid being discontented about
being discontented.If you feel exhausted, it's not necessarily because there's something
wrong with you.  Maybe you're just running fast.Notes[1]
Gauss was supposedly asked this when he was 10.  Instead of
laboriously adding together the numbers like the other students,
he saw that they consisted of 50 pairs that each summed to 101 (100
+ 1, 99 + 2, etc), and that he could just multiply 101 by 50 to get
the answer, 5050.[2]
A variant is that intelligence is the ability to solve problems,
and wisdom the judgement to know how to use those solutions.   But
while this is certainly an important relationship between wisdom
and intelligence, it's not the distinction between them.  Wisdom
is useful in solving problems too, and intelligence can help in
deciding what to do with the solutions.[3]
In judging both intelligence and wisdom we have to factor out
some knowledge. People who know the combination of a safe will be
better at opening it than people who don't, but no one would say
that was a test of intelligence or wisdom.But knowledge overlaps with wisdom and probably also intelligence.
A knowledge of human nature is certainly part of wisdom.  So where
do we draw the line?Perhaps the solution is to discount knowledge that at some point
has a sharp drop in utility.  For example, understanding French
will help you in a large number of situations, but its value drops
sharply as soon as no one else involved knows French.  Whereas the
value of understanding vanity would decline more gradually.The knowledge whose utility drops sharply is the kind that has
little relation to other knowledge.  This includes mere conventions,
like languages and safe combinations, and also what we'd call
"random" facts, like movie stars' birthdays, or how to distinguish
1956 from 1957 Studebakers.[4]
People seeking some single thing called "wisdom" have been
fooled by grammar.  Wisdom is just knowing the right thing to do,
and there are a hundred and one different qualities that help in
that.  Some, like selflessness, might come from meditating in an
empty room, and others, like a knowledge of human nature, might
come from going to drunken parties.Perhaps realizing this will help dispel the cloud of semi-sacred
mystery that surrounds wisdom in so many people's eyes.  The mystery
comes mostly from looking for something that doesn't exist.  And
the reason there have historically been so many different schools
of thought about how to achieve wisdom is that they've focused on
different components of it.When I use the word "wisdom" in this essay, I mean no more than
whatever collection of qualities helps people make the right choice
in a wide variety of situations.[5]
Even in English, our sense of the word "intelligence" is
surprisingly recent.  Predecessors like "understanding" seem to
have had a broader meaning.[6]
There is of course some uncertainty about how closely the remarks
attributed to Confucius and Socrates resemble their actual opinions.
I'm using these names as we use the name "Homer," to mean the
hypothetical people who said the things attributed to them.[7]
Analects VII:36, Fung trans.Some translators use "calm" instead of "happy."  One source of
difficulty here is that present-day English speakers have a different
idea of happiness from many older societies.  Every language probably
has a word meaning "how one feels when things are going well," but
different cultures react differently when things go well.  We react
like children, with smiles and laughter.  But in a more reserved
society, or in one where life was tougher, the reaction might be a
quiet contentment.[8]
It may have been Andrew Wiles, but I'm not sure.  If anyone
remembers such an interview, I'd appreciate hearing from you.[9]
Confucius claimed proudly that he had never invented
anything—that he had simply passed on an accurate account of
ancient traditions.  [Analects VII:1] It's hard for us now to
appreciate how important a duty it must have been in preliterate
societies to remember and pass on the group's accumulated knowledge.
Even in Confucius's time it still seems to have been the first duty
of the scholar.[10]
The bias toward wisdom in ancient philosophy may be exaggerated
by the fact that, in both Greece and China, many of the first
philosophers (including Confucius and Plato) saw themselves as
teachers of administrators, and so thought disproportionately about
such matters.  The few people who did invent things, like storytellers,
must have seemed an outlying data point that could be ignored.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston,
and Robert Morris for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




July 2004(This essay is derived from a talk at Oscon 2004.)
A few months ago I finished a new 
book, 
and in reviews I keep
noticing words like "provocative'' and "controversial.'' To say
nothing of "idiotic.''I didn't mean to make the book controversial.  I was trying to make
it efficient.  I didn't want to waste people's time telling them
things they already knew.  It's more efficient just to give them
the diffs.  But I suppose that's bound to yield an alarming book.EdisonsThere's no controversy about which idea is most controversial:
the suggestion that variation in wealth might not be as big a
problem as we think.I didn't say in the book that variation in wealth was in itself a
good thing.  I said in some situations it might be a sign of good
things.  A throbbing headache is not a good thing, but it can be
a sign of a good thing-- for example, that you're recovering
consciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.
(In a society of one, they're identical.) And that
is almost certainly a good thing: if your society has no variation
in productivity, it's probably not because everyone is Thomas
Edison.  It's probably because you have no Thomas Edisons.In a low-tech society you don't see much variation in productivity.
If you have a tribe of nomads collecting sticks for a fire, how
much more productive is the best stick gatherer going to be than
the worst?  A factor of two?  Whereas when you hand people a complex tool
like a computer, the variation in what they can do with
it is enormous.That's not a new idea.  Fred Brooks wrote about it in 1974, and
the study he quoted was published in 1968.  But I think he
underestimated the variation between programmers.  He wrote about productivity in lines
of code:  the best programmers can solve a given problem in a tenth
the time.  But what if the problem isn't given? In programming, as
in many fields, the hard part isn't solving problems, but deciding
what problems to solve.  Imagination is hard to measure, but
in practice it dominates the kind of productivity that's measured
in lines of code.Productivity varies in any field, but there are few in which it
varies so much.  The variation between programmers
is so great that it becomes a difference in kind.  I don't
think this is something intrinsic to programming, though.  In every field,
technology magnifies differences in productivity.  I think what's
happening in programming is just that we have a lot of technological
leverage.  But in every field the lever is getting longer, so the
variation we see is something that more and more fields will see
as time goes on.  And the success of companies, and countries, will
depend increasingly on how they deal with it.If variation in productivity increases with technology, then the
contribution of the most productive individuals will not only be
disproportionately large, but will actually grow with time.  When
you reach the point where 90% of a group's output is created by 1%
of its members, you lose big if something (whether Viking raids,
or central planning) drags their productivity down to the average.If we want to get the most out of them, we need to understand these
especially productive people.  What motivates them?  What do they
need to do their jobs?  How do you recognize them? How do you
get them to come and work for you?  And then of course there's the
question, how do you become one?More than MoneyI know a handful of super-hackers, so I sat down and thought about
what they have in common.  Their defining quality is probably that
they really love to program.  Ordinary programmers write code to pay
the bills.  Great hackers think of it as something they do for fun,
and which they're delighted to find people will pay them for.Great programmers are sometimes said to be indifferent to money.
This isn't quite true.  It is true that all they really care about
is doing interesting work.  But if you make enough money, you get
to work on whatever you want, and for that reason hackers are
attracted by the idea of making really large amounts of money.
But as long as they still have to show up for work every day, they
care more about what they do there than how much they get paid for
it.Economically, this is a fact of the greatest importance, because
it means you don't have to pay great hackers anything like what
they're worth.  A great programmer might be ten or a hundred times
as productive as an ordinary one, but he'll consider himself lucky
to get paid three times as much.  As I'll explain later, this is
partly because great hackers don't know how good they are.  But
it's also because money is not the main thing they want.What do hackers want?  Like all craftsmen, hackers like good tools.
In fact, that's an understatement.  Good hackers find it unbearable
to use bad tools.  They'll simply refuse to work on projects with
the wrong infrastructure.At a startup I once worked for, one of the things pinned up on our
bulletin board was an ad from IBM.  It was a picture of an AS400,
and the headline read, I think, "hackers despise
it.'' [1]When you decide what infrastructure to use for a project, you're
not just making a technical decision.  You're also making a social
decision, and this may be the more important of the two.  For
example, if your company wants to write some software, it might
seem a prudent choice to write it in Java.  But when you choose a
language, you're also choosing a community.  The programmers you'll
be able to hire to work on a Java project won't be as
smart as the
ones you could get to work on a project written in Python.
And the quality of your hackers probably matters more than the
language you choose.  Though, frankly, the fact that good hackers
prefer Python to Java should tell you something about the relative
merits of those languages.Business types prefer the most popular languages because they view
languages as standards. They don't want to bet the company on
Betamax.  The thing about languages, though, is that they're not
just standards.  If you have to move bits over a network, by all
means use TCP/IP.  But a programming language isn't just a format.
A programming language is a medium of expression.I've read that Java has just overtaken Cobol as the most popular
language.  As a standard, you couldn't wish for more.  But as a
medium of expression, you could do a lot better.  Of all the great
programmers I can think of, I know of only one who would voluntarily
program in Java.  And of all the great programmers I can think of
who don't work for Sun, on Java, I know of zero.Great hackers also generally insist on using open source software.
Not just because it's better, but because it gives them more control.
Good hackers insist on control.  This is part of what makes them
good hackers:  when something's broken, they need to fix it.  You
want them to feel this way about the software they're writing for
you.  You shouldn't be surprised when they feel the same way about
the operating system.A couple years ago a venture capitalist friend told me about a new
startup he was involved with.  It sounded promising.  But the next
time I talked to him, he said they'd decided to build their software
on Windows NT, and had just hired a very experienced NT developer
to be their chief technical officer.  When I heard this, I thought,
these guys are doomed.  One, the CTO couldn't be a first rate
hacker, because to become an eminent NT developer he would have
had to use NT voluntarily, multiple times, and I couldn't imagine
a great hacker doing that; and two, even if he was good, he'd have
a hard time hiring anyone good to work for him if the project had
to be built on NT. [2]The Final FrontierAfter software, the most important tool to a hacker is probably
his office.  Big companies think the function of office space is to express
rank.  But hackers use their offices for more than that: they
use their office as a place to think in.  And if you're a technology
company, their thoughts are your product.  So making hackers work
in a noisy, distracting environment is like having a paint factory
where the air is full of soot.The cartoon strip Dilbert has a lot to say about cubicles, and with
good reason.  All the hackers I know despise them.  The mere prospect
of being interrupted is enough to prevent hackers from working on
hard problems.  If you want to get real work done in an office with
cubicles, you have two options: work at home, or come in early or
late or on a weekend, when no one else is there.  Don't companies
realize this is a sign that something is broken?  An office
environment is supposed to be something that helps
you work, not something you work despite.Companies like Cisco are proud that everyone there has a cubicle,
even the CEO.  But they're not so advanced as they think; obviously
they still view office space as a badge of rank.  Note too that
Cisco is famous for doing very little product development in house.
They get new technology by buying the startups that created it-- where
presumably the hackers did have somewhere quiet to work.One big company that understands what hackers need is Microsoft.
I once saw a recruiting ad for Microsoft with a big picture of a
door.  Work for us, the premise was, and we'll give you a place to
work where you can actually get work done.   And you know, Microsoft
is remarkable among big companies in that they are able to develop
software in house.  Not well, perhaps, but well enough.If companies want hackers to be productive, they should look at
what they do at home.  At home, hackers can arrange things themselves
so they can get the most done.  And when they work at home, hackers
don't work in noisy, open spaces; they work in rooms with doors.  They
work in cosy, neighborhoody places with people around and somewhere
to walk when they need to mull something over, instead of in glass
boxes set in acres of parking lots.  They have a sofa they can take
a nap on when they feel tired, instead of sitting in a coma at
their desk, pretending to work.  There's no crew of people with
vacuum cleaners that roars through every evening during the prime
hacking hours.  There are no meetings or, God forbid, corporate
retreats or team-building exercises.  And when you look at what
they're doing on that computer, you'll find it reinforces what I
said earlier about tools.  They may have to use Java and Windows
at work, but at home, where they can choose for themselves, you're
more likely to find them using Perl and Linux.Indeed, these statistics about Cobol or Java being the most popular
language can be misleading.  What we ought to look at, if we want
to know what tools are best, is what hackers choose when they can
choose freely-- that is, in projects of their own.  When you ask
that question, you find that open source operating systems already
have a dominant market share, and the number one language is probably
Perl.InterestingAlong with good tools, hackers want interesting projects.  What
makes a project interesting?  Well, obviously overtly sexy
applications like stealth planes or special effects software would
be interesting to work on.  But any application can be interesting
if it poses novel technical challenges.  So it's hard to predict
which problems hackers will like, because some become
interesting only when the people working on them discover a new
kind of solution.  Before ITA
(who wrote the software inside Orbitz),
the people working on airline fare searches probably thought it
was one of the most boring applications imaginable.  But ITA made
it interesting by 
redefining the problem in a more ambitious way.I think the same thing happened at Google.  When Google was founded,
the conventional wisdom among the so-called portals was that search
was boring and unimportant.  But the guys at Google didn't think
search was boring, and that's why they do it so well.This is an area where managers can make a difference.  Like a parent
saying to a child, I bet you can't clean up your whole room in
ten minutes, a good manager can sometimes redefine a problem as a
more interesting one.  Steve Jobs seems to be particularly good at
this, in part simply by having high standards.  There were a lot
of small, inexpensive computers before the Mac.  He redefined the
problem as: make one that's beautiful.  And that probably drove
the developers harder than any carrot or stick could.They certainly delivered.  When the Mac first appeared, you didn't
even have to turn it on to know it would be good; you could tell
from the case.  A few weeks ago I was walking along the street in
Cambridge, and in someone's trash I saw what appeared to be a Mac
carrying case.  I looked inside, and there was a Mac SE.  I carried
it home and plugged it in, and it booted.  The happy Macintosh
face, and then the finder.  My God, it was so simple.  It was just
like... Google.Hackers like to work for people with high standards.  But it's not
enough just to be exacting.  You have to insist on the right things.
Which usually means that you have to be a hacker yourself.  I've
seen occasional articles about how to manage programmers.  Really
there should be two articles: one about what to do if
you are yourself a programmer, and one about what to do if you're not.  And the 
second could probably be condensed into two words:  give up.The problem is not so much the day to day management.  Really good
hackers are practically self-managing.  The problem is, if you're
not a hacker, you can't tell who the good hackers are.  A similar
problem explains why American cars are so ugly.  I call it the
design paradox.  You might think that you could make your products
beautiful just by hiring a great designer to design them.  But if
you yourself don't have good taste, 
how are you going to recognize
a good designer?  By definition you can't tell from his portfolio.
And you can't go by the awards he's won or the jobs he's had,
because in design, as in most fields, those tend to be driven by
fashion and schmoozing, with actual ability a distant third.
There's no way around it:  you can't manage a process intended to
produce beautiful things without knowing what beautiful is.  American
cars are ugly because American car companies are run by people with
bad taste.Many people in this country think of taste as something elusive,
or even frivolous.  It is neither.  To drive design, a manager must
be the most demanding user of a company's products.  And if you
have really good taste, you can, as Steve Jobs does, make satisfying
you the kind of problem that good people like to work on.Nasty Little ProblemsIt's pretty easy to say what kinds of problems are not interesting:
those where instead of solving a few big, clear, problems, you have
to solve a lot of nasty little ones.  One of the worst kinds of
projects is writing an interface to a piece of software that's
full of bugs.  Another is when you have to customize
something for an individual client's complex and ill-defined needs.
To hackers these kinds of projects are the death of a thousand
cuts.The distinguishing feature of nasty little problems is that you
don't learn anything from them.   Writing a compiler is interesting
because it teaches you what a compiler is.  But writing an interface
to a buggy piece of software doesn't teach you anything, because the
bugs are random.  [3] So it's not just fastidiousness that makes good
hackers avoid nasty little problems.  It's more a question of
self-preservation.  Working on nasty little problems makes you
stupid.  Good hackers avoid it for the same reason models avoid
cheeseburgers.Of course some problems inherently have this character.  And because
of supply and demand, they pay especially well.  So a company that
found a way to get great hackers to work on tedious problems would
be very successful.  How would you do it?One place this happens is in startups.  At our startup we had 
Robert Morris working as a system administrator.  That's like having the
Rolling Stones play at a bar mitzvah.  You can't hire that kind of
talent.  But people will do any amount of drudgery for companies
of which they're the founders.  [4]Bigger companies solve the problem by partitioning the company.
They get smart people to work for them by establishing a separate
R&D department where employees don't have to work directly on
customers' nasty little problems. [5] In this model, the research
department functions like a mine. They produce new ideas; maybe
the rest of the company will be able to use them.You may not have to go to this extreme.  
Bottom-up programming
suggests another way to partition the company: have the smart people
work as toolmakers.  If your company makes software to do x, have
one group that builds tools for writing software of that type, and
another that uses these tools to write the applications.  This way
you might be able to get smart people to write 99% of your code,
but still keep them almost as insulated from users as they would
be in a traditional research department.  The toolmakers would have
users, but they'd only be the company's own developers.  [6]If Microsoft used this approach, their software wouldn't be so full
of security holes, because the less smart people writing the actual
applications wouldn't be doing low-level stuff like allocating
memory.  Instead of writing Word directly in C, they'd be plugging
together big Lego blocks of Word-language.  (Duplo, I believe, is
the technical term.)ClumpingAlong with interesting problems, what good hackers like is other
good hackers.  Great hackers tend to clump together-- sometimes
spectacularly so, as at Xerox Parc.   So you won't attract good
hackers in linear proportion to how good an environment you create
for them.  The tendency to clump means it's more like the square
of the environment.  So it's winner take all.  At any given time,
there are only about ten or twenty places where hackers most want to
work, and if you aren't one of them, you won't just have fewer
great hackers, you'll have zero.Having great hackers is not, by itself, enough to make a company
successful.  It works well for Google and ITA, which are two of
the hot spots right now, but it didn't help Thinking Machines or
Xerox.  Sun had a good run for a while, but their business model
is a down elevator.  In that situation, even the best hackers can't
save you.I think, though, that all other things being equal, a company that
can attract great hackers will have a huge advantage.  There are
people who would disagree with this.  When we were making the rounds
of venture capital firms in the 1990s, several told us that software
companies didn't win by writing great software, but through brand,
and dominating channels, and doing the right deals.They really seemed to believe this, and I think I know why.  I
think what a lot of VCs are looking for, at least unconsciously,
is the next Microsoft.  And of course if Microsoft is your model,
you shouldn't be looking for companies that hope to win by writing
great software.  But VCs are mistaken to look for the next Microsoft,
because no startup can be the next Microsoft unless some other
company is prepared to bend over at just the right moment and be
the next IBM.It's a mistake to use Microsoft as a model, because their whole
culture derives from that one lucky break.  Microsoft is a bad data
point.  If you throw them out, you find that good products do tend
to win in the market.  What VCs should be looking for is the next
Apple, or the next Google.I think Bill Gates knows this.  What worries him about Google is
not the power of their brand, but the fact that they have
better hackers. [7]
RecognitionSo who are the great hackers?  How do you know when you meet one?
That turns out to be very hard.  Even hackers can't tell.  I'm
pretty sure now that my friend Trevor Blackwell is a great hacker.
You may have read on Slashdot how he made his 
own Segway.  The
remarkable thing about this project was that he wrote all the
software in one day (in Python, incidentally).For Trevor, that's
par for the course.  But when I first met him, I thought he was a
complete idiot.  He was standing in Robert Morris's office babbling
at him about something or other, and I remember standing behind
him making frantic gestures at Robert to shoo this nut out of his
office so we could go to lunch.  Robert says he misjudged Trevor
at first too.  Apparently when Robert first met him, Trevor had
just begun a new scheme that involved writing down everything about
every aspect of his life on a stack of index cards, which he carried
with him everywhere.  He'd also just arrived from Canada, and had
a strong Canadian accent and a mullet.The problem is compounded by the fact that hackers, despite their
reputation for social obliviousness, sometimes put a good deal of
effort into seeming smart.  When I was in grad school I used to
hang around the MIT AI Lab occasionally. It was kind of intimidating
at first.  Everyone there spoke so fast.  But after a while I
learned the trick of speaking fast.  You don't have to think any
faster; just use twice as many words to say everything.  With this amount of noise in the signal, it's hard to tell good
hackers when you meet them.  I can't tell, even now.  You also
can't tell from their resumes.  It seems like the only way to judge
a hacker is to work with him on something.And this is the reason that high-tech areas 
only happen around universities.  The active ingredient
here is not so much the professors as the students.  Startups grow up
around universities because universities bring together promising young
people and make them work on the same projects.  The
smart ones learn who the other smart ones are, and together
they cook up new projects of their own.Because you can't tell a great hacker except by working with him,
hackers themselves can't tell how good they are.  This is true to
a degree in most fields.  I've found that people who
are great at something are not so much convinced of their own
greatness as mystified at why everyone else seems so incompetent.
But it's particularly hard for hackers to know how good they are,
because it's hard to compare their work.  This is easier in most
other fields.  In the hundred meters, you know in 10 seconds who's
fastest.  Even in math there seems to be a general consensus about
which problems are hard to solve, and what constitutes a good
solution.  But hacking is like writing.<|begin_of_text|>
The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.
  Who can say which of two
novels is better?  Certainly not the authors.With hackers, at least, other hackers can tell.  That's because,
unlike novelists, hackers collaborate on projects.  When you get
to hit a few difficult problems over the net at someone, you learn
pretty quickly how hard they hit them back.  But hackers can't
watch themselves at work.  So if you ask a great hacker how good
he is, he's almost certain to reply, I don't know.  He's not just
being modest.  He really doesn't know.And none of us know, except about people we've actually worked
with.  Which puts us in a weird situation: we don't know who our
heroes should be.  The hackers who become famous tend to become
famous by random accidents of PR.  Occasionally I need to give an
example of a great hacker, and I never know who to use.  The first
names that come to mind always tend to be people I know personally,
but it seems lame to use them.  So, I think, maybe I should say
Richard Stallman, or Linus Torvalds, or Alan Kay, or someone famous
like that.  But I have no idea if these guys are great hackers.
I've never worked with them on anything.If there is a Michael Jordan of hacking, no one knows, including
him.CultivationFinally, the question the hackers have all been wondering about:
how do you become a great hacker?  I don't know if it's possible
to make yourself into one.  But it's certainly possible to do things
that make you stupid, and if you can make yourself stupid, you
can probably make yourself smart too.The key to being a good hacker may be to work on what you like.
When I think about the great hackers I know, one thing they have
in common is the extreme 
difficulty of making them work 
on anything they
don't want to.  I don't know if this is cause or effect; it may be
both.To do something well you have to love it.  
So to the extent you
can preserve hacking as something you love, you're likely to do it
well.  Try to keep the sense of wonder you had about programming at
age 14.  If you're worried that your current job is rotting your
brain, it probably is.The best hackers tend to be smart, of course, but that's true in
a lot of fields.  Is there some quality that's unique to hackers?
I asked some friends, and the number one thing they mentioned was
curiosity.  
I'd always supposed that all smart people were curious--
that curiosity was simply the first derivative of knowledge.  But
apparently hackers are particularly curious, especially about how
things work.  That makes sense, because programs are in effect
giant descriptions of how things work.Several friends mentioned hackers' ability to concentrate-- their
ability, as one put it, to "tune out everything outside their own
heads.''  I've certainly noticed this.  And I've heard several 
hackers say that after drinking even half a beer they can't program at
all.   So maybe hacking does require some special ability to focus.
Perhaps great hackers can load a large amount of context into their
head, so that when they look at a line of code, they see not just
that line but the whole program around it.  John McPhee
wrote that Bill Bradley's success as a basketball player was due
partly to his extraordinary peripheral vision.  "Perfect'' eyesight
means about 47 degrees of vertical peripheral vision.  Bill Bradley
had 70; he could see the basket when he was looking at the floor.
Maybe great hackers have some similar inborn ability.  (I cheat by
using a very dense language, 
which shrinks the court.)This could explain the disconnect over cubicles.  Maybe the people
in charge of facilities, not having any concentration to shatter,
have no idea that working in a cubicle feels to a hacker like having
one's brain in a blender.  (Whereas Bill, if the rumors of autism
are true, knows all too well.)One difference I've noticed between great hackers and smart people
in general is that hackers are more 
politically incorrect.  To the
extent there is a secret handshake among good hackers, it's when they
know one another well enough to express opinions that would get
them stoned to death by the general public.  And I can see why
political incorrectness would be a useful quality in programming.
Programs are very complex and, at least in the hands of good
programmers, very fluid.  In such situations it's helpful to have
a habit of questioning assumptions.Can you cultivate these qualities?  I don't know.  But you can at
least not repress them.  So here is my best shot at a recipe.  If
it is possible to make yourself into a great hacker, the way to do
it may be to make the following deal with yourself: you never have
to work on boring projects (unless your family will starve otherwise),
and in return, you'll never allow yourself to do a half-assed job.
All the great hackers I know seem to have made that deal, though
perhaps none of them had any choice in the matter.Notes
[1] In fairness, I have to say that IBM makes decent hardware.  I
wrote this on an IBM laptop.[2] They did turn out to be doomed.  They shut down a few months
later.[3] I think this is what people mean when they talk
about the "meaning of life."  On the face of it, this seems an 
odd idea.  Life isn't an expression; how could it have meaning?
But it can have a quality that feels a lot like meaning.  In a project
like a compiler, you have to solve a lot of problems, but the problems
all fall into a pattern, as in a signal.  Whereas when the problems
you have to solve are random, they seem like noise.
[4] Einstein at one point worked designing refrigerators. (He had equity.)[5] It's hard to say exactly what constitutes research in the
computer world, but as a first approximation, it's software that
doesn't have users.I don't think it's publication that makes the best hackers want to work
in research departments.  I think it's mainly not having to have a
three hour meeting with a product manager about problems integrating
the Korean version of Word 13.27 with the talking paperclip.[6] Something similar has been happening for a long time in the
construction industry. When you had a house built a couple hundred
years ago, the local builders built everything in it.  But increasingly
what builders do is assemble components designed and manufactured
by someone else.  This has, like the arrival of desktop publishing,
given people the freedom to experiment in disastrous ways, but it
is certainly more efficient.[7] Google is much more dangerous to Microsoft than Netscape was.
Probably more dangerous than any other company has ever been.  Not
least because they're determined to fight.  On their job listing
page, they say that one of their "core values'' is "Don't be evil.''
From a company selling soybean oil or mining equipment, such a
statement would merely be eccentric.  But I think all of us in the
computer world recognize who that is a declaration of war on.Thanks to Jessica Livingston, Robert Morris, and Sarah Harlin
for reading earlier versions of this talk.November 2021(This essay is derived from a talk at the Cambridge Union.)When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kind. The strongest proof
would be to show that taste exists in the narrowest sense, so I'm
going to talk about taste in art. You have better taste than me if
the art you like is better than the art I like.If there's no such thing as good taste, then there's no such thing
as good art. Because if there is such a
thing as good art, it's
easy to tell which of two people has better taste. Show them a lot
of works by artists they've never seen before and ask them to
choose the best, and whoever chooses the better art has better
taste.So if you want to discard the concept of good taste, you also have
to discard the concept of good art. And that means you have to
discard the possibility of people being good at making it. Which
means there's no way for artists to be good at their jobs. And not
just visual artists, but anyone who is in any sense an artist. You
can't have good actors, or novelists, or composers, or dancers
either. You can have popular novelists, but not good ones.We don't realize how far we'd have to go if we discarded the concept
of good taste, because we don't even debate the most obvious cases.
But it doesn't just mean we can't say which of two famous painters
is better. It means we can't say that any painter is better than a
randomly chosen eight year old.That was how I realized my father was wrong. I started studying
painting. And it was just like other kinds of work I'd done: you
could do it well, or badly, and if you tried hard, you could get
better at it. And it was obvious that Leonardo and Bellini were
much better at it than me. That gap between us was not imaginary.
They were so good. And if they could be good, then art could be
good, and there was such a thing as good taste after all.Now that I've explained how to show there is such a thing as good
taste, I should also explain why people think there isn't. There
are two reasons. One is that there's always so much disagreement
about taste. Most people's response to art is a tangle of unexamined
impulses. Is the artist famous? Is the subject attractive? Is this
the sort of art they're supposed to like? Is it hanging in a famous
museum, or reproduced in a big, expensive book? In practice most
people's response to art is dominated by such extraneous factors.And the people who do claim to have good taste are so often mistaken.
The paintings admired by the so-called experts in one generation
are often so different from those admired a few generations later.
It's easy to conclude there's nothing real there at all. It's only
when you isolate this force, for example by trying to paint and
comparing your work to Bellini's, that you can see that it does in
fact exist.The other reason people doubt that art can be good is that there
doesn't seem to be any room in the art for this goodness. The
argument goes like this. Imagine several people looking at a work
of art and judging how good it is. If being good art really is a
property of objects, it should be in the object somehow. But it
doesn't seem to be; it seems to be something happening in the heads
of each of the observers. And if they disagree, how do you choose
between them?The solution to this puzzle is to realize that the purpose of art
is to work on its human audience, and humans have a lot in common.
And to the extent the things an object acts upon respond in the
same way, that's arguably what it means for the object to have the
corresponding property. If everything a particle interacts with
behaves as if the particle had a mass of m, then it has a mass of
m. So the distinction between "objective" and "subjective" is not
binary, but a matter of degree, depending on how much the subjects
have in common. Particles interacting with one another are at one
pole, but people interacting with art are not all the way at the
other; their reactions aren't random.Because people's responses to art aren't random, art can be designed
to operate on people, and be good or bad depending on how effectively
it does so. Much as a vaccine can be. If someone were talking about
the ability of a vaccine to confer immunity, it would seem very
frivolous to object that conferring immunity wasn't really a property
of vaccines, because acquiring immunity is something that happens
in the immune system of each individual person. Sure, people's
immune systems vary, and a vaccine that worked on one might not
work on another, but that doesn't make it meaningless to talk about
the effectiveness of a vaccine.The situation with art is messier, of course. You can't measure
effectiveness by simply taking a vote, as you do with vaccines.
You have to imagine the responses of subjects with a deep knowledge
of art, and enough clarity of mind to be able to ignore extraneous
influences like the fame of the artist. And even then you'd still
see some disagreement. People do vary, and judging art is hard,
especially recent art. There is definitely not a total order either
of works or of people's ability to judge them. But there is equally
definitely a partial order of both. So while it's not possible to
have perfect taste, it is possible to have good taste.
Thanks to the Cambridge Union for inviting me, and to Trevor
Blackwell, Jessica Livingston, and Robert Morris for reading drafts
of this.
May 2003If Lisp is so great, why don't more people use it?  I was    
asked this question by a student in the audience at a 
talk I gave recently.  Not for the first time, either.In languages, as in so many things, there's not much     
correlation between popularity and quality.  Why does   
John Grisham (King of Torts sales rank, 44) outsell
Jane Austen (Pride and Prejudice sales rank, 6191)?
Would even Grisham claim that it's because he's a better
writer?Here's the first sentence of Pride and Prejudice:

It is a truth universally acknowledged, that a single man 
in possession of a good fortune must be in want of a
wife.

"It is a truth universally acknowledged?"  Long words for
the first sentence of a love story.Like Jane Austen, Lisp looks hard.  Its syntax, or lack
of syntax, makes it look completely unlike 
the languages
most people are used to.  Before I learned Lisp, I was afraid
of it too.  I recently came across a notebook from 1983
in which I'd written:

I suppose I should learn Lisp, but it seems so foreign.

Fortunately, I was 19 at the time and not too resistant to learning
new things.  I was so ignorant that learning
almost anything meant learning new things.People frightened by Lisp make up other reasons for not
using it.  The standard
excuse, back when C was the default language, was that Lisp
was too slow.  Now that Lisp dialects are among
the faster
languages available, that excuse has gone away.
Now the standard excuse is openly circular: that other languages
are more popular.(Beware of such reasoning.  It gets you Windows.)Popularity is always self-perpetuating, but it's especially
so in programming languages. More libraries
get written for popular languages, which makes them still
more popular.  Programs often have to work with existing programs,
and this is easier if they're written in the same language,
so languages spread from program to program like a virus.
And managers prefer popular languages, because they give them 
more leverage over developers, who can more easily be replaced.Indeed, if programming languages were all more or less equivalent,
there would be little justification for using any but the most
popular.  But they aren't all equivalent, not by a long
shot.  And that's why less popular languages, like Jane Austen's 
novels, continue to survive at all.  When everyone else is reading 
the latest John Grisham novel, there will always be a few people 
reading Jane Austen instead.

Want to start a startup?  Get funded by
Y Combinator.




October 2010After barely changing at all for decades, the startup funding
business is now in what could, at least by comparison, be called
turmoil.  At Y Combinator we've seen dramatic changes in the funding
environment for startups.  Fortunately one of them is much higher
valuations.The trends we've been seeing are probably not YC-specific.  I wish
I could say they were, but the main cause is probably just that we
see trends first—partly because the startups we fund are very
plugged into the Valley and are quick to take advantage of anything
new, and partly because we fund so many that we have enough data
points to see patterns clearly.What we're seeing now, everyone's probably going to be seeing in
the next couple years.  So I'm going to explain what we're seeing,
and what that will mean for you if you try to raise money.Super-AngelsLet me start by describing what the world of startup funding used
to look like.  There used to be two sharply differentiated types
of investors: angels and venture capitalists.  Angels are individual
rich people who invest small amounts of their own money, while VCs
are employees of funds that invest large amounts of other people's.For decades there were just those two types of investors, but now
a third type has appeared halfway between them: the so-called
super-angels. 
[1]
  And VCs have been provoked by their arrival
into making a lot of angel-style investments themselves.  So the
previously sharp line between angels and VCs has become hopelessly
blurred.There used to be a no man's land between angels and VCs.  Angels
would invest $20k to $50k apiece, and VCs usually a million or more.
So an angel round meant a collection of angel investments that
combined to maybe $200k, and a VC round meant a series A round in
which a single VC fund (or occasionally two) invested $1-5 million.The no man's land between angels and VCs was a very inconvenient
one for startups, because it coincided with the amount many wanted
to raise.  Most startups coming out of Demo Day wanted to raise
around $400k.  But it was a pain to stitch together that much out
of angel investments, and most VCs weren't interested in investments
so small.  That's the fundamental reason the super-angels have
appeared.  They're responding to the market.The arrival of a new type of investor is big news for startups,
because there used to be only two and they rarely competed with one
another.  Super-angels compete with both angels and VCs.  That's
going to change the rules about how to raise money.  I don't know
yet what the new rules will be, but it looks like most of the changes
will be for the better.A super-angel has some of the qualities of an angel, and some of
the qualities of a VC.  They're usually individuals, like angels.
In fact many of the current super-angels were initially angels of
the classic type.  But like VCs, they invest other people's money.
This allows them to invest larger amounts than angels:  a typical
super-angel investment is currently about $100k.  They make investment
decisions quickly, like angels.  And they make a lot more investments
per partner than VCs—up to 10 times as many.The fact that super-angels invest other people's money makes them
doubly alarming to VCs. They don't just compete for startups; they
also compete for investors.  What super-angels really are is a new
form of fast-moving, lightweight VC fund.   And those of us in the
technology world know what usually happens when something comes
along that can be described in terms like that.  Usually it's the
replacement.Will it be?  As of now, few of the startups that take money from
super-angels are ruling out taking VC money.  They're just postponing
it.  But that's still a problem for VCs.  Some of the startups that
postpone raising VC money may do so well on the angel money they
raise that they never bother to raise more.  And those who do raise
VC rounds will be able to get higher valuations when they do.  If
the best startups get 10x higher valuations when they raise series
A rounds, that would cut VCs' returns from winners at least tenfold.
[2]So I think VC funds are seriously threatened by the super-angels.
But one thing that may save them to some extent is the uneven
distribution of startup outcomes: practically all the returns are
concentrated in a few big successes.  The expected value of a startup
is the percentage chance it's Google.  So to the extent that winning
is a matter of absolute returns, the super-angels could win practically
all the battles for individual startups and yet lose the war, if
they merely failed to get those few big winners.  And there's a
chance that could happen, because the top VC funds have better
brands, and can also do more for their portfolio companies.  
[3]Because super-angels make more investments per partner, they have
less partner per investment.  They can't pay as much attention to
you as a VC on your board could.  How much is that extra attention
worth?  It will vary enormously from one partner to another.  There's
no consensus yet in the general case.  So for now this is something
startups are deciding individually.Till now, VCs' claims about how much value they added were sort of
like the government's.  Maybe they made you feel better, but you
had no choice in the matter, if you needed money on the scale only
VCs could supply.  Now that VCs have competitors, that's going to
put a market price on the help they offer.  The interesting thing
is, no one knows yet what it will be.Do startups that want to get really big need the sort of advice and
connections only the top VCs can supply?  Or would super-angel money
do just as well?  The VCs will say you need them, and the super-angels
will say you don't.  But the truth is, no one knows yet, not even
the VCs and super-angels themselves.   All the super-angels know
is that their new model seems promising enough to be worth trying,
and all the VCs know is that it seems promising enough to worry
about.RoundsWhatever the outcome, the conflict between VCs and super-angels is
good news for founders.  And not just for the obvious reason that
more competition for deals means better terms.  The whole shape of
deals is changing.One of the biggest differences between angels and VCs is the amount
of your company they want.  VCs want a lot.  In a series A round
they want a third of your company, if they can get it.  They don't
care much how much they pay for it, but they want a lot because the
number of series A investments they can do is so small.  In a
traditional series A investment, at least one partner from the VC
fund takes a seat on your board.  
[4]
 Since board seats last about
5 years and each partner can't handle more than about 10 at once,
that means a VC fund can only do about 2 series A deals per partner
per year. And that means they need to get as much of the company
as they can in each one.  You'd have to be a very promising startup
indeed to get a VC to use up one of his 10 board seats for only a
few percent of you.Since angels generally don't take board seats, they don't have this
constraint.  They're happy to buy only a few percent of you.  And
although the super-angels are in most respects mini VC funds, they've
retained this critical property of angels.  They don't take board
seats, so they don't need a big percentage of your company.Though that means you'll get correspondingly less attention from
them, it's good news in other respects.  Founders never really liked
giving up as much equity as VCs wanted.  It was a lot of the company
to give up in one shot.  Most founders doing series A deals would
prefer to take half as much money for half as much stock, and then
see what valuation they could get for the second half of the stock
after using the first half of the money to increase its value.  But
VCs never offered that option.Now startups have another alternative.  Now it's easy to raise angel
rounds about half the size of series A rounds.  Many of the startups
we fund are taking this route, and I predict that will be true of
startups in general.A typical big angel round might be $600k on a convertible note with
a valuation cap of $4 million premoney.  Meaning that when the note
converts into stock (in a later round, or upon acquisition), the
investors in that round will get.6 / 4.6, or 13% of the company.
That's a lot less than the 30 to 40% of the company you usually
give up in a series A round if you do it so early.  
[5]But the advantage of these medium-sized rounds is not just that
they cause less dilution.  You also lose less control.  After an
angel round, the founders almost always still have control of the
company, whereas after a series A round they often don't.  The
traditional board structure after a series A round is two founders,
two VCs, and a (supposedly) neutral fifth person.  Plus series A
terms usually give the investors a veto over various kinds of
important decisions, including selling the company.  Founders usually
have a lot of de facto control after a series A, as long as things
are going well.  But that's not the same as just being able to do
what you want, like you could before.A third and quite significant advantage of angel rounds is that
they're less stressful to raise.  Raising a traditional series A
round has in the past taken weeks, if not months.  When a VC firm
can only do 2 deals per partner per year, they're careful about
which they do.  To get a traditional series A round you have to go
through a series of meetings, culminating in a full partner meeting
where the firm as a whole says yes or no.  That's the really scary
part for founders: not just that series A rounds take so long, but
at the end of this long process the VCs might still say no.  The
chance of getting rejected after the full partner meeting averages
about 25%.  At some firms it's over 50%.Fortunately for founders, VCs have been getting a lot faster.
Nowadays Valley VCs are more likely to take 2 weeks than 2 months.
But they're still not as fast as angels and super-angels, the most
decisive of whom sometimes decide in hours.Raising an angel round is not only quicker, but you get feedback
as it progresses.  An angel round is not an all or nothing thing
like a series A.  It's composed of multiple investors with varying
degrees of seriousness, ranging from the upstanding ones who commit
unequivocally to the jerks who give you lines like "come back to
me to fill out the round." You usually start collecting money from
the most committed investors and work your way out toward the
ambivalent ones, whose interest increases as the round fills up.But at each point you know how you're doing.  If investors turn
cold you may have to raise less, but when investors in an angel
round turn cold the process at least degrades gracefully, instead
of blowing up in your face and leaving you with nothing, as happens
if you get rejected by a VC fund after a full partner meeting.
Whereas if investors seem hot, you can not only close the round
faster, but now that convertible notes are becoming the norm,
actually raise the price to reflect demand.ValuationHowever, the VCs have a weapon they can use against the super-angels,
and they have started to use it.   VCs have started making angel-sized
investments too.  The term "angel round" doesn't mean that all the
investors in it are angels; it just describes the structure of the
round.  Increasingly the participants include VCs making investments
of a hundred thousand or two.  And when VCs invest in angel rounds
they can do things that super-angels don't like.  VCs are quite
valuation-insensitive in angel rounds—partly because they are
in general, and partly because they don't care that much about the
returns on angel rounds, which they still view mostly as a way to
recruit startups for series A rounds later.  So VCs who invest in
angel rounds can blow up the valuations for angels and super-angels
who invest in them. 
[6]Some super-angels seem to care about valuations.  Several turned
down YC-funded startups after Demo Day because their valuations
were too high.  This was not a problem for the startups; by definition
a high valuation means enough investors were willing to accept it.
But it was mysterious to me that the super-angels would quibble
about valuations.  Did they not understand that the big returns
come from a few big successes, and that it therefore mattered far
more which startups you picked than how much you paid for them?After thinking about it for a while and observing certain other
signs, I have a theory that explains why the super-angels may be
smarter than they seem.  It would make sense for super-angels to
want low valuations if they're hoping to invest in startups that
get bought early.  If you're hoping to hit the next Google, you
shouldn't care if the valuation is 20 million.  But if you're looking
for companies that are going to get bought for 30 million, you care.
If you invest at 20 and the company gets bought for 30, you only
get 1.5x.  You might as well buy Apple.So if some of the super-angels were looking for companies that could
get acquired quickly, that would explain why they'd care about
valuations.  But why would they be looking for those?   Because
depending on the meaning of "quickly," it could actually be very
profitable.  A company that gets acquired for 30 million is a failure
to a VC, but it could be a 10x return for an angel, and moreover,
a quick 10x return.  Rate of return is what matters in
investing—not the multiple you get, but the multiple per year.
If a super-angel gets 10x in one year, that's a higher rate of
return than a VC could ever hope to get from a company that took 6
years to go public.  To get the same rate of return, the VC would
have to get a multiple of 10^6—one million x.  Even Google
didn't come close to that.So I think at least some super-angels are looking for companies
that will get bought.  That's the only rational explanation for
focusing on getting the right valuations, instead of the right
companies.  And if so they'll be different to deal with than VCs.
They'll be tougher on valuations, but more accommodating if you want
to sell early.PrognosisWho will win, the super-angels or the VCs?  I think the answer to
that is, some of each.  They'll each become more like one another.
The super-angels will start to invest larger amounts, and the VCs
will gradually figure out ways to make more, smaller investments
faster.  A decade from now the players will be hard to tell apart,
and there will probably be survivors from each group.What does that mean for founders?  One thing it means is that the
high valuations startups are presently getting may not last forever.
To the extent that valuations are being driven up by price-insensitive
VCs, they'll fall again if VCs become more like super-angels and
start to become more miserly about valuations.  Fortunately if this
does happen it will take years.The short term forecast is more competition between investors, which
is good news for you.  The super-angels will try to undermine the
VCs by acting faster, and the VCs will try to undermine the
super-angels by driving up valuations.  Which for founders will
result in the perfect combination: funding rounds that close fast,
with high valuations.But remember that to get that combination, your startup will have
to appeal to both super-angels and VCs.  If you don't seem like you
have the potential to go public, you won't be able to use VCs to
drive up the valuation of an angel round.There is a danger of having VCs in an angel round: the so-called
signalling risk.  If VCs are only doing it in the hope of investing
more later, what happens if they don't?  That's a signal to everyone
else that they think you're lame.How much should you worry about that?  The seriousness of signalling
risk depends on how far along you are.  If by the next time you
need to raise money, you have graphs showing rising revenue or
traffic month after month, you don't have to worry about any signals
your existing investors are sending.  Your results will speak for
themselves.  
[7]Whereas if the next time you need to raise money you won't yet have
concrete results, you may need to think more about the message your
investors might send if they don't invest more.  I'm not sure yet
how much you have to worry, because this whole phenomenon of VCs
doing angel investments is so new. But my instincts tell me you
don't have to worry much.  Signalling risk smells like one of those
things founders worry about that's not a real problem.  As a rule,
the only thing that can kill a good startup is the startup itself.
Startups hurt themselves way more often than competitors hurt them,
for example.  I suspect signalling risk is in this category too.One thing YC-funded startups have been doing to mitigate the risk
of taking money from VCs in angel rounds is not to take too much
from any one VC.  Maybe that will help, if you have the luxury of
turning down money.Fortunately, more and more startups will.  After decades of competition
that could best be described as intramural, the startup funding
business is finally getting some real competition.  That should
last several years at least, and maybe a lot longer. Unless there's
some huge market crash, the next couple years are going to be a
good time for startups to raise money.  And that's exciting because
it means lots more startups will happen.
Notes[1]
I've also heard them called "Mini-VCs" and "Micro-VCs." I
don't know which name will stick.There were a couple predecessors.  Ron Conway had angel funds
starting in the 1990s, and in some ways First Round Capital is closer to a
super-angel than a VC fund.[2]
It wouldn't cut their overall returns tenfold, because investing
later would probably (a) cause them to lose less on investments
that failed, and (b) not allow them to get as large a percentage
of startups as they do now.  So it's hard to predict precisely what
would happen to their returns.[3]
The brand of an investor derives mostly from the success of
their portfolio companies.  The top VCs thus have a big brand
advantage over the super-angels.  They could make it self-perpetuating
if they used it to get all the best new startups.  But I don't think
they'll be able to.  To get all the best startups, you have to do
more than make them want you.  You also have to want them; you have
to recognize them when you see them, and that's much harder.
Super-angels will snap up stars that VCs miss.  And that will cause
the brand gap between the top VCs and the super-angels gradually
to erode.[4]
Though in a traditional series A round VCs put two partners
on your board, there are signs now that VCs may begin to conserve
board seats by switching to what used to be considered an angel-round
board, consisting of two founders and one VC.  Which is also to the
founders' advantage if it means they still control the company.[5]
In a series A round, you usually have to give up more than
the actual amount of stock the VCs buy, because they insist you
dilute yourselves to set aside an "option pool" as well.  I predict
this practice will gradually disappear though.[6]
The best thing for founders, if they can get it, is a convertible
note with no valuation cap at all.  In that case the money invested
in the angel round just converts into stock at the valuation of the
next round, no matter how large.  Angels and super-angels tend not
to like uncapped notes. They have no idea how much of the company
they're buying.  If the company does well and the valuation of the
next round is high, they may end up with only a sliver of it.  So
by agreeing to uncapped notes, VCs who don't care about valuations
in angel rounds can make offers that super-angels hate to match.[7]
Obviously signalling risk is also not a problem if you'll
never need to raise more money.  But startups are often mistaken
about that.Thanks to Sam Altman, John Bautista, Patrick Collison, James
Lindenbaum, Reid Hoffman, Jessica Livingston and Harj Taggar
for reading drafts
of this.September 2007In high school I decided I was going to study philosophy in college.
I had several motives, some more honorable than others.  One of the
less honorable was to shock people.  College was regarded as job
training where I grew up, so studying philosophy seemed an impressively
impractical thing to do.  Sort of like slashing holes in your clothes
or putting a safety pin through your ear, which were other forms
of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying
philosophy would be a shortcut straight to wisdom.  All the people
majoring in other things would just end up with a bunch of domain
knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you
wouldn't find those in our high school library.  But I tried to
read Plato and Aristotle.  I doubt I believed I understood them,
but they sounded like they were talking about something important.
I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned
a lot in the calculus class, but I didn't learn much in Philosophy
101.  And yet my plan to study philosophy remained intact.  It was
my fault I hadn't learned anything.  I hadn't read the books we
were assigned carefully enough.  I'd give Berkeley's Principles
of Human Knowledge another shot in college.  Anything so admired
and so difficult to read must have something in it, if one could
only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have
a nice edition of his collected works.  Will I ever read it?  Seems
unlikely.The difference between then and now is that now I understand why
Berkeley is probably not worth trying to understand.  I think I see
now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It
didn't work out as I'd hoped.  I didn't learn any magical truths
compared to which everything else was mere domain knowledge.  But
I do at least know now why I didn't.  Philosophy doesn't really
have a subject matter in the way math or history or most other
university subjects do.  There is no core of knowledge one must
master.  The closest you come to that is a knowledge of what various
individual philosophers have said about different topics over the
years.  Few were sufficiently correct that people have forgotten
who discovered what they discovered.Formal logic has some subject matter. I took several classes in
logic.  I don't know if I learned anything from them.
[1]
It does seem to me very important to be able to flip ideas around in
one's head: to see when two ideas don't fully cover the space of
possibilities, or when one idea is the same as another but with a
couple things changed.  But did studying logic teach me the importance
of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The
most dramatic I learned immediately, in the first semester of
freshman year, in a class taught by Sydney Shoemaker.  I learned
that I don't exist.  I am (and you are) a collection of cells that
lurches around driven by various forces, and calls itself I.  But
there's no central, indivisible thing that your identity goes with.
You could conceivably lose half your brain and live.  Which means
your brain could conceivably be split into two halves and each
transplanted into different bodies.  Imagine waking up after such
an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life
are fuzzy, and break down if pushed too hard.  Even a concept as
dear to us as I.  It took me a while to grasp this, but when I
did it was fairly sudden, like someone in the nineteenth century
grasping evolution and realizing the story of creation they'd been
told as a child was all wrong. 
[2]
Outside of math there's a limit
to how far you can push words; in fact, it would not be a bad
definition of math to call it the study of terms that have precise
meanings.  Everyday words are inherently imprecise.  They work well
enough in everyday life that you don't notice.  Words seem to work,
just as Newtonian physics seems to.  But you can always make them
break if you push them far enough.I would say that this has been, unfortunately for philosophy, the
central fact of philosophy.  Most philosophical debates are not
merely afflicted by but driven by confusions over words.  Do we
have free will?  Depends what you mean by "free." Do abstract ideas
exist?  Depends what you mean by "exist."Wittgenstein is popularly credited with the idea that most philosophical
controversies are due to confusions over language.  I'm not sure
how much credit to give him.  I suspect a lot of people realized
this, but reacted simply by not studying philosophy, rather than
becoming philosophy professors.How did things get this way?  Can something people have spent
thousands of years studying really be a waste of time?  Those are
interesting questions.  In fact, some of the most interesting
questions you can ask about philosophy.  The most valuable way to
approach the current philosophical tradition may be neither to get
lost in pointless speculations like Berkeley, nor to shut them down
like Wittgenstein, but to study it as an example of reason gone
wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.
What we know of their predecessors comes from fragments and references
in later works; their doctrines could be described as speculative
cosmology that occasionally strays into analysis.  Presumably they
were driven by whatever makes people in every other society invent
cosmologies.
[3]With Socrates, Plato, and particularly Aristotle, this tradition
turned a corner.  There started to be a lot more analysis.  I suspect
Plato and Aristotle were encouraged in this by progress in math.
Mathematicians had by then shown that you could figure things out
in a much more conclusive way than by making up fine sounding stories
about them.  
[4]People talk so much about abstractions now that we don't realize
what a leap it must have been when they first started to.  It was
presumably many thousands of years between when people first started
describing things as hot or cold and when someone asked "what is
heat?"  No doubt it was a very gradual process.  We don't know if
Plato or Aristotle were the first to ask any of the questions they
did.  But their works are the oldest we have that do this on a large
scale, and there is a freshness (not to say naivete) about them
that suggests some of the questions they asked were new to them,
at least.Aristotle in particular reminds me of the phenomenon that happens
when people discover something new, and are so excited by it that
they race through a huge percentage of the newly discovered territory
in one lifetime.  If so, that's evidence of how new this kind of
thinking was. 
[5]This is all to explain how Plato and Aristotle can be very impressive
and yet naive and mistaken.  It was impressive even to ask the
questions they did.  That doesn't mean they always came up with
good answers.  It's not considered insulting to say that ancient
Greek mathematicians were naive in some respects, or at least lacked
some concepts that would have made their lives easier.  So I hope
people will not be too offended if I propose that ancient philosophers
were similarly naive.  In particular, they don't seem to have fully
grasped what I earlier called the central fact of philosophy: that
words break if you push them too far."Much to the surprise of the builders of the first digital computers,"
Rod Brooks wrote, "programs written for them usually did not work."
[6]
Something similar happened when people first started trying
to talk about abstractions.  Much to their surprise, they didn't
arrive at answers they agreed upon.  In fact, they rarely seemed
to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at
too low a resolution.The proof of how useless some of their answers turned out to be is
how little effect they have.  No one after reading Aristotle's
Metaphysics does anything differently as a result.
[7]Surely I'm not claiming that ideas have to have practical applications
to be interesting?  No, they may not have to.  Hardy's boast that
number theory had no use whatsoever wouldn't disqualify it.  But
he turned out to be mistaken.  In fact, it's suspiciously hard to
find a field of math that truly has no practical use.  And Aristotle's
explanation of the ultimate goal of philosophy in Book A of the
Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.
The examples he gives are convincing: an ordinary worker builds
things a certain way out of habit; a master craftsman can do more
because he grasps the underlying principles.  The trend is clear:
the more general the knowledge, the more admirable it is.  But then
he makes a mistake—possibly the most important mistake in the
history of philosophy.  He has noticed that theoretical knowledge
is often acquired for its own sake, out of curiosity, rather than
for any practical need.  So he proposes there are two kinds of
theoretical knowledge: some that's useful in practical matters and
some that isn't.  Since people interested in the latter are interested
in it for its own sake, it must be more noble.  So he sets as his
goal in the Metaphysics the exploration of knowledge that has no
practical use.  Which means no alarms go off when he takes on grand
but vaguely understood questions and ends up getting lost in a sea
of words.His mistake was to confuse motive and result.  Certainly, people
who want a deep understanding of something are often driven by
curiosity rather than any practical need.  But that doesn't mean
what they end up learning is useless.  It's very valuable in practice
to have a deep understanding of what you're doing; even if you're
never called on to solve advanced problems, you can see shortcuts
in the solution of simple ones, and your knowledge won't break down
in edge cases, as it would if you were relying on formulas you
didn't understand.  Knowledge is power.  That's what makes theoretical
knowledge prestigious.  It's also what causes smart people to be
curious about certain things and not others; our DNA is not so
disinterested as we might think.So while ideas don't have to have immediate practical applications
to be interesting, the kinds of things we find interesting will
surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was
partly that he set off with contradictory aims: to explore the most
abstract ideas, guided by the assumption that they were useless.
He was like an explorer looking for a territory to the north of
him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future
explorers, he sent them off in the wrong direction as well. 
[8]
Perhaps worst of all, he protected them from both the criticism of
outsiders and the promptings of their own inner compass by establishing
the principle that the most noble sort of theoretical knowledge had
to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from
it turned out to be worth keeping; the bulk of it has had no effect
at all.  The Metaphysics is among the least read of all famous
books.  It's not hard to understand the way Newton's Principia
is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately
that was not the conclusion Aristotle's successors derived from
works like the Metaphysics. 
[9]
Soon after, the western world
fell on intellectual hard times.  Instead of version 1s to be
superseded, the works of Plato and Aristotle became revered texts
to be mastered and discussed.  And so things remained for a shockingly
long time.  It was not till around 1600 (in Europe, where the center
of gravity had shifted by then) that one found people confident
enough to treat Aristotle's work as a catalog of mistakes.  And
even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little
progress there was in math between Hellenistic times and the
Renaissance.In the intervening years an unfortunate idea took hold:  that it
was not only acceptable to produce works like the Metaphysics,
but that it was a particularly prestigious line of work, done by a
class of people called philosophers.  No one thought to go back and
debug Aristotle's motivating argument.  And so instead of correcting
the problem Aristotle discovered by falling into it—that you can
easily get lost if you talk too loosely about very abstract ideas—they 
continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract
new readers.  Traditional philosophy occupies a kind of singularity
in this respect.  If you write in an unclear way about big ideas,
you produce something that seems tantalizingly attractive to
inexperienced but intellectually ambitious students.  Till one knows
better, it's hard to distinguish something that's hard to understand
because the writer was unclear in his own mind from something like
a mathematical proof that's hard to understand because the ideas
it represents are hard to understand.  To someone who hasn't learned
the difference, traditional philosophy seems extremely attractive:
as hard (and therefore impressive) as math, yet broader in scope.
That was what lured me in as a high school student.This singularity is even more singular in having its own defense
built in.  When things are hard to understand, people who suspect
they're nonsense generally keep quiet.  There's no way to prove a
text is meaningless.  The closest you can get is to show that the
official judges of some class of texts can't distinguish them from
placebos. 
[10]And so instead of denouncing philosophy, most people who suspected
it was a waste of time just studied other things.  That alone is
fairly damning evidence, considering philosophy's claims.  It's
supposed to be about the ultimate truths. Surely all smart people
would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might
have corrected them, they tended to be self-perpetuating.  Bertrand
Russell wrote in a letter in 1912:

  Hitherto the people attracted to philosophy have been mostly those
  who loved the big generalizations, which were all wrong, so that
  few people with exact minds have taken up the subject.
[11]

His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery
that most previous philosophy was a waste of time, which judging
from the circumstantial evidence must have been made by every smart
person who studied a little philosophy and declined to pursue it
further, but for how he acted in response.
[12]
Instead of quietly
switching to another field, he made a fuss, from inside.  He was
Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein
gave it. 
[13]
Later in life he spent a lot of time talking about
how words worked.  Since that seems to be allowed, that's what a
lot of philosophers do now.  Meanwhile, sensing a vacuum in the
metaphysical speculation department, the people who used to do
literary criticism have been edging Kantward, under new names like
"literary theory," "critical theory," and when they're feeling
ambitious, plain "theory."  The writing is the familiar word salad:

  Gender is not like some of the other grammatical modes which
  express precisely a mode of conception without any reality that
  corresponds to the conceptual mode, and consequently do not express
  precisely something in reality by which the intellect could be
  moved to conceive a thing the way it does, even where that motive
  is not something in the thing as such.
  [14]

The singularity I've described is not going away.  There's a market
for writing that sounds impressive and can't be disproven. There
will always be both supply and demand.  So if one group abandons
this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.
Perhaps we should do what Aristotle meant to do, instead of what
he did.  The goal he announces in the Metaphysics seems one worth
pursuing: to discover the most general truths.  That sounds good.
But instead of trying to discover them because they're useless,
let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised
criterion, applicability, as a guide to keep us from wondering
off into a swamp of abstractions.  Instead of trying to answer the
question:

  What are the most general truths?

let's try to answer the question

  Of all the useful things we can say, which are the most general?

The test of utility I propose is whether we cause people who read
what we've written to do anything differently afterward.  Knowing
we have to give definite (if implicit) advice will keep us from
straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a
different direction.As an example of a useful, general idea, consider that of the
controlled experiment.  There's an idea that has turned out to be
widely applicable.  Some might say it's part of science, but it's
not part of any specific science; it's literally meta-physics (in
our sense of "meta").   The idea of evolution is another. It turns
out to have quite broad applications—for example, in genetic
algorithms and even product design.  Frankfurt's distinction between
lying and bullshitting seems a promising recent example.
[15]These seem to me what philosophy should look like: quite general
observations that would cause someone who understood them to do
something differently.Such observations will necessarily be about things that are imprecisely
defined.  Once you start using words with precise meanings, you're
doing math.  So starting from utility won't entirely solve the
problem I described above—it won't flush out the metaphysical
singularity.  But it should help.  It gives people with good
intentions a new roadmap into abstraction.  And they may thereby
produce things that make the writing of the people with bad intentions
look bad by comparison.One drawback of this approach is that it won't produce the sort of
writing that gets you tenure.  And not just because it's not currently
the fashion.  In order to get tenure in any field you must not
arrive at conclusions that members of tenure committees can disagree
with.  In practice there are two kinds of solutions to this problem.
In math and the sciences, you can prove what you're saying, or at
any rate adjust your conclusions so you're not claiming anything
false ("6 of 8 subjects had lower blood pressure after the treatment").
In the humanities you can either avoid drawing any definite conclusions
(e.g. conclude that an issue is a complex one), or draw conclusions
so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either
of these routes.  At best you'll be able to achieve the essayist's
standard of proof, not the mathematician's or the experimentalist's.
And yet you won't be able to meet the usefulness test without
implying definite and fairly broadly applicable conclusions.  Worse
still, the usefulness test will tend to produce results that annoy
people: there's no use in telling people things they already believe,
and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting
to general plus useful by starting with useful and cranking up the
generality may be unsuitable for junior professors trying to get
tenure, but it's better for everyone else, including professors who
already have it.  This side of the mountain is a nice gradual slope.
You can start by writing things that are useful but very specific,
and then gradually make them more general.  Joe's has good burritos.
What makes a good burrito?  What makes good food?  What makes
anything good?  You can take as long as you want.  You don't have
to get all the way to the top of the mountain.  You don't have to
tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an
encouraging thought.  The field is a lot younger than it seems.
Though the first philosophers in the western tradition lived about
2500 years ago, it would be misleading to say the field is 2500
years old, because for most of that time the leading practitioners
weren't doing much more than writing commentaries on Plato or
Aristotle while watching over their shoulders for the next invading
army.  In the times when they weren't, philosophy was hopelessly
intermingled with religion.  It didn't shake itself free till a
couple hundred years ago, and even then was afflicted by the
structural problems I've described above.  If I say this, some will
say it's a ridiculously overbroad and uncharitable generalization,
and others will say it's old news, but here goes: judging from their
works, most philosophers up to the present have been wasting their
time.  So in a sense the field is still at the first step. 
[16]That sounds a preposterous claim to make.  It won't seem so
preposterous in 10,000 years.  Civilization always seems old, because
it's always the oldest it's ever been.  The only way to say whether
something is really old or not is by looking at structural evidence,
and structurally philosophy is young; it's still reeling from the
unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot
more to discover.Notes
[1]
In practice formal logic is not much use, because despite
some progress in the last 150 years we're still only able to formalize
a small percentage of statements.  We may never do that much better,
for the same reason 1980s-style "knowledge representation" could
never have worked; many statements may have no representation more
concise than a huge, analog brain state.[2]
It was harder for Darwin's contemporaries to grasp this than
we can easily imagine.  The story of creation in the Bible is not
just a Judeo-Christian concept; it's roughly what everyone must
have believed since before people were people.  The hard part of
grasping evolution was to realize that species weren't, as they
seem to be, unchanging, but had instead evolved from different,
simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized
country encounters the idea of evolution for the first time as an
adult.  Everyone's taught about it as a child, either as truth or
heresy.[3]
Greek philosophers before Plato wrote in verse.  This must
have affected what they said.  If you try to write about the nature
of the world in verse, it inevitably turns into incantation.  Prose
lets you be more precise, and more tentative.[4]
Philosophy is like math's
ne'er-do-well brother.  It was born when Plato and Aristotle looked
at the works of their predecessors and said in effect "why can't
you be more like your brother?"  Russell was still saying the same
thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy
the imprecise half.  It's probably inevitable that philosophy will
suffer by comparison, because there's no lower bound to its precision.
Bad math is merely boring, whereas bad philosophy is nonsense.  And
yet there are some good ideas in the imprecise half.[5]
Aristotle's best work was in logic and zoology, both of which
he can  be said to have invented.  But the most dramatic departure
from his predecessors was a new, much more analytical style of
thinking.  He was arguably the first scientist.[6]
Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.
94.[7]
Some would say we depend on Aristotle more than we realize,
because his ideas were one of the ingredients in our common culture.
Certainly a lot of the words we use have a connection with Aristotle,
but it seems a bit much to suggest that we wouldn't have the concept
of the essence of something or the distinction between matter and
form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to
diff European culture with Chinese: what ideas did European culture
have in 1800 that Chinese culture didn't, in virtue of Aristotle's
contribution?[8]
The meaning of the word "philosophy" has changed over time.
In ancient times it covered a broad range of topics, comparable in
scope to our "scholarship" (though without the methodological
implications).  Even as late as Newton's time it included what we
now call "science."  But core of the subject today is still what
seemed to Aristotle the core: the attempt to discover the most
general truths.Aristotle didn't call this "metaphysics."  That name got assigned
to it because the books we now call the Metaphysics came after
(meta = after) the Physics in the standard edition of Aristotle's
works compiled by Andronicus of Rhodes three centuries later.  What
we call "metaphysics" Aristotle called "first philosophy."[9]
Some of Aristotle's immediate successors may have realized
this, but it's hard to say because most of their works are lost.[10]
Sokal, Alan, "Transgressing the Boundaries: Toward a Transformative
Hermeneutics of Quantum Gravity," Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's
aligned with some axe the audience already has to grind.  If this
is so we should find it's most popular with groups that are (or
feel) weak.  The powerful don't need its reassurance.[11]
Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,
p. 75.[12]
A preliminary result, that all metaphysics between Aristotle
and 1783 had been a waste of time, is due to I. Kant.[13]
Wittgenstein asserted a sort of mastery to which the inhabitants
of early 20th century Cambridge seem to have been peculiarly
vulnerable—perhaps partly because so many had been raised religious
and then stopped believing, so had a vacant space in their heads
for someone to tell them what to do (others chose Marx or Cardinal
Newman), and partly because a quiet, earnest place like Cambridge
in that era had no natural immunity to messianic figures, just as
European politics then had no natural immunity to dictators.[14]
This is actually from the Ordinatio of Duns Scotus (ca.
1300), with "number" replaced by "gender."  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,
1963, p. 92.[15]
Frankfurt, Harry, On Bullshit,  Princeton University Press,
2005.[16]
Some introductions to philosophy now take the line that
philosophy is worth studying as a process rather than for any
particular truths you'll learn.  The philosophers whose works they
cover would be rolling in their graves at that.  They hoped they
were doing more than serving as examples of how to argue: they hoped
they were getting results.  Most were wrong, but it doesn't seem
an impossible hope.This argument seems to me like someone in 1500 looking at the lack
of results achieved by alchemy and saying its value was as a process.
No, they were going about it wrong.  It turns out it is possible
to transmute lead into gold (though not economically at current
energy prices), but the route to that knowledge was to
backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, 
Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.

Want to start a startup?  Get funded by
Y Combinator.




April 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz
Developer Symposium.)
In the summer of 1995, my friend Robert Morris and I
started a startup called 
Viaweb.  
Our plan was to write
software that would let end users build online stores.
What was novel about this software, at the time, was
that it ran on our server, using ordinary Web pages
as the interface.A lot of people could have been having this idea at the
same time, of course, but as far as I know, Viaweb was
the first Web-based application.  It seemed such
a novel idea to us that we named the company after it:
Viaweb, because our software worked via the Web,
instead of running on your desktop computer.Another unusual thing about this software was that it
was written primarily in a programming language called
Lisp. It was one of the first big end-user
applications to be written in Lisp, which up till then
had been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called "How to Become a Hacker,"
and in it, among other things, he tells would-be hackers what
languages they should learn.  He suggests starting with Python and
Java, because they are easy to learn.  The serious hacker will also
want to learn C, in order to hack Unix, and Perl for system
administration and cgi scripts.  Finally, the truly serious hacker
should consider learning Lisp:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

This is the same argument you tend to hear for learning Latin.  It
won't get you a job, except perhaps as a classics professor, but
it will improve your mind, and make you a better writer in languages
you do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The
reason Latin won't get you a job is that no one speaks it.  If you
write in Latin, no one can understand you.  But Lisp is a computer
language, and computers speak whatever language you, the programmer,
tell them to.So if Lisp makes you a better programmer, like he says, why wouldn't
you want to use it? If a painter were offered a brush that would
make him a better painter, it seems to me that he would want to
use it in all his paintings, wouldn't he? I'm not trying to make
fun of Eric Raymond here.  On the whole, his advice is good.  What
he says about Lisp is pretty much the conventional wisdom.  But
there is a contradiction in the conventional wisdom:  Lisp will
make you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp
really does yield better programs, you should use it.  And if it
doesn't, then who needs it?This is not just a theoretical question.  Software is a very
competitive business, prone to natural monopolies.  A company that
gets software written faster and better will, all other things
being equal, put its competitors out of business.  And when you're
starting a startup, you feel this very keenly.  Startups tend to
be an all or nothing proposition.  You either get rich, or you get
nothing.  In a startup, if you bet on the wrong technology, your
competitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason
not to trust our instincts and go with Lisp.  We knew that everyone
else was writing their software in C++ or Perl.  But we also knew
that that didn't mean anything.  If you chose technology that way,
you'd be running Windows.  When you choose technology, you have to
ignore what other people are doing, and consider only what will
work the best.This is especially true in a startup.  In a big company, you can
do what all the other big companies are doing.  But a startup can't
do what all the other startups do.  I don't think a lot of people
realize this, even in startups.The average big company grows at about ten percent a year.  So if
you're running a big company and you do everything the way the
average big company does it, you can expect to do as well as the
average big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.
If you do everything the way the average startup does it, you should
expect average performance.  The problem here is, average performance
means that you'll go out of business.  The survival rate for startups
is way less than fifty percent.  So if you're running a startup,
you had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors
understood, and few understand even now:  when you're writing
software that only has to run on your own servers, you can use
any language you want.  When you're writing desktop software,
there's a strong bias toward writing applications in the same
language as the operating system.  Ten years ago, writing applications
meant writing applications in C.  But with Web-based software,
especially when you have the source code of both the language and
the operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you
can use any language, you have to think about which one to use.
Companies that try to pretend nothing has changed risk finding that
their competitors do not.If you can use any language, which do you use?  We chose Lisp.
For one thing, it was obvious that rapid development would be
important in this market.  We were all starting from scratch, so
a company that could get new features done before its competitors
would have a big advantage.  We knew Lisp was a really good language
for writing software quickly, and server-based applications magnify
the effect of rapid development, because you can release software
the minute it's done.If other companies didn't want to use Lisp, so much the better.
It might give us a technological edge, and we needed all the help
we could get.  When we started Viaweb, we had no experience in
business.  We didn't know anything about marketing, or hiring
people, or raising money, or getting customers.  Neither of us had
ever even had what you would call a real job.  The only thing we
were good at was writing software.  We hoped that would save us.
Any advantage we could get in the software department, we would
take.So you could say that using Lisp was an experiment.  Our hypothesis
was that if we wrote our software in Lisp, we'd be able to get
features done faster than our competitors, and also to do things
in our software that they couldn't do.  And because Lisp was so
high-level, we wouldn't need a big development team, so our costs
would be lower.  If this were so, we could offer a better product
for less money, and still make a profit.  We would end up getting
all the users, and our competitors would get none, and eventually
go out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,
it worked.  We eventually had many competitors, on the order of
twenty to thirty of them, but none of their software could compete
with ours.  We had a wysiwyg online store builder that ran on the
server and yet felt like a desktop application.  Our competitors
had cgi scripts.  And we were always far ahead of them in features.
Sometimes, in desperation, competitors would try to introduce
features that we didn't have.  But with Lisp our development cycle
was so fast that we could sometimes duplicate a new feature within
a day or two of a competitor announcing it in a press release.  By
the time journalists covering the press release got round to calling
us, we would have the new feature too.It must have seemed to our competitors that we had some kind of
secret weapon-- that we were decoding their Enigma traffic or
something.  In fact we did have a secret weapon, but it was simpler
than they realized.  No one was leaking news of their features to
us.   We were just able to develop software faster than anyone
thought possible.When I was about nine I happened to get hold of a copy of The Day
of the Jackal, by Frederick Forsyth.  The main character is an
assassin who is hired to kill the president of France.  The assassin
has to get past the police to get up to an apartment that overlooks
the president's route.  He walks right by them, dressed up as an
old man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird
AI language, with a bizarre syntax full of parentheses.  For years
it had annoyed me to hear Lisp described that way.  But now it
worked to our advantage.  In business, there is nothing more valuable
than a technical advantage your competitors don't understand.  In
business, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything
publicly about Lisp while we were working on Viaweb.  We never
mentioned it to the press, and if you searched for Lisp on our Web
site, all you'd find were the titles of two books in my bio.  This
was no accident.  A startup should give its competitors as little
information as possible.  If they didn't know what language our
software was written in, or didn't care, I wanted to keep it that
way.[2]The people who understood our technology best were the customers.
They didn't care what language Viaweb was written in either, but
they noticed that it worked really well.  It let them build great
looking online stores literally in minutes.  And so, by word of
mouth mostly, we got more and more users.  By the end of 1996 we
had about 70 stores online.  At the end of 1997 we had 500.  Six
months later, when Yahoo bought us, we had 1070 users.  Today, as
Yahoo Store, this software continues to dominate its market.  It's
one of the more profitable pieces of Yahoo, and the stores built
with it are the foundation of Yahoo Shopping.  I left Yahoo in
1999, so I don't know exactly how many users they have now, but
the last I heard there were about 20,000.
The Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't
everyone use it?  These sound like rhetorical questions, but actually
they have straightforward answers.  Lisp is so great not because
of some magic quality visible only to devotees, but because it is
simply the most powerful language available.  And the reason everyone
doesn't use it is that programming languages are not merely
technologies, but habits of mind as well, and nothing changes
slower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming
languages vary in power.Few would dispute, at least, that high level languages are more
powerful than machine language.  Most programmers today would agree
that you do not, ordinarily, want to program in machine language.
Instead, you should program in a high-level language, and have a
compiler translate it into machine language for you.  This idea is
even built into the hardware now: since the 1980s, instruction sets
have been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand
in machine language.  What's less often understood is that there
is a more general principle here: that if you have a choice of
several languages, it is, all other things being equal, a mistake
to program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program
that has to work very closely with a program written in a certain
language, it might be a good idea to write the new program in the
same language.  If you're writing a program that only has to do
something very simple, like number crunching or bit manipulation,
you may as well use a less abstract language, especially since it
may be slightly faster.  And if you're writing a short, throwaway
program, you may be better off just using whatever language has
the best library functions for the task.  But in general, for
application software, you want to be using the most powerful
(reasonably efficient) language you can get, and using anything
else is a mistake, of exactly the same kind, though possibly in a
lesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least
as a kind of social convention, high-level languages are often all
treated as equivalent.  They're not.  Technically the term "high-level
language" doesn't mean anything very definite.  There's no dividing
line with machine languages on one side and all the high-level
languages on the other.  Languages fall along a continuum [4] of
abstractness, from the most powerful all the way down to machine
languages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that
it gets compiled into machine language.  Would anyone seriously
argue that Cobol is equivalent in power to, say, Python?  It's
probably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures
got added to the language.  Most Perl hackers would agree that Perl
5 is more powerful than Perl 4.  But once you've admitted that,
you've admitted that one high level language can be more powerful
than another.  And it follows inexorably that, except in special
cases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a
certain age, programmers rarely switch languages voluntarily.
Whatever language people happen to be used to, they tend to consider
just good enough.Programmers get very attached to their favorite languages, and I
don't want to hurt anyone's feelings, so to explain this point I'm
going to use a hypothetical language called Blub.  Blub falls right
in the middle of the abstractness continuum.  It is not the most
powerful language, but it is more powerful than Cobol or machine
language.And in fact, our hypothetical Blub programmer wouldn't use either
of them.  Of course he wouldn't program in machine language.  That's
what compilers are for.  And as for Cobol, he doesn't know how
anyone can get anything done with it.  It doesn't even have x (Blub
feature of your choice).As long as our hypothetical Blub programmer is looking down the
power continuum, he knows he's looking down.  Languages less powerful
than Blub are obviously less powerful, because they're missing some
feature he's used to.  But when our hypothetical Blub programmer
looks in the other direction, up the power continuum, he doesn't
realize he's looking up.  What he sees are merely weird languages.
He probably considers them about equivalent in power to Blub, but
with all this other hairy stuff thrown in as well.  Blub is good
enough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of
the languages higher up the power continuum, however, we find that
he in turn looks down upon Blub.  How can you get anything done in
Blub? It doesn't even have y.By induction, the only programmers in a position to see all the
differences in power between the various languages are those who
understand the most powerful one.  (This is probably what Eric
Raymond meant about Lisp making you a better programmer.) You can't
trust the opinions of the others, because of the Blub paradox:
they're satisfied with whatever language they happen to use, because
it dictates the way they think about programs.I know this from my own experience, as a high school kid writing
programs in Basic.  That language didn't even support recursion.
It's hard to imagine writing programs without using recursion, but
I didn't miss it at the time.  I thought in Basic.  And I was a
whiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at
various points on the power continuum.  Where they fall relative
to one another is a sensitive topic.  What I will say is that I
think Lisp is at the top.  And to support this claim I'll tell you
about one of the things I find missing when I look at the other
four languages.  How can you get anything done in them, I think,
without macros? [5]Many languages have something called a macro.  But Lisp macros are
unique.  And believe it or not, what they do is related to the
parentheses.  The designers of Lisp didn't put all those parentheses
in the language just to be different.  To the Blub programmer, Lisp
code looks weird.  But those parentheses are there for a reason.
They are the outward evidence of a fundamental difference between
Lisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial
sense that the source files contain characters, and strings are
one of the data types supported by the language.  Lisp code, after
it's read by the parser, is made of data structures that you can
traverse.If you understand how compilers work, what's really going on is
not so much that Lisp has a strange syntax as that Lisp has no
syntax.  You write programs in the parse trees that get generated
within the compiler when other languages are parsed.  But these
parse trees are fully accessible to your programs.  You can write
programs that manipulate them.  In Lisp, these programs are called
macros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?
Not very often, if you think in Cobol.  All the time, if you think
in Lisp.  It would be convenient here if I could give an example
of a powerful macro, and say there! how about that?  But if I did,
it would just look like gibberish to someone who didn't know Lisp;
there isn't room here to explain everything you'd need to know to
understand what it meant.  In 
Ansi Common Lisp I tried to move
things along as fast as I could, and even so I didn't get to macros
until page 160.But I think I can give a kind of argument that might be convincing.
The source code of the Viaweb editor was probably about 20-25%
macros.  Macros are harder to write than ordinary Lisp functions,
and it's considered to be bad style to use them when they're not
necessary.  So every macro in that code is there because it has to
be.  What that means is that at least 20-25% of the code in this
program is doing things that you can't easily do in any other
language.  However skeptical the Blub programmer might be about my
claims for the mysterious powers of Lisp, this ought to make him
curious.  We weren't writing this code for our own amusement.  We
were a tiny startup, programming as hard as we could in order to
put technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some
correlation here.  A big chunk of our code was doing things that
are very hard to do in other languages.  The resulting software
did things our competitors' software couldn't do.  Maybe there was
some kind of connection.  I encourage you to follow that thread.
There may be more to that old man hobbling along on his crutches
than meets the eye.Aikido for StartupsBut I don't expect to convince anyone 
(over 25) 
to go out and learn
Lisp.  The purpose of this article is not to change anyone's mind,
but to reassure people already interested in using Lisp-- people
who know that Lisp is a powerful language, but worry because it
isn't widely used.  In a competitive situation, that's an advantage.
Lisp's power is multiplied by the fact that your competitors don't
get it.If you think of using Lisp in a startup, you shouldn't worry that
it isn't widely understood.  You should hope that it stays that
way. And it's likely to.  It's the nature of programming languages
to make most people satisfied with whatever they currently use.
Computer hardware changes so much faster than personal habits that
programming practice is usually ten to twenty years behind the
processor.  At places like MIT they were writing programs in
high-level languages in the early 1960s, but many companies continued
to write code in machine language well into the 1980s.  I bet a
lot of people continued to write machine language until the processor,
like a bartender eager to close up and go home, finally kicked them
out by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are
different: programming languages are not just technology, but what
programmers think in.  They're half technology and half religion.[6]
And so the median language, meaning whatever language the median
programmer uses, moves as slow as an iceberg.  Garbage collection,
introduced by Lisp in about 1960, is now widely considered to be
a good thing.  Runtime typing, ditto, is growing in popularity.
Lexical closures, introduced by Lisp in the early 1970s, are now,
just barely, on the radar screen.  Macros, introduced by Lisp in the
mid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not
proposing that you can fight this powerful force.  What I'm proposing
is exactly the opposite: that, like a practitioner of Aikido, you
can use it against your opponents.If you work for a big company, this may not be easy.  You will have
a hard time convincing the pointy-haired boss to let you build
things in Lisp, when he has just read in the paper that some other
language is poised, like Ada was twenty years ago, to take over
the world.  But if you work for a startup that doesn't have
pointy-haired bosses yet, you can, like we did, turn the Blub
paradox to your advantage:  you can use technology that your
competitors, glued immovably to the median language, will never be
able to match.If you ever do find yourself working for a startup, here's a handy
tip for evaluating competitors.  Read their job listings.  Everything
else on their site may be stock photos or the prose equivalent,
but the job listings have to be specific about what they want, or
they'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.
A new competitor seemed to emerge out of the woodwork every month
or so.  The first thing I would do, after checking to see if they
had a live online demo, was look at their job listings.  After a
couple years of this I could tell which companies to worry about
and which not to.  The more of an IT flavor the job descriptions
had, the less dangerous the company was.  The safest kind were the
ones that wanted Oracle experience.  You never had to worry about
those.  You were also safe if they said they wanted C++ or Java
developers.  If they wanted Perl or Python programmers, that would
be a bit frightening-- that's starting to sound like a company
where the technical side, at least, is run by real hackers.  If I
had ever seen a job posting looking for Lisp hackers, I would have
been really worried.
Notes[1] Viaweb at first had two parts: the editor, written in Lisp,
which people used to build their sites, and the ordering system,
written in C, which handled orders.  The first version was mostly
Lisp, because the ordering system was small.  Later we added two
more modules, an image generator written in C, and a back-office
manager written mostly in Perl.In January 2003, Yahoo released a new version of the editor 
written in C++ and Perl.  It's hard to say whether the program is no
longer written in Lisp, though, because to translate this program
into C++ they literally had to write a Lisp interpreter: the source
files of all the page-generating templates are still, as far as I
know,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because
even if our competitors had known we were using Lisp, they wouldn't
have understood why:  "If they were that smart they'd already be
programming in Lisp."[3] All languages are equally powerful in the sense of being Turing
equivalent, but that's not the sense of the word programmers care
about. (No one wants to program a Turing machine.)  The kind of
power programmers care about may not be formally definable, but
one way to explain it would be to say that it refers to features
you could only get in the less powerful language by writing an
interpreter for the more powerful language in it. If language A
has an operator for removing spaces from strings and language B
doesn't, that probably doesn't make A more powerful, because you
can probably write a subroutine to do it in B.  But if A supports,
say, recursion, and B doesn't, that's not likely to be something
you can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;
it's not the shape that matters here but the idea that there is at
least a partial order.[5] It is a bit misleading to treat macros as a separate feature.
In practice their usefulness is greatly enhanced by other Lisp
features like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take
the form of religious wars or undergraduate textbooks so determinedly
neutral that they're really works of anthropology.  People who
value their peace, or want tenure, avoid the topic.  But the question
is only half a religious one; there is something there worth
studying, especially if you want to design new languages.December 2001 (rev. May 2002)

(This article came about in response to some questions on
the LL1 mailing list.  It is now
incorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was
a radical departure from existing languages,
the most important of which was Fortran.Lisp embodied nine new ideas:
1. Conditionals.  A conditional is an if-then-else
construct.  We take these for granted now.  They were 
invented
by McCarthy in the course of developing Lisp. 
(Fortran at that time only had a conditional
goto, closely based on the branch instruction in the 
underlying hardware.)  McCarthy, who was on the Algol committee, got
conditionals into Algol, whence they spread to most other
languages.2. A function type. In Lisp, functions are first class 
objects-- they're a data type just like integers, strings,
etc, and have a literal representation, can be stored in variables,
can be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept
before Lisp of course, but Lisp was the first programming language to support
it.  (It's arguably implicit in making functions first class
objects.)4. A new concept of variables.  In Lisp, all variables
are effectively pointers. Values are what
have types, not variables, and assigning or binding
variables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are 
trees of expressions, each of which returns a value.  
(In some Lisps expressions
can return multiple values.)  This is in contrast to Fortran
and most succeeding languages, which distinguish between
expressions and statements.It was natural to have this
distinction in Fortran because (not surprisingly in a language
where the input format was punched cards) the language was
line-oriented.  You could not nest statements.  And
so while you needed expressions for math to work, there was
no point in making anything else return a value, because
there could not be anything waiting for it.This limitation
went away with the arrival of block-structured languages,
but by then it was too late. The distinction between
expressions and statements was entrenched.  It spread from 
Fortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can
compose expressions however you want.  You can say either
(using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that
you can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.  
There is
no real distinction between read-time, compile-time, and runtime.
You can compile or run code while reading, read or run code
while compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax;
running code at compile-time is the basis of macros; compiling
at runtime is the basis of Lisp's use as an extension
language in programs like Emacs; and reading at runtime
enables programs to communicate using s-expressions, an
idea recently reinvented as XML.
When Lisp was first invented, all these ideas were far
removed from ordinary programming practice, which was
dictated largely by the hardware available in the late 1950s.Over time, the default language, embodied
in a succession of popular languages, has
gradually evolved toward Lisp.  1-5 are now widespread.
6 is starting to appear in the mainstream.
Python has a form of 7, though there doesn't seem to be
any syntax for it.  
8, which (with 9) is what makes Lisp macros
possible, is so far still unique to Lisp,
perhaps because (a) it requires those parens, or something 
just as bad, and (b) if you add that final increment of power, 
you can no 
longer claim to have invented a new language, but only
to have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's
strange to describe Lisp in terms of its
variation from the random expedients other languages
adopted.  That was not, probably, how McCarthy
thought of it.  Lisp wasn't designed to fix the mistakes
in Fortran; it came about more as the byproduct of an
attempt to axiomatize computation.January 2012A few hours before the Yahoo acquisition was announced in June 1998
I took a snapshot of Viaweb's
site.  I thought it might be interesting to look at one day.The first thing one notices is is how tiny the pages are.  Screens
were a lot smaller in 1998.  If I remember correctly, our frontpage
used to just fit in the size window people typically used then.Browsers then (IE 6 was still 3 years in the future) had few fonts
and they weren't antialiased.  If you wanted to make pages that
looked good, you had to render display text as images.You may notice a certain similarity between the Viaweb and Y Combinator logos.  We did that
as an inside joke when we started YC.  Considering how basic a red
circle is, it seemed surprising to me when we started Viaweb how
few other companies used one as their logo.  A bit later I realized
why.On the Company
page you'll notice a mysterious individual called John McArtyem.
Robert Morris (aka Rtm) was so publicity averse after the 
Worm that he
didn't want his name on the site.  I managed to get him to agree
to a compromise: we could use his bio but not his name.  He has
since relaxed a bit
on that point.Trevor graduated at about the same time the acquisition closed, so in the
course of 4 days he went from impecunious grad student to millionaire
PhD.  The culmination of my career as a writer of press releases
was one celebrating
his graduation, illustrated with a drawing I did of him during
a meeting.(Trevor also appears as Trevino
Bagwell in our directory of web designers merchants could hire
to build stores for them.  We inserted him as a ringer in case some
competitor tried to spam our web designers.   We assumed his logo
would deter any actual customers, but it did not.)Back in the 90s, to get users you had to get mentioned in magazines
and newspapers.  There were not the same ways to get found online
that there are today.  So we used to pay a PR
firm $16,000 a month to get us mentioned in the press.  Fortunately
reporters liked
us.In our advice about
getting traffic from search engines (I don't think the term SEO
had been coined yet), we say there are only 7 that matter: Yahoo,
AltaVista, Excite, WebCrawler, InfoSeek, Lycos, and HotBot.  Notice
anything missing?  Google was incorporated that September.We supported online transactions via a company called 
Cybercash,
since if we lacked that feature we'd have gotten beaten up in product
comparisons.  But Cybercash was so bad and most stores' order volumes
were so low that it was better if merchants processed orders like phone orders.  We had a page in our site trying to talk merchants
out of doing real time authorizations.The whole site was organized like a funnel, directing people to the
test drive.
It was a novel thing to be able to try out software online.  We put
cgi-bin in our dynamic urls to fool competitors about how our
software worked.We had some well
known users.  Needless to say, Frederick's of Hollywood got the
most traffic.  We charged a flat fee of $300/month for big stores,
so it was a little alarming to have users who got lots of traffic.
I once calculated how much Frederick's was costing us in bandwidth,
and it was about $300/month.Since we hosted all the stores, which together were getting just
over 10 million page views per month in June 1998, we consumed what
at the time seemed a lot of bandwidth.  We had 2 T1s (3 Mb/sec)
coming into our offices.  In those days there was no AWS.  Even
colocating servers seemed too risky, considering how often things
went wrong with them.  So we had our servers in our offices.  Or
more precisely, in Trevor's office.  In return for the unique
privilege of sharing his office with no other humans, he had to
share it with 6 shrieking tower servers.  His office was nicknamed
the Hot Tub on account of the heat they generated.  Most days his
stack of window air conditioners could keep up.For describing pages, we had a template language called RTML, which
supposedly stood for something, but which in fact I named after
Rtm.  RTML was Common Lisp augmented by some macros and libraries,
and concealed under a structure editor that made it look like it
had syntax.Since we did continuous releases, our software didn't actually have
versions.  But in those days the trade press expected versions, so
we made them up.  If we wanted to get lots of attention, we made
the version number an
integer.  That "version 4.0" icon was generated by our own
button generator, incidentally.  The whole Viaweb site was made
with our software, even though it wasn't an online store, because
we wanted to experience what our users did.At the end of 1997, we released a general purpose shopping search
engine called Shopfind.  It
was pretty advanced for the time.  It had a programmable crawler
that could crawl most of the different stores online and pick out
the products.

April 2009I usually avoid politics, but since we now seem to have an administration that's open to suggestions, I'm going to risk making one.  The single biggest thing the government could do to increase the number of startups in this country is a policy that would cost nothing: establish a new class of visa for startup founders.The biggest constraint on the number of new startups that get created in the US is not tax policy or employment law or even Sarbanes-Oxley.  It's that we won't let the people who want to start them into the country.Letting just 10,000 startup founders into the country each year could have a visible effect on the economy.  If we assume 4 people per startup, which is probably an overestimate, that's 2500 new companies.  Each year.  They wouldn't all grow as big as Google, but out of 2500 some would come close.By definition these 10,000 founders wouldn't be taking jobs from Americans: it could be part of the terms of the visa that they couldn't work for existing companies, only new ones they'd founded.  In fact they'd cause there to be 
more jobs for Americans, because the companies they started would hire more employees as they grew.The tricky part might seem to be how one defined a startup. But that could be solved quite easily: let the market decide.  Startup investors work hard to find the best startups.  The government could not do better than to piggyback on their expertise, and use investment by recognized startup investors as the test of whether a company was a real startup.How would the government decide who's a startup investor?  The same way they decide what counts as a university for student visas. We'll establish our own accreditation procedure. We know who one another are.10,000 people is a drop in the bucket by immigration standards, but would represent a huge increase in the pool of startup founders.  I think this would have such a visible effect on the economy that it would make the legislator who introduced the bill famous.  The only way to know for sure would be to try it, and that would cost practically nothing.
Thanks to Trevor Blackwell, Paul Buchheit, Jeff Clavier, David Hornik, Jessica Livingston, Greg Mcadoo, Aydin Senkut, and Fred Wilson for reading drafts of this.Related:November 2022Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I wasn't responsible
for my actions. I gradually lost hope of that, but the puzzle
remained: How do you reconcile being a machine made of matter with
the feeling that you're free to choose what you do?
[1]The best way to explain the answer may be to start with a slightly
wrong version, and then fix it. The wrong version is: You can do
what you want, but you can't want what you want. Yes, you can control
what you do, but you'll do what you want, and you can't control
that.The reason this is mistaken is that people do sometimes change what
they want. People who don't want to want something — drug addicts,
for example — can sometimes make themselves stop wanting it. And
people who want to want something — who want to like classical
music, or broccoli — sometimes succeed.So we modify our initial statement: You can do what you want, but
you can't want to want what you want.That's still not quite true. It's possible to change what you want
to want. I can imagine someone saying "I decided to stop wanting
to like classical music." But we're getting closer to the truth.
It's rare for people to change what they want to want, and the more
"want to"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more "want
to"s in much the same way we can get arbitrarily close to 1 by adding
more 9s to a string of 9s following a decimal point. In practice
three or four "want to"s must surely be enough. It's hard even to
envision what it would mean to change what you want to want to want
to want, let alone actually do it.So one way to express the correct answer is to use a regular
expression. You can do what you want, but there's some statement
of the form "you can't (want to)* want what you want" that's true.
Ultimately you get back to a want that you don't control.
[2]
Notes[1]
I didn't know when I was 9 that matter might behave randomly,
but I don't think it affects the problem much. Randomness destroys
the ghost in the machine as effectively as determinism.[2]
If you don't like using an expression, you can make the same
point using higher-order desires: There is some n such that you
don't control your nth-order desires.
Thanks to Trevor Blackwell,
Jessica Livingston, Robert Morris, and
Michael Nielsen for reading drafts of this.May 2006(This essay is derived from a keynote at Xtech.)Could you reproduce Silicon Valley elsewhere, or is there something
unique about it?It wouldn't be surprising if it were hard to reproduce in other
countries, because you couldn't reproduce it in most of the US
either.  What does it take to make a silicon valley even here?What it takes is the right people.  If you could get the right ten
thousand people to move from Silicon Valley to Buffalo, Buffalo
would become Silicon Valley.  
[1]That's a striking departure from the past.  Up till a couple decades
ago, geography was destiny for cities.  All great cities were located
on waterways, because cities made money by trade, and water was the
only economical way to ship.Now you could make a great city anywhere, if you could get the right
people to move there.  So the question of how to make a silicon
valley becomes: who are the right people, and how do you get them
to move?Two TypesI think you only need two kinds of people to create a technology
hub: rich people and nerds.  They're the limiting reagents in the
reaction that produces startups, because they're the only ones
present when startups get started.  Everyone else will move.Observation bears this out: within the US, towns have become startup
hubs if and only if they have both rich people and nerds.  Few
startups happen in Miami, for example, because although it's full
of rich people, it has few nerds.  It's not the kind of place nerds
like.Whereas Pittsburgh has the opposite problem: plenty of nerds, but
no rich people.  The top US Computer Science departments are said
to be MIT, Stanford, Berkeley, and Carnegie-Mellon.  MIT yielded
Route 128.  Stanford and Berkeley yielded Silicon Valley.  But
Carnegie-Mellon?  The record skips at that point.  Lower down the
list, the University of Washington yielded a high-tech community
in Seattle, and the University of Texas at Austin yielded one in
Austin.  But what happened in Pittsburgh?  And in Ithaca, home of
Cornell, which is also high on the list?I grew up in Pittsburgh and went to college at Cornell, so I can
answer for both.  The weather is terrible,  particularly in winter,
and there's no interesting old city to make up for it, as there is
in Boston.  Rich people don't want to live in Pittsburgh or Ithaca.
So while there are plenty of hackers who could start startups,
there's no one to invest in them.Not BureaucratsDo you really need the rich people?  Wouldn't it work to have the
government invest in the nerds?  No, it would not.  Startup investors
are a distinct type of rich people.  They tend to have a lot of
experience themselves in the technology business.  This (a) helps
them pick the right startups, and (b) means they can supply advice
and connections as well as money.  And the fact that they have a
personal stake in the outcome makes them really pay attention.Bureaucrats by their nature are the exact opposite sort of people
from startup investors. The idea of them making startup investments
is comic.  It would be like mathematicians running Vogue-- or
perhaps more accurately, Vogue editors running a math journal.
[2]Though indeed, most things bureaucrats do, they do badly.   We just
don't notice usually, because they only have to compete against
other bureaucrats.  But as startup investors they'd have to compete
against pros with a great deal more experience and motivation.Even corporations that have in-house VC groups generally forbid
them to make their own investment decisions.  Most are only allowed
to invest in deals where some reputable private VC firm is willing
to act as lead investor.Not BuildingsIf you go to see Silicon Valley, what you'll see are buildings.
But it's the people that make it Silicon Valley, not the buildings.
I read occasionally about attempts to set up "technology
parks" in other places, as if the active ingredient of Silicon
Valley were the office space.  An article about Sophia Antipolis
bragged that companies there included Cisco, Compaq, IBM, NCR, and
Nortel.  Don't the French realize these aren't startups?Building office buildings for technology companies won't get you a
silicon valley, because the key stage in the life of a startup
happens before they want that kind of space.  The key stage is when
they're three guys operating out of an apartment.  Wherever the
startup is when it gets funded, it will stay.  The defining quality
of Silicon Valley is not that Intel or Apple or Google have offices
there, but that they were started there.So if you want to reproduce Silicon Valley, what you need to reproduce
is those two or three founders sitting around a kitchen table
deciding to start a company.  And to reproduce that you need those
people.UniversitiesThe exciting thing is, all you need are the people.  If you could
attract a critical mass of nerds and investors to live somewhere,
you could reproduce Silicon Valley.  And both groups are highly
mobile.  They'll go where life is good.  So what makes a place good
to them?What nerds like is other nerds.  Smart people will go wherever other
smart people are.  And in particular, to great universities.  In
theory there could be other ways to attract them, but so far
universities seem to be indispensable.  Within the US, there are
no technology hubs without first-rate universities-- or at least,
first-rate computer science departments.So if you want to make a silicon valley, you not only need a
university, but one of the top handful in the world.  It has to be
good enough to act as a magnet, drawing the best people from thousands
of miles away.  And that means it has to stand up to existing magnets
like MIT and Stanford.This sounds hard.  Actually it might be easy.  My professor friends,
when they're deciding where they'd like to work, consider one thing
above all: the quality of the other faculty.  What attracts professors
is good colleagues.  So if you managed to recruit, en masse, a
significant number of the best young researchers, you could create
a first-rate university from nothing overnight.  And you could do
that for surprisingly little.  If you paid 200 people hiring bonuses
of $3 million apiece, you could put together a faculty that would
bear comparison with any in the world.  And from that point the
chain reaction would be self-sustaining.  So whatever it costs to
establish a mediocre university, for an additional half billion or
so you could have a great one.  
[3]PersonalityHowever, merely creating a new university would not be enough to
start a silicon valley. The university is just the seed.  It has
to be planted in the right soil, or it won't germinate.  Plant it
in the wrong place, and you just create Carnegie-Mellon.To spawn startups, your university has to be in a town that has
attractions other than the university.  It has to be a place where
investors want to live, and students want to stay after they graduate.The two like much the same things, because most startup investors
are nerds themselves.  So what do nerds look for in a town?  Their
tastes aren't completely different from other people's, because a
lot of the towns they like most in the US are also big tourist
destinations: San Francisco, Boston, Seattle.   But their tastes
can't be quite mainstream either, because they dislike other big
tourist destinations, like New York, Los Angeles, and Las Vegas.There has been a lot written lately about the "creative class." The
thesis seems to be that as wealth derives increasingly from ideas,
cities will prosper only if they attract those who have them.  That
is certainly true; in fact it was the basis of Amsterdam's prosperity
400 years ago.A lot of nerd tastes they share with the creative class in general.
For example, they like well-preserved old neighborhoods instead of
cookie-cutter suburbs, and locally-owned shops and restaurants
instead of national chains.  Like the rest of the creative class,
they want to live somewhere with personality.What exactly is personality?  I think it's the feeling that each
building is the work of a distinct group of people.  A town with
personality is one that doesn't feel mass-produced.  So if you want
to make a startup hub-- or any town to attract the "creative class"--
you probably have to ban large development projects.
When a large tract has been developed by a single organization, you
can always tell. 
[4]Most towns with personality are old, but they don't have to be.
Old towns have two advantages: they're denser, because they were
laid out before cars, and they're more varied, because they were
built one building at a time.  You could have both now.  Just have
building codes that ensure density, and ban large scale developments.A corollary is that you have to keep out the biggest developer of
all: the government.  A government that asks "How can we build a
silicon valley?" has probably ensured failure by the way they framed
the question.  You don't build a silicon valley; you let one grow.NerdsIf you want to attract nerds, you need more than a town with
personality.  You need a town with the right personality.  Nerds
are a distinct subset of