{
    "d_model": 1024,
    "ssm_cfg": {
        "expand": 1,
        "ngroups": 8,
        "d_state": 128
    },
    "rms_norm_eps": 1e-05,
    "vocab_size": null,
    "d_inner": 1024,
    "d_xb": 256,
    "intermediate_size": 2048,
    "hidden_act": "silu",
    "n_layer": 2,
    "attn_layers": []
}